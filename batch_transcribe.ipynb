{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e014d6c1-2727-4e4a-83ff-203f8e7b3f2a",
   "metadata": {},
   "source": [
    "# Transcribe audio files in batch mode as fast as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9d1fda0-bbd8-4b3e-bd9c-57164c860bb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T07:25:09.596992Z",
     "iopub.status.busy": "2024-09-29T07:25:09.596545Z",
     "iopub.status.idle": "2024-09-29T07:25:09.602935Z",
     "shell.execute_reply": "2024-09-29T07:25:09.601931Z",
     "shell.execute_reply.started": "2024-09-29T07:25:09.596961Z"
    }
   },
   "outputs": [],
   "source": [
    "from importlib.metadata import version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56df76d7-4856-45fc-9f91-e361b1bb4d80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T07:07:17.512126Z",
     "iopub.status.busy": "2024-09-29T07:07:17.511762Z",
     "iopub.status.idle": "2024-09-29T07:07:17.522523Z",
     "shell.execute_reply": "2024-09-29T07:07:17.522269Z",
     "shell.execute_reply.started": "2024-09-29T07:07:17.512102Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b158edb7-33a0-4628-9eaf-9db3cce2e896",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9039795-f1f1-49d1-a0b6-cf92cf22c736",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11130d56-5a5b-40e8-b85c-d90dfd9577f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T07:25:11.304413Z",
     "iopub.status.busy": "2024-09-29T07:25:11.303756Z",
     "iopub.status.idle": "2024-09-29T07:25:11.314357Z",
     "shell.execute_reply": "2024-09-29T07:25:11.313938Z",
     "shell.execute_reply.started": "2024-09-29T07:25:11.304385Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.45.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ba96ccf-f1b0-4097-aa8f-209eae467360",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T07:25:11.685464Z",
     "iopub.status.busy": "2024-09-29T07:25:11.685140Z",
     "iopub.status.idle": "2024-09-29T07:25:11.691436Z",
     "shell.execute_reply": "2024-09-29T07:25:11.691172Z",
     "shell.execute_reply.started": "2024-09-29T07:25:11.685441Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.34.2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('accelerate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c196fc82-43f8-490a-a9e9-e8fb90427038",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b73db5e4-7516-484a-a76e-05aed1ed1b87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T07:30:55.669480Z",
     "iopub.status.busy": "2024-09-29T07:30:55.669060Z",
     "iopub.status.idle": "2024-09-29T07:30:55.679344Z",
     "shell.execute_reply": "2024-09-29T07:30:55.678910Z",
     "shell.execute_reply.started": "2024-09-29T07:30:55.669448Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.3'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('flash_attn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44703518-d75c-4223-93b7-92457811f4b0",
   "metadata": {},
   "source": [
    "## Huggingface Whisper\n",
    "\n",
    "https://github.com/huggingface/speech-to-speech/blob/main/STT/whisper_stt_handler.py\n",
    "\n",
    "https://huggingface.co/eustlb/distil-large-v3-fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffcb2fd4-e273-4125-8269-7874e7516d68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T05:55:42.752467Z",
     "iopub.status.busy": "2024-09-30T05:55:42.751823Z",
     "iopub.status.idle": "2024-09-30T05:55:48.231204Z",
     "shell.execute_reply": "2024-09-30T05:55:48.230838Z",
     "shell.execute_reply.started": "2024-09-30T05:55:42.752440Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"eustlb/distil-large-v3-fr\"\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, \n",
    "    use_safetensors=True, low_cpu_mem_usage=True, device_map=device, \n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    torch_dtype=torch_dtype\n",
    ")\n",
    "\n",
    "# warmup\n",
    "dummy_input = torch.randn( (1, model.config.num_mel_bins, 3000), dtype=torch_dtype, device=device)\n",
    "_ = model.generate(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1865cd0b-eb16-4c1c-8064-4a6edac0cf23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T05:57:37.504260Z",
     "iopub.status.busy": "2024-09-30T05:57:37.503405Z",
     "iopub.status.idle": "2024-09-30T05:57:43.329167Z",
     "shell.execute_reply": "2024-09-30T05:57:43.328828Z",
     "shell.execute_reply.started": "2024-09-30T05:57:37.504229Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-voice/.venv/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ok, donc dans la première partie, on a fait beaucoup de choses, on s'est posé beaucoup de questions pour pouvoir cadrer, sélectionner, identifier des projets à Basse-Dia qui soient pertinents. La pha\n"
     ]
    }
   ],
   "source": [
    "# ./audio/2024-09-19 16-32-50.mp3\n",
    "# - batch size 64, flash attention 2, no compile: 6.14 sec\n",
    "# - batch size 64, sdpa attention, compile default with fullgraph: 18 sec (first test) / 34 sec (second test)\n",
    "# => follow line by line the example of https://huggingface.co/eustlb/distil-large-v3-fr, it is the fastest combination\n",
    "\n",
    "# Test with 2 mp3 files\n",
    "# - small: 24 min 47 sec (1487 sec), 22.7 MB file\n",
    "# - big : 1h 24 min 19 sec (5059 sec), 77.2 MB file\n",
    "\n",
    "# Sequential long-form: pipe(mp3file)\n",
    "# - gpu memory = 3.1 GB -> 27 sec / 94 sec    || A100 : 29 sec\n",
    "\n",
    "# Chunked long-form: pipe(mp3file, chunk_length_s=25, batch_size=xxx)\n",
    "# batch size 1 : gpu memory = 3.1 GB -> 33 sec / 111 sec   || A100 : 42 sec / \n",
    "# batch size 8 : gpu memory = 3.6 GB -> 13 sec / 46 sec    || A100 : 21 sec / \n",
    "# batch size 16 : gpu memory = 4.4 GB -> 11 sec / 37 sec   || A100 : 15 sec / \n",
    "# batch size 32 : gpu memory = 5.1 GB -> 9.5 sec / 34 sec  || A100 : 16.6 sec / \n",
    "# batch size 64 : gpu memory = 7.2 GB -> 10.3 sec / 31 sec || A100 : 13.6 sec /\n",
    "# batch size 128 : gpu memory = 11 GB -> 9.3 sec / 33 sec  || A100 : 13.3 sec / \n",
    "\n",
    "# Sequential long-form algorithm + batch_size 2: pipe([mp3file1, mp3file2], batch_size=2)\n",
    "# => RuntimeError: The expanded size of the tensor (505922) must match the existing size (148730) at non-singleton dimension 1.  Target sizes: [128, 505922].  Tensor sizes: [128, 148730]\n",
    "\n",
    "#result = pipe(\"./audio/2024-09-19 16-32-50.mp3\", return_timestamps=True)\n",
    "result = pipe(\"./audio/2024-09-19 16-32-50.mp3\", chunk_length_s=25, batch_size=128)\n",
    "print(result[\"text\"][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c67d5b25-eb54-4118-822c-60447350dfa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T06:09:26.299806Z",
     "iopub.status.busy": "2024-09-30T06:09:26.298923Z",
     "iopub.status.idle": "2024-09-30T06:09:44.999945Z",
     "shell.execute_reply": "2024-09-30T06:09:44.999570Z",
     "shell.execute_reply.started": "2024-09-30T06:09:26.299765Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-voice/.venv/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ok. Donc, on poursuit notre énumération de tous les aspects à prendre en compte pour voir si un projet doit être fait, faisable et rentable et raisonnable en matière d'environnement, etc. Donc, point important, sinon on finit tous en prison, c'est quand même de respect, enfin, pas tous, moi plutôt que vous, mais donc s'il vous plaît, pensez à moi. Respecter la réglementation, avoir une démarche éthique. Donc, en deux mots, comme sur tous les sujets, on pourrait faire trois heures sur chaque site. Quand on parle d'une démarche é éthique c'est pour moi très énervant parce que les gens parlent de souvent de ça en ayant s'ils ont une vague intuition des personnes ne précise exactement de où est le problème qu'est ce qu'on essaye de mettre sous contrôle éthique bon c'est dans de la philo mais c'est pas ce mot donc qu'est ce qu'on essaie de mettre sous contrôle donc je vais essayer juste de lister là un peu sur nos différents types d'IA que les trois types d'IA qu'on a vu tout à l'heure et de lister un petit peu les enjeux qui se posent donc en fait quand les gens parlent de ont des soucis éthiques sur l'IA alors avant d'aller en fait dans les différents types d'IA le point principal c'est que les gens sont sensibilisés sur ces thématiques à cause de sujets qui se posent dans leur vie de tous les jours. Donc les gens, ils ont peur des drones tueurs. Ils ont peur des robots qui vont les remplacer à l'usine. Ils ont peur peut-être des voitures autonomes qui vont être, etc. Et en fait, 99,9 des peurs des gens, ils ont peur de des IA qui vont être et en fait 99% des peurs que les gens ils ont peur de des iac qui vont des trolls russes qui vont déstabiliser nos élections qui ont déstabilisé nos élections en fait ça et quand nous on fait nos présentations et quand on va dans les métiers dans les réseaux etc on a les questions qu'on a sont toujours de cette nature là donc là moi je refuse toujours de répondre là dessus parce que j'ai pas d'avis sur l'IA et l'impact sur la démocratie sur l'IA pour les militaires sur je suis incompétent j'ai un un avis à titre personnel, mais là en tant que professionnel de la Cognitive Factory, je ne discute pas, donc ça c'est important, je ne rentre pas dans une discussion avec vous là-dessus, vraiment je suis incompétent, sincèrement, moi aussi j'ai peur, etc. Mais nous, on parle de l'IA dans la banque, donc l'éthique de l'IA dans la banque, je veux dire autant si on est fabricant de missiles, on se pose des questions, autant si on vend des plans épargne-nogement, bon, qu'est-ce qu'on peut faire de païtique ? c'est quand même plus limité, ça ne veut pas dire qu'il n'y a rien à faire. mais il faut aussi le premier à première chose à faire c'est quand même de ramener le problème à notre contexte dire ok je comprends je suis en empathie avec vos inquiétudes sur l'évolution de la société mais moi personnellement ma contribution là dedans elle va être quasi nulle je peux rien sur les drones tueurs je peux rien sur la désinformation moi je suis dans la banque donc premier pas donc ensuite finalement dans la banque les risques autour d'aléthique ils sont principalement en fait sur les projets de machine learning sur des données numériques sur des données structurées on fait des prédictions Pourquoi? Parce que c'est là qu'on va trouver tous les projets où on vous accorde un crédit ou pas un crédit où vous fait un tarif d'assurance etc et ça on vous fait un prêt pour vos études où on ne fait pas le prêt pour vos études et ça il faut c'est on porte quand on fait ces actes là d'accordé ou de refuser un crédit on a on porte une responsabilité extrêmement lourde vis-à-vis de la société dire la personne qui n'obtient pas son crédit pour être étudiant il peut pas il peut pas obtenir du diplôme il ira petit boule en petit bleu toute la vie peut pas se loger enfin c'est une décision très très lourde de conséquences et donc on doit rendre des comptes c'est normal on doit rendre des comptes à la société quand on fait ça c'est on fait pas ce qu'on veut il faut le faire correctement et donc là les enjeux pour nous ils sont quasiment entièrement sur ces problèmes de données structurées on peut dire du coup cognitive factory on est alors on participe mais on est en général pas directement concerné là dedans et il faut savoir que dans le groupe du coup tous pour toutes ces opérations qui sont particulièrement sensibles c'est vrai dans les autres banques aussi on n'utilise que des modèles de très très simple vraiment des régressions comme vous faisiez en terminale ni plus ni moins parce que il faut que les choses soient entièrement transparentes auditable très clair et donc après il ya tout un débat dans l'entreprise chez nous pour les actes européens dit une régression linéaire c'est dire elle est banquier dans leur ensemble on dit à pour arrêter les conneries quoi c'est des statistiques de niveau collège on va pas dire que c'est de l'intelligence artificielle donc donc chez nous on a appris avec on accord avec toutes les autres banques de la place et on a exclu pour nous toutes ces régressions linéaires logistique etc ça n'est pas de lien donc toute la législation qui sera portalia on l'exclut ceci dit ils sont pourquoi on l'exclut. Ceci dit, ils sont pourquoi on l'exclut de la législation par rapport à l'IA, je viens de vous dire qu'il fallait rendre des comptes à la société. Par contre, ces modèles-là, ils sont soumis à la réglementation bancaire, 1000% donc ils sont audités de tous les bouts par la bce et la cpr tous les six mois ils viennent regarder tous les modèles tout audité donc en fait tous et il ya des enjeux éthiques sur l'utilisation du machine learning dans les banques et dans les assurances mais s'est identifié depuis 20 ans et 30 ans 50 ans je, je ne sais pas, et c'est toujours et c'est sous un contrôle de régulation extrêmement sévère. On n'a pas entendu de la réglementation sur l'IA pour le contrôler. C'est pour ça que les banques, elles disent pas qu'elles veulent se décharger de leurs responsabilités, elles disent écoutez, on est déjà régulés sur régulés sur ce truc, donc ne me pas une couche sert à rien donc là aussi donc et là le risque c'est quoi donc le risque c'est d'avoir c'est un risque de juste d'avoir des modèles qui sont techniquement bien faits si j'ai fait un modèle complètement de travers je vais répondre n'importeimporte quoi au client, donc ça c'est pas acceptable, donc il faut déjà avoir collecté des données représentatives, avoir bien fait son modèle, avoir le surveillé, etc. Mais après, le risque, c'est des risques de discrimination, surtout que les gens entendent. La discrimination, ça veut dire quoi ? que je ne traite pas de manière égale un groupe ou un autre les blonds ont des conditions favorables par rapport aux roues et c'est pas en ligne alors voilà donc est ce que c'est acceptable que les blonds et des meilleures conditions d'assurance que les roumands. A votre avis? Oui? Non, mais... Donc en fait, c'est pas si simple, c'est-à-dire que notre métier, il faut bien... Le métier du banquier, c'est de faire de la discrimination. Parce que si le banquier, il accorde un crédit à tout le monde, la banque elle fait faillite en deux semaines et puis tout le monde a perdu son argent. Les gens, si tu confies ton argent, il faut que ton boulot, c'est justement d'être une banque, de bien gérer le risque et de pouvoir restituer l'argent 20 ans plus tard. Donc notre métier, c'est de discriminer les gens. Et quand on fait de l'assurance, c'est pareil. Si je ne mesure pas bien les risques à l'entrée, je vais faire des tarifs monstrueux. Il faut quand même que chacun paye en fonction quelque part un peu de ces risques. On peut mutualiser, mais il y a des limites. Donc quand on dit « je m'engage à ne pas faire de discrimination », c'est ce qui avait marqué dans notre charte type de l'IA, ça j'ai pas censuré la phrase, fortement il a fallu qu'on se batte, on me dit non, vous ne pouvez pas dire ça. Donc notre critère à nous, c'est'est de dire il faut pas que je fasse de discrimination qui soit pas lié à l'objectif métier du processus si on a un processus qui dit je mesure la probabilité que la personne rembourse alors j'ai le droit de faire des différences entre les gens dans la mesure où je suis capable de prouver que bah oui ils n'ont pas le même niveau de risque et c'est avéré et donc là probablement pour reprendre mon exemple probablement on voit pas bien pourquoi les blonds les roues rembourseraient leurs crédits de manière différente. On n'a pas besoin de faire des tests statistiques pour se dire que, je regarde mon sujet tout à l'heure, il n'y a pas de relation de cause à effet, on ne voit pas bien ce qui pourrait justifier ça. Donc ça, c'est une discrimination qui n'est pas justifiée. Mais il y en a plein d'autres qui le seraient. Et après, je reste sur le même slide, j'anticipe un peu, mais je suis dans mon explication. Après, qu'est-ce que c'est qu'une discrimination justifiée ? Est-ce que chez Euroinformation, on fait de la discrimination à l'embauche? Je ne le regarde dans la salle. Pas très coloré, tout ça. Pas beaucoup de filles. Quand même, on ne ferait pas un peu de discrimination. Comment ça se fait qu'on a une population qui n'est pas tellement diverse là ? qu'est-ce que c'est que ces gens qui recrutent. En fait, si on regarde, j'espère que j'en ai de témoins, vous voyez là. C'est-à-dire, non, il n'y a quand même, voilà, il n'y a pas du tout dans le processus ni dans la culture d'entreprise au contraire etc de discrimination qui soit faite de manière en tout cas on lutte contre et qu'est ce qui se passe en fait ce qui se passe c'est que on recrute à bac plus 5 voire bac plus 8 et ben la discrimination elle s'est fait avant bien avant dans parcours scolaire. Les gens qui sont arrivés à Bac+, malheureusement, ils représentent une diversité moyenne par rapport à toute la population. Mais du coup, est-ce que quand je recrute à Bac plus 5, je suis discriminant ? ? ? Jusqu'où il faut remonter dans le passé pour considérer que c'est plus ma responsable est ce que moi c'est ma responsabilité que les gens soient faits que qu'on a eu tous tout cet écremage pour avant d'arriver à bac plus 5 est ce que je prends ça sur mon dos et là c'est pour c' voilà je prends cet exemple pour montrer que pour tout le monde faut pas discriminer c'est mal ok mais si on essaie de réfléchir vraiment au problème c'est beaucoup moins simple que ça en fait et donc je peux choisir donc là si je prends mon exemple c'est un choix de l'entreprise et quand on dit on fait une charte éthique, etc. Est-ce que je me donne comme mission en tant que crédit mutuel, et ça ne serait pas délirant pour une boîte de la taille, etc. du crédit mutuel, est-ce que je me donne comme mission d'essayer d'aller combattre la discrimination plus tôt ?, et qu'est-ce que je peux faire? Moi je suis dans les associations, nos quartiers ont du talent, chemin d'avenir, les deux, j'ai des fioles dans les deux, peut-être en faisant ça, j'essaye à ma petite échelle de résoudre ce problème de discrimination. Maintenant est-ce que c'est le modèle? Donc tu, c'est aussi pour revenir à... Les gens veulent faire porter aujourd'hui le poids de ces discriminations ou modèle d'IA. C'est quand même un peu... Moi, ça m'énerve. Attends, c'est un peu gros, quoi. Tu as tellement de trucs à traiter dans la société pour... C'est pas le modèle DA qui a discriminé, il se passe plein de trucs. Il y a plein de choses à faire des politiques publiques, tu peux faire plein de choses, c'est pas le modèle DA qui va... Donc tu vois, aujourd'hui, on va mettre les actes et il n'y aura plus de discrimination pas le rapport donc je sais pas si c'est je vais passer cinq minutes quand même pour expliquer ce truc là je sais pas si je vous ai fait sentir on n'a pas le temps de rentrer dans le sujet mais quand on a travaillé sur la charte ça nous a quand même fait beaucoup réfléchir j'ai lu de la littérature sur le sujet c'est vraiment pas simple enfin je c'est vu de loin c'est on a des bons sentiments mais en vrai donc donc c'est pas évident donc ça c'est quand même surtout sur les données on va dire c'est sur les données structurées maintenant et derrière on pourrait dire choses techniques. Moi, je vous ai parlé qu'un peu des grandes idées. Après, vous pour aller chez HD10, comment on mesure les biais et tout. Mais pour moi, la réflexion, elle se situe au niveau social et ce n'est pas les métriques techniques de ton modèle qui vraiment te permettent de réfléchir au sujet. Pour les solutions cognitives de première génération, alors là, on ne voit vraiment pas les enjeux éthiques et là où elles peuvent avoir des biais. Donc il y a un enjeu éthique quand on fait un projet de dire est-ce que c'est une bonne idée d'automatiser ça ou pas ? Est-ce que je déshumanise les choses? Est-ce que je crée du chômage? Donc ça, on peut se poser la question de la légitimité du projet. Une fois qu'on a décidé de faire le projet, qu'est-ce qu' peut faire de terribles dans un dans notre analyseur d'e-mail dans notre assistant virtuel qui pose des problèmes éthiques je passe vite parce que honnêtement on peut trouver des trucs mais je honnêtement on voit pas bien agir à tout ce qui peut arriver c'est que ton assistant virtuel il donne une réponse à la conque à rien avoir avec ta question la société la vie la vie du client du conseiller s'en trouve pas impacté on voit pas bien l'impact donc ok on fait un projet ya confiance, on fait un référentiel, on met en place tout un tas de workflows avec Odile, on a 180 actions pour mettre en place la charte de l'IA, mais ça va concerner 99% des modèles structurés. Avec l'arrivée de l'IA générative la question se pose un peu plus parce que effectivement on peut nous on peut réfléchir à l'impact que ça aurait. On maîtrise moins le résultat. Maintenant, je prends aussi deux minutes ici pour réfléchir aux impacts. Si un modèle vous fait une réponse insultante ou raciste. C'est pas cool. Ça va énerver la personne qu'il reçoit il va se plaindre et c'est maintenant à notre échelle je dis bien à notre échelle sur nos solutions à nous dire quel impact ça sur la société évolution des gens nul pas évidemment la question est très différente si vous êtes méta que vous avez deux milliards d'utilisateurs quotidien et que vous générez des milliards des milliards de de contenus tous les jours. Vous avez évidemment, si vous êtes YouTube avec des algorithmes de recommandation, vous mettez les gens dans des bulles, ok. Donc tous ces problèmes existent, mais je reviens à ma remarque initiale, nous, attention, on est dans un assistant conseiller qui répond à des questions sur le taux du Pell. Donc si quand je lui dis quel taux du pelle, il me fait une remarque raciste, je vais dire, ok, non mais ça m'énervait, je vais tout de suite faire un incident et crier au scandale. Je veux dire, quel est l'impact que ça a derrière? Voilà, je ne vais pas endoctriner des gens, je ne vais pas... Donc, il faut prendre ce recul. Et la deuxième chose, c'est... Je profite aussi de cette remarque-là, et je pense que c'est très important pour que vous ayez des bonnes discussions avec les métiers, c'est cette remarque de... Il faut retirer les biais des modèles. Donc, tout le monde sait que le boulot de gens qui travaillent dans l'IA c'est des cochons ils ont fait des modèles n'importe comment et donc il faut qu'ils retirent les biais absolument or je vous invite à réfléchir à cette phrase dire que on l'a expliqué tout à l'heure entraîner un modèle c'est on lui a donné des exemples et il apprend à les reproduire. Donc en fait, il reproduit, si on a bien fait notre boulot, on peut aussi faire mal de notre boulot, mais là je suppose, je ne suis pas dans un speech technique, je suppose qu'on fait bien le boulot en tant que data scientiste, en tant qu'entraîneur. Si on fait bien le boulot, le que data scientiste en tant que entraîneur si on fait bien le boulot on en a donc le modèle va refléter le monde tel qu'il est donc tel qu'il est il n'est pas jojou mais c'est le boulot d'un modèle faut pas se tromper le boulot d'un modèle c'est il va donc il a lu tout l'internet les ln ils ont eu tout l'internet ils ont eu plein d'horreurs sur l'internet, les LLM, ils ont lu tout l'internet, ils ont eu plein d'horreurs sur l'internet, si vous voulez faire dire des horreurs, ils vont vous faire dire des horreurs, parce qu'elles existent dans le monde et parce qu'il les a vues quand il s'est entraîné. Donc en fait, ce qu'on nous demande, c'est de faire un truc en plusieurs étapes, c'est-à-dire il y ya le monde tel qu'il est et si on a fait notre boulot de data scientiste le modèle il reflète le monde tel qu'il est avec toutes les horreurs qui a dedans ensuite nous on doit se projeter et se projeter sur un monde idéal donc nous on doit définir c'est là qu'on éthique moi je dis ce qui qu'est ce qui est du langage correct acceptable et qu' qu'est-ce qui n'est pas acceptable ? Je définis un monde idéal où il y a des choses que je ne veux plus voir. Je veux corriger le monde tel qu'il est. Et donc, je vais ensuite introduire des biais dans le modèle. Dans un sens mathématique, c'est le contraire que je dois faire. C'est le contraire que je dois biaiser mon modèle pour ne pas qu'ils reflètent le monde mais pour qu'ils reflètent le monde idéal que je voudrais qui existe donc là encore est ce que donner à un modèle la tâche de changer le monde pour aller vers un idéal peut-être qu'il faut un peu se retrousser les manches et faire plus que mettre des biais dans un modèle pour changer le monde mais bon admettons ça peut y contribuer mais bon donc en fait tu vois j'insiste toujours j'attire l'attention aux interlocuteurs sur le fait que il faut ce qu'on doit faire c'est biaiser le modèle c'est pas le débi et c'est point de vocabulaire mais c'est important et du coup je crois que ça arrivait après donc là j'avais dit et du coup qu'est ce qu'on fait quand on aligne les modèles ou quand on les sécurise on parle de la sécurité des modèles ça s'appelle de la censure alors si on le fait la censure qui est fait dans le bon sens c'est de la sécurité là ça après toutes les questions de deux systèmes de valeur et donc quand aujourd'hui on dit attention faut montrer au gouvernement faut surtout pas laisser les gens publiés à large language modèle qui auraient pas été validés tamponnés par l'europe par les états unis par les gouvernements par les machins ce qu'on est en train de dire c'est qu'en fait on a super peur de mettre une connaissance du monde tel qu'il est dans les mains des gens et donc en fait c'est des débats qui nous rappellent beaucoup ce qui se passait à l'époque des lumières de gère est ce que le peuple est assez est ce que c'est pas trop dangereux quand même de laisser le peuple lire des bouquins et ce et découvrir des vérités dérangeantes enfin on peut tout dire dans un bouquin on peut être très subversif enfin c'est quand même dangereux donc il vaut peut-être mieux pas les laisser lire n'importe quoi et donc je sais pas si vous avez comment vous voyez les choses mais il ya moi dès qu'on parle d'aligner de sécuriser etc de supprimer les biais d'un modèle ça me fait peur parce que je pense censure je pense voilà on peut pas laisser les utilisateurs d'un système voir tout ce qui est parce que bon quel usage il pourrait faire une bombe avec ça ok mais bon est ce qu'on laisse accès à la connaissance à tout le monde ou pas donc ça on peut en débattre mais il faut juste voilà je voulais juste reposer le problème dans des termes un peu différents donc ça veut pas dire que on est très content d'avoir un lama 3 points, très bien aligné, qu'on a auquel on a du mal de faire dire les orres. Je ne suis pas en train de dire le contraire. Mais attention, aligner, ça veut dire censurer, en gros. Donc bon. Et donc c'est pour ça que c'est super important là tous les gens qui se battent contre les lois trop restrictives sur la publication qui veulent rendre illégaux les modèles open source de langage etc parce que justement ça donnerait accès, ça donnerait trop de pouvoir au peuple, en gros. Il faut le garder que, l'idée, c'est que ça doit être gardé que... Faites confiance à Sam Altman, Elon Musk, des gens très responsables, très stables, qui ont le bien-être de l'humanité en tête, et eux, ils vont gérer l'IA. Mais ne donnez pas en open source à tout le monde c'est quand même dangereux bon après on pense qu'on veut mais il y a des réglementations non mais je fais exprès de faire un peu de la provoque heureusement qu'il y avait des réglementations qu'en cas de des trucs, sinon on serait tous surveillés dans un état totalitaire et tout. Donc, il faut quand même mettre des barrières. Donc, on a l'EIA Act qui est en train d'entrer en vigueur. Je ne sais pas si vous avez les dates en tête, mais la première tranche de l'EIAAC est entrée en vigueur sur les modèles interdits. Et la prochaine étape, en début d'année prochaine, c'est, donc très bientôt dans 3 à 4 mois, c'est tout ce qui concerne les modèles fondations, les modèles à usage générales, tels c'est écrit dans les Iact, c'est-à-dire les LLM pour nous. On a toute la réglementation de l'AIact qui arrive. Et donc pour être conforme, il faut aussi penser dans nos projets qu'en début de projet, on va passer dans la plateforme IA de confiance et dans tous les filtres pour vérifier qu'on aura prémaché ça pour vérifier que bon on est dans les clous et qu'on est conforme et il faut pas oublier aussi la partie RGDP et on s'est aussi engagé dans notre charte éthique de l'IA de minimiser l'usage des données personnelles, c'est-à-dire qu'en général on n'a pas besoin de données personnelles pour faire nos modèles, c'est juste, Rérat c'est un erreur de conception, je ne vois pas pourquoi on aura besoin de données personnelles dans le modèle, mais vérifions qu'on traite ça correctement. Il peut avoir pour vous donner un exemple quand même des choses qui sont un peu délicates à gérer de notre côté sur tout côté ocr dans notre secteur je fais un service dont le job est d'extraire les informations d'une carte d'identité. Anonyiser les données dans ce service, par définition, c'est pas possible. D'autant que ce service, il doit au passage détecter les fraudes. Donc, imaginez que je prenne des vraies cartes d'identité que je commence à les bricoler pour changer le nom, pour faut anonymiser. Normalement, mon système devrait les rejeter j'ai censé détecter les fraudes. Il y a quelques services et systèmes qu'on fait où il ne faut qu'on travaille même à l'entraînement, même pour les informaticiens, il faut qu'on travaille sur des données personnelles en clair. Je ne peux pas faire autrement. Et donc là, il y a tout un encadrement à mettre pour s'assurer que tous les accès aux données sont tracés justifiés et qu'on puisse en faire l'audit et s'assurer qu'on n'accède aux données que pour des beugais quand c'est vraiment nécessaire il n'y a pas quelqu'un qui est en train de qu'un répertoire partagé et toutes les cartes d'identité de nos clients qui regardent tous les soirs. Donc, encore une fois, je dis des trucs bêtes, mais il faut... Malheureusement, quand on regarde, on trouve des projets dans notre secteur qui, le coup du répertoire partagé avec toutes les cartes d'identité ça a existé soit y attentif à ça donc ça c'est la réglementation la réglementation elle vise à s'assurer voilà qu'on se comporte de manière responsable vis-à-vis de nos concitoyens de la société nos clients de nos etc. Au-delà de ça, le crédit mutuel est une entreprise à mission, et donc, il a pris une série d'engagement qui doivent être respectés dans tous les projets, mais également dans les projets d'IA. Les projets d'IA, il faut le reconnaître, posent un certain nombre de questions, on vient d'en parler juste avant, qui nécessite peut-être de certaines précisions et certaines précautions, en tout cas, il faut rassurer, en fait. Et j'aurais dû d'ailleurs aborder, introduire mon sujet comme ça, c'est-à-dire que la charte éthique de l'IA, elle a mis des engagements en face des inquiétudes des gens. Dans cette démarche, on a collecté les principales inquiétudes des gens, qu'elles soient justifiées ou non. C'est pour ça que des fois, il y a des engagements, il n'y a quasiment rien à faire chez nous, pourquoi on l'a mis? Alors, on l'a mis parce qu'en face il y a une inquiétude. Donc on s'engage, on rassure et puis on s'assure d'être responsable, mais en fait chez nous, il n'y a pas vraiment d'impact et c'est pas important. Et donc on a 26 engagements qui ont été pris. Je rappelle, ça a été validé vraiment par la chambre interfédérale du groupe c'est vraiment c'est signé par l'adg le président et le directeur général donc c'est pas un papier qu'on a fait dans un coin en disant on promet d'être sage laisser nous bosser tranquille donc c'est un truc qui sera qui est géré par le'admission du crédit mutuel, qui sera audité par des auditeurs externes à manière indépendante. Les engagements qu'on a pris là-dedans, on va vraiment les respecter. Et du coup, il y a cinq grandes catégories, protéger les données à l'intimité numérique de nos sociétaires et clients, garantir une utilisation d'IA transparente et documentée, s'engager pour une IA inclusive et respectueuse de l'environnement, développer un contrat de confiance de l'IA avec les collaborateurs et les clients, surtout avec les collaborateurs, et 5e pilier, assurer la robustesse des solutions sécurité et mon comportement tout ça donc là je rentre pas dans les détails mais donc sachez que il ya un projet en cours sera peut-être abouti au moment où vous regardez la vidéo mais un projet en cours qui décline en fait ses engagements en action concrète on doit réaliser il y en a 180 quelque chose comme ça dans le fichier de est géré par odile gilles et le domaine hd chez nous actuellement et ils vont tous être implémentés, mais donc ça veut dire que nos projets devront s'inscrire dans ce cadre-là. On a fait en sorte que ça soit le plus light possible, mais il y a un pense-bête de choses à cocher pour vous assurer que vous êtes bien dans les clous, avec pour l'impact qui sera principalement je dirais sur la partie parce qu'il y a beaucoup de choses qui sont des bonnes pratiques donc ça nous oblige à les mettre mettre la bonne supervision sur les modèles me bien mesurer leur performance mesurer que on n'est pas discriminant sur des choses dont on ne pas etc tout ça faut le faire ça nous obligera à bosser proprement et après c'est ça va aussi nous mettre des exigences de documentation qui sont importantes c'est à dire je fais un projet d'IA ça passera peut-être avec le temps, c'est même sûrement. Moi je pense que dans 3-4 ans, déjà le truc sera retombé, en tout cas sur des projets de notre type. Pour l'instant, les gens sont inquiets et on les comprend. Nous, on peut l'être aussi à titre personnel, donc on les comprend. Donc il faut qu'on les rassure, et donc il va falloir faire beaucoup de documentation, beaucoup de transparence, beaucoup de présentation, etc. autour de chaque projet, tout devra être bien documenté, bien visible, de manière claire et tout pour que les gens s'inquiètent pas. Voilà. Et donc on a la plateforme Watsonics Governance qui est en train de remplacer les petits open pages qu'on avait avant, dans laquelle tous les modèles sont référencés, leur site devis est géré et tous les aspects de conformité sont pointés pour que quand un éditeur vient, il y a unrentiel centralisé dans lequel on dit voilà on écline sur nos modèles ok donc on a déjà eu un peu cette discussion donc tout est tout est sous contrôle maintenant quand même avant de lancer le projet il faut qu'on évalue les coûts, les bénéfices et qu'on priorise. Alors ça, je voulais juste faire quelques remarques là-dessus sur estimer les bénéfices métiers des projets. Il est important de garder toujours à l'esprit la fiabilité réelle des solutions qu'on met en place. Un outil trait de tes mails à ta place. Si il est fiable 90% du temps ça veut dire qu'il ne fait pas réellement gagner de temps il a évité d'écrire le mail mais faut que tu le relises éventuellement tu l'as juste une fois sur deux mais dix fois sur dix faut que tu le relises parce que tu peux pas te permettre une fois sur dix c'est à dire deux fois par jour envoyer une grosse connerie à tes clients parce qu'à ce rythme là on n'a plus de clients dans six mois quoi donc il ya vraiment ce truc où elle est enfin tant qu'on n'est pas capable de d'automatiser à 100% alors les gains sont limités et ça c'est les gens qui c'est pour ça que malgré les prédictions de l'on musc quasiment toutes les années j'ai des slides rigolo où tu vois ces prédictions tous les ans l'année prochaine combien de voitures autonomes voyez dans les rues aujourd'hui malgré tous les progrès d'IA quand elles sont fiables à 99,9% sûrement j'ai pas les stats chez Wemow je pense que c'est eux les plus avancés tu vas à San Francisco ça marche donc c'est toujours pareil on a un monde simple qui est quadrillé avec des rues qui font 500 mètres de large et qui se croisent à angle droit, on arrive à faire des voitures autonomes. À Strasbourg, dans les petites rues de la vieille ville, c'est plus compliqué, avec des cyclistes qui passent partout. Donc, il il ya toujours ce risque de regarder le problème de trop loin et du coup de de faire des estimations comme ça à l'emporte pièce qui est en réalité et dans la réalité on est complètement à côté de la plaque donc là je vois des gens qui étaient dans des équipes chez nous si voilà faisaient les avant-projets avec les métiers, donc pour eux, c'est évident, mais je ne suis pas sûr que tout le monde dans nos équipes ait bien ce point d'attention en tête, c'est évident pour une partie des gens, ça n'est pas pour d'autres. Attention, quand on travaille avec les métiers à l'estimation des bénéfices de bien penser qu'en fait il faut tout vérifier quoi donc est ce qu'on a libéré du temps aux humains un petit peu mais pas beaucoup ça veut pas dire que pour autant on leur a pas apporté du confort et puis quand même un gain de temps mais le gain de temps il est limité quoi donc après si le coût est énorme il faut est ce que le jeu en veut la chandelle bon je reviens pas puisque là du coup je l'ai abordé tout à l'heure quand je vais faire mon estimé je le remettre dans ce contexte là c'est à dire que quand je vais faire l'estimation du coût là je parlais des bénéfices quand je vais faire l'estimation du coût du projet bien pensé perdre les réflexes qu'on a en tant que développeur d'un projet classique le coup va pas être sur les mêmes phases au même moment et dans les mêmes ratios du tout donc ne pas sous estimer le coût de la collecte des données le coup de la création mon système d'évaluation c'est là que va être le gros du coup de mes projets faut que j'en prenne compte donc je en tiens de compte pardon bon pareil donc là c'est c'est la répartition du temps dans les projets qui est différent, mais il faut aussi ne pas reproduire les biais qu'on a, donc là, c'est pour les gens qui sont dans la cognitive factory depuis un moment. Très vite, on crée ses habitudes et on se crée ses propres idées. Et on a pris des réflexes en disant « Quelqu'un vient me demander un projet ». Très bien, il va falloir que je collègue des données, il va falloir que j'entraîne un modèle, il va falloir que je l que je déploie donc il me faut à peu près tant de jours il me faut tel profil il me faut des analystes d'extérieur pensez bien que aujourd'hui avec l'arrivée des modèles fondations d'IA générative peut-être que mon projet c'est trois jours pour trouver le bon prompt et le packager correctement et c'est fini donc c'est on n'est plus et du coup les profils dont j'ai besoin c'est beaucoup moins peut-être d'analyses qui vont collecter plein de données grondues et dedans je dis ça avec prudence parce que c'est toujours une bonne idée de regarder ces données quand même il faut le faire mais je veux dire voilà les ratios d'activité la durée des projets la manière dont on les fait là où se trouve la difficulté change totalement et je vois en tout cas encore maintenant on n'a pas fait cette bascule je vois dans nos équipes ce réflexe on répète qu'on sait on est sûr à la lancer là donc on répète ce qu'on a un peu toujours fait mes projets je les étages fait six mois d'études puis sinon non c'est plus comme ça qu'on fait les projets et pour d'autres raisons sur lesquelles je reviendrai après comprendre l'importance des capacités de calcul à mobiliser dans les différents cas d'utilisation on en a parlé je passe et savoir estimer les coûts pareil on a parlé je passe on sait pas le faire mais il faudra qu'on sache le faire peut-être dernier point là ou dernier point, très important dans cet aspect priorisation des projets. Donc là, ça fait 8 ans qu'on fait de l'IA dans la Cognitive Factory. On a fait toutes les bêtises qu'on pouvait faire et j'espère qu'on apprend. En tout cas, il faut qu'on apprennene. Et donc on a vraiment eu du mal pour différentes raisons, on a fait énormément d'études initiales, d'études d'opportunité, de très grande qualité avec tous les métiers du groupe. On a dans la première phase de l'IA entre 2016 et maintenant sorti des études des civiers comme on les appelait qui faisaient 150 pages, quand on les ressort aujourd'hui, la qualité est stratosphérique. Et là je vois franchement, c'est incroyable. Moi je suis bluffé, on a fait un travail d'une qualité.'est extraordinaire on avait identifié 50 projets chacun était chiffré les bénéfices les coûts les machins les impacts d'accompagnement tout est fait c'est magnifique c'est en plus c'est bien on a et on a dépensé énormément et combien de ces projets ont été faits souvent 0 1 souvent 0 donc la dépense d'énergie nous qu'on a fait pour essayer d'emmener les métiers dans des projets est ce qu'on a réellement réussi à emmener il ya alors on a fait un travail qui payera un jour je veux dire on a fait un travail un moment d'en tirera les bénéfices. Mais quand même, à court terme, c'est déjà moyen terme même. C'est quand même, c'est pas top. Et donc, en fait, il faut qu'on travaille dans cette phase-là avec les métiers de manière un peu différente. On a analysé, on a réfléchi. Et en fait, c'est pas que la méthode est mauvaise, mais'est qu'on l'a trop je vous le résume c'est plus compliqué je vous le résume en une minute donc mais si c'est probablement qu'on a trop concentré les choses c'est à dire que vous prenez un métier pendant trois mois six mois on faisait avec lui une grosse étude on sort plein de trucs on essaye de l'inciter à lancer des projets, puis après on part, on va au métier suivant. Donc on fait des interventions comme ça ponctuelles, comme en dos. Et en fait, il y a besoin de temps pour que les métiers montent en maturité. Pas nos interlocuteurs directs. Nos interlocuteurs directs, s'ils font un projet six six mois avec nous je regarde dorian qui était de l'autre côté elle pourra donner son avis après si tu prends un micro mais ceux qui étaient embarqués dans le projet avec nous c'est eux qui avaient imaginé les projets qui les ont priorisé et c'est eux ils étaient embarqués ils étaient montés à natureté mais l'organisation autour n'a pas eu le temps de bouger, et puis il n'a pas une perspective de long terme, et puis il n'a pas fait... En résumé, dans cette phase-là, on va essayer de proposer, en présentant notre feuille de route pour le prochain plan stratégique la semaine prochaine, de proposer une création vraiment de groupe, je ne sais pas comment on va les appeler le groupe de travail de groupes de contacts permanents qui doivent durer pour les cinq dix prochaines années où on met une personne la cognitive factory une personne du métier concerné une personne du secteur au reinformation qui va avec le métier acme id et assurances et puis peut-être d'autres ont besoin quand ils ont ça mais avec ces gens là qui se voit tous les mois sur dix ans ils se connaissent ils ont bu des bières dix fois ensemble et et on laisse le métier on lui met des choses dans les mains on le stimule externe on laisse le métier faire sa roadmap tout seul c'est lui qui la porte c'est pas nous qui font tirons et c'est un on le revoit tous les mois pour le lâche pas tant qu'il n'a pas fait mais c'est lui qui le ferait on tient pas le stylo pour lui c'est lui qui le fait il a sa roadmap et après il pilote sa roadmap de cas d'utilisation et on fait ça sur la durée pour laisser le temps à la maturité en fait autour de l'IA se diffuser au delà de juste nos interlocuteurs il faut c'est ce qu'on voit ce qui se passe à la dire quoi en ce moment je trouve c'est à dire que vraiment le truc est en train à force de mais bon vous y passez avec angélique du temps là vraiment de les voir de les revoir de les revoirs de les revoir des gens on a fait au moins 20 30 réunions avec la dire co sur les six derniers mois plus même je suis sûr parce qu'il faut tout ce temps là pour que tout le monde, petit à petit, parce qu'il y ait chaque, chez notre organisation, une personne dans un coin, tout le monde peut bloquer un projet. Tout le monde dépend de tout le monde, donc tout le monde a un pouvoir de blocage. Donc il faut emmener l'organisation dans son ensemble. Est-ce qu'en fait, c n'était pas une question aussi de tendance, peut-être pas trop en avance de phase? Justement, maintenant, on parle beaucoup plus. Oui. Et ça rentre? Il y a plein de facteurs. Je crois qu'on en a recensé 25. Je vous en cite un parce que voilà. raison dire que il ya une des grandes raisons aussi pour les cas on a eu du mal c'est que la techno en fait était pas la technologie était limitée tout simplement gère le nous en faisant le meilleur job du monde la qualité de service qu'on amenait était limitée par la technologie qui est en fait entre tu vois on a commencé en 2016 en 2017 il ya eu des transformeurs il y a eu berthe qu'on peut mais qui ont qui ont un peu amélioré la qualité et après entre 2017 et 2023 en gros il s'est rien passé un point de vue technique pour nous il ya plein de choses qui sont passés mais applicable dans notre contexte il ya eu aucun progrès technique quasiment sur huit ans 7 7 ans sur sept ans il ya et c'est d'où la difficulté aussi de nos équipes de se renouveler de rester motivé tout parce que en fait la technologie stagnait et là et du coup ça a été pas linéaire et d'un seul coup il ya eu des progrès linéaire mais ça devient utilisable il ya un effet de marche et des trucs linéaires ce coup par tu montes truc devient utilisable. Et là, maintenant, effectivement, ça va être beaucoup plus facile parce que les métiers ont envie. Là où avant, c'est plutôt, bon, intellectuellement, il disait faut le faire, mais là, il y a des trucs, c'est bluffant, on a envie de l'avoir, quoi. Donc oui, t'as raison. Mais donc, vas-y, alors, attends, t'as pas. Pour poursuivre un petit peu sur cet échange, en fait, oui, la technologie n'était peut-être pas mature et aujourd'hui, elle l'est, donc du coup, il y a des attentes, des métiers, puisqu'il y a les faits de Tcharté-Ptéter, etc. Et de l'autre côté, la corollaire qu'on peut craindre, quand on est en relation avec les métiers, il y a cet effet un peu magicien où il y a beaucoup d'attentes, peut-être trop, et c'est toujours un peu un jeu d'équilibreiste, en leur disant, on peut faire beaucoup de choses, mais on ne va pas tout faire, on n'est pas magicien, il y a cet effet déceptif qui a craint aussi. Oui, oui, on aura d'autres problèmes. Donc encore une fois, c'est vraiment la surface. Mais je... Le message que je voulais passer ici, c'est de... D'avoir, de laisser plus la main au métier le temps. De leur laisser plus le temps. On a voulu trop... Et pourtant, il s'est passé 8 ans. Quand on dit on a voulu trop pousser, c'est... Mais voilà. Dernier point, c'est quand même pour être de bons conseils sur l'identification des cas d'utilisation et gagner du temps. Il faut regarder ce se passe autour de nous faut connaître les tendances du marché donc c'est important de savoir ce que font les autres donc je vais rester là sur le niveau général juste pour dire que si je suis à la cognitive factory dans l'équipe qui s'occupe de la dictée vocale. Il est impératif que je sache exactement ce que font toutes nos banques concurrentes, par exemple, sur la dictée vocale. Quel est le niveau de... Je veux dire, quelle est la meilleure dictée vocale disponible dans le monde, d'ailleurs ? C' quoi c'est celle qu'on a sur nos téléphones c'est quelques qui sont les fournisseurs enfin quel est le marché autour de la dictée vocale moi je sais pas mais par contre ceux qui sont dans l'équipe à dire dans l'équipe dictée vocale doivent avoir cette vision de marché que font les concurrents qui sont les fournisseurs quel est l'état de l'art exter même s'ils sont pas l'équipe algorithme c'est pas une question d'algorithme je veux il faut que je sache aujourd'hui ce qui est possible pas possible je ne peux pas intervenir donc tu vois moi j'étais plutôt dans la présentation on parlait de faire des pourquoi je fais des tests le week-end sur comment coder avec un llm c'est que je suis dans les comités de pilotage toute la semaine là sur eraklion et sur les trucs je sais pas comment parler intelligemment du sujet si j'ai pas au moins essayé tu vois il ya différents outils qui sont à la mode c'est quoi les avantages et les inconvénients des autres qu'est ce-ce qu'on peut faire ou pas faire? Je ne sais pas techniquement comment ils font derrière, je ne suis pas l'équipe algorithme, je ne suis pas dans le détail des... comment les modèles ont été fabriqués, je m'en fous, mais par contre je sais te décrire que voilà comment on se présente les artefacts, les projets dans Claude 3.5 comment ça se présente dans tchat gpté commencer dans curseur commencé dans et dehors machin et il faut que je vous j'ai un peu testé il faut que je sache et que j'ai testé je prends deux minutes c'est c'est important de comprendre que ça fait partie de nos jobs et que dans l'entreprise il n'y a pas de spécialistes derrière nous c'est nous les spécialistes de l'IA et donc on a effectivement un certain nombre d'entre nous des fois le réflexe de se dire bon bah tu me demandes de travailler sur une nouvelle techno, maintenant à l'entreprise de me former parce que c'estest comme ça que ça se passe d'habitude, c'est-à-dire que tu m'amènes un nouvel outil, tu m'amènes une nouvelle techno, merci de me faire une formation. Moi, je ne connais pas, si tu veux que je fasse ce nouveau boulot, il faut me former. Mais donc, il faut que quelqu'un forme. Mais dans l'entreentreprise le quelqu'un qui doit former c'est nous dire que nous il ya personne derrière nous pour venir nous former et là pareil je prends deux minutes parce c'est important de vous prenez bien ça c'est pas moi qui vais venir vous former sur la dictée vocale je connaissaisissais mieux. Les gens de l'équité vocale, c'est les spécialistes dans l'entreprise. Il n'y a personne d'autre. Donc il faut qu'à eux de faire la veille, à eux de connaître tous les outils, à eux de tester, à eux de faire l'investissement qu'il faut. Donc ça veut dire facturer des heuresale ça veut dire ça veut dire les prendre sur son temps de travail on ne demande pas de faire ça trois heures du matin même si ça peut aider mais on demande pas mais par contre si je fais bien faire mon boulot à la cognitive factory c'est c'est être le spécialiste de mon sujet. Je travaille sur l'assistant client, je connais par cœur les assistants clients de la société générale, du crédit agricole et tout. J'ai fait un bilan et je sais quelle fonctionnalité j'offre, quelles fonctionnalités ils offrent, qu'est-ce qu'ils font, qu'est-ce qu'on fait, c'est- c'est quoi leur quelle quels identités ils ont choisi quel temps ils ont choisi par rapport aux nôtres pourquoi donc ça c'est l'idée générale après j'avais quelques points à zoomer donc ce qu'on doit expliquer à chaque fois au niveau de la veille au niveau des éléments il y a la quantité d'innovation d'euros information offrir des services de veille ou en le cas de la volonté de monter que chaque équipe puisse monter en connaissance et est-ce que du coup il y a des passerelles qui ont été déjà envisagées de telle façon à ce que des spécialistes de la veille viennent effectivement ou articuler peut-être un premier jet de sources à identifier, à suivre, à étudier, de telle façon à ce que n'importe qui qui arrive est une base initiale de travail et puis il y aura sa propre liberté derrière, d'identifier des sources complémentaires, des sujets complémentaires à étudier et observer. Parallèlement, question complémentaire,er est une solution qui sont dans d'autres banques, c'est assez compliqué et est forcément assez proche aussi dans certains cas de l'espénage industriel, à certains niveaux d'IIE, en tout cas, l'intelligence économique, et ça implique effectivement qu'on puisse, on a l'avantage d'avoir des prestataires, mais du coup, pensez à nous trouver une multitude de profils avec des gens qui auraient des comptes sur cette banque, ça semble tout, effectivement pas toujours aisé. Alors est-ce que, par hasard, il y a aussi la partie innovation, la possibilité d'accéder ou à intégrer des études de benchmark qui ferait que notre chatbot ait mis à n'importe quelle solution, ait mis à gérer à la phase de tout le monde, mais que toutes les banques participent à aux benchmarkes, elles seraient gérées à bas de bonnes. Merci pour ces questions, comme sur tous les sujets, on pourrait faire une heure sur comment faire la veille. Je vais essayer de répondre de manière succincte. Donc oui, n'hésitez pas à l'innovation factory de renformation a été créée pour faciliter, donc ne fais pas le travail de veille à la place de personne, parce qu'il ne peut pas être spécialiste de tous les sujets, dans toute l'entreprise, dans des demi-liés, donc ce n'est pas possible. Par contre, a pour rôle de vous fournir les outils et de vous l'idée un peu pour le faire, et d'autre part d'assurer une mise en commun. Donc, tous les ans, l'immigration Factory recense les sujets de veille de tout le monde et s'assure qu'on met en commun quand on voit un salon donc on a le portail pépineau dans lequel normalement toutes les informations remontent pour pas faire tous la même veille en même temps en parallèle pour essayer de mettre en commun ceci dit le niveau de service qui est donné donc est donné, ça met le pied à l'étrier un petit peu, ça aide. Ils peuvent effectivement, par exemple, générer le newsletter sur un sujet, mais ça reste une veille quand même assez superficielle par rapport au niveau de profondeur auquel on doit aller. Par exemple, si tu veux faire une veille sur l'IA, l'outil, donc c'est Olivier Guérin, l'interlocuteur, c'est pour ceux qui le connaissent. Vous pouvez lui demander, il va vous créer une newsletter et vous allez recevoir toutes les semaines une newsletter filtrée sur un certain nombre de thèmes et tout ça. Mais ces outils sont branchés sur la presse généraliste. Or, j'ai commencé en introduction en vous disant pour enfin sur l'IA la presse généraliste dit essentiellement des nous on a besoin d'infos un peu plus souvent quand même qui vont au delà de plus sur des publications plus techniques, pas dans le sens, je suis data scientist, mais dans le sens pour spécialistes, plutôt que pour techniques. Donc ça, là-dessus, l'innovation factory pourra pas forcément aider quoi que pour toujours discuter avec eux. Donc oui, il est contacté, mais ça suffira pas. Et ta deuxième question, c'était pour... Est-ce que comme ça si on peut être amené à intégrer un benchmark... Ah? Ah oui, pour avoir accès, pour savoir ce qui se fait sur le marché. Effectivement, on avait quand même réussi à le faire à un moment, par exemple, autour de l'assistant client, on avait juste recensé avec les 150 personnes qui étaient à ce moment-là dans la Cognitive Factory. On n'avait pas eu toutes les banques, mais on en avait eu avait eu beaucoup parce que simplement les uns les autres avaient des comptes à droite à gauche en tant que client pas pour espionner pour de vrai donc ça nous permettait quand même de regarder donc ça c'est soit c'est juste voilà très pragmatique dans le secteur si tu rajoutes l'ocsr on est 250 bientôt 300 avec si tu rajoutes sur 300 personnes tu as toutes les banques donc on y arrive à peu près et ensuite en fait il ya quand même des sources souvent tu vas avoir je sais pas accenture mackinsey gartner tu as plein de cabinets d'analystes et de conseils qui publient des livres blancs, où on regarde ce qui se passe. Et donc ça, l'idée c'est aussi de les attraper. Donc ça, sur ces aspects-là, notamment les rapports au Gartner et tout ça, ça c'est centralisé à l'innovation factorée, qui peuvent nous aider. Mais en réalité, ça vient plus un peu tout seul. Quand tu fais de la veille, tu commences, de la veille, ce n'est pas des trucs extraordinaires. Quand tu fais tes recherches Google ou quand tu es sur YouTube ou, si tu vas regarder tous les jours des sujets sur les assistants clients là tu laisses faire les réseaux sociaux pour toi ils vont ils vont petit à petit à t'alimenter tout seul dans les rapports quand tu es ou si tu es par exemple sur x et que tu suis des gens parce que tu as repéré que c'est des gens qui parlent d'assistants clients que tes vocales etc souvent ils vont reposter des postes d'autres personnes et là tu vois une personne qui revient souvent et tu t'abonnes aussi parce que tu dis tiens maintenant mais c'est intéressant et assez rapidement en fait un truc qui se stabilise où tu as une source d'information qui s'est créée un petit peu de manière à au fil du temps sur les premiers mois ou maintenant s'il ya un rapport d'accenture qui sort sur les assistants clients des banques tu peux être sûr que tu seras en copie tu vas le voir passer sur tes quelque part les algorithmes dont tu les gens que tu suis ou quelqu'un tu vois sur linkedin va le reposter et l'indignes va te le mettre enfin moi c'est ce qui se passe vraiment tu es je te montre ma timeline je ne suis pas particulièrement je ne suis pas partie énormément abonné à des gens mais ce qui ressort dans la timeline c'est que des trucs liés à générative et je n'ai pas grand chose donc ça se fait un peu ça se fait un peu tout comme ça aussi c'est à dire qu'il ya un faux s'appuyer sur les cabinets de conseil qui alors et toujours les lire donc ça me donne l'occasion de dire n'oubliez jamais que le cabinet de conseil qui vous fait un rapport est là pour vous vendre de la prestation. Donc, et donc, il va tous les... Alors là, c'est sans distinction, tous, enfin le dernier qu'on a été accentuant sur mais tous, il te promets entre 30 et 50% de gains sur les développements. Vous y croyez, vous n'y croyez pas. Ils sont là pour vendre un grand projet. Mais donc si on prend le recul nécessaire dans ces rapports et qu'on les chiffres notamment qu'on les prend avec avec un petit sourire il ya souvent plein d'infos très intéressante parce que ça donne quand même les tendances du marché merci pour la question donc dans les question. Donc dans les tendances du marché qu'il faut qu'on soit capable d'expliquer, il faut qu'on explique au métier pourquoi on a choisi de pourquoi c'est si long chez nous de déployer l'IA génératif. C'est tout comme ça qu'ils vont le formuler. Moi, mon copain, chez Orange, ils ont Microsoft Copilot depuis un an et demi déjà. Nous, on est complètement à la rue. Qu'est-ce que vous faites? Tous les autres sont en train de faire des projets depuis des mois, et nous, on n'a toujours rien dans les mains. Qu'est-ce que c'est que ce bordel vous dormez chez eux ils sont nuls enfin vraiment il y en a marre il manque que vous faites tous remplacer là et qu'il y ait des gens un peu efficace donc là il faut qu'on soit capable d'expliquer que c'est super de effectivement siivement, si je fais le choix de... Je vais vous faire mon explication dans l'ordre. Je leur prendra à votre compte ou pas, mais... Donc, le point important, c'est de leur expliquer que, premièrement, il y a une différence qu'on a évoquée tout à l'heure, c'est-à'est à dire que autant dans la phase précédente de l'IA on pouvait fonctionner avec nos machines existantes et nos infrastructures existantes autant dans cette nouvelle phase il faut faire des investissements construire des grosses infrastructures créer des nouvelles piles d'outils logiciels et donc il ya des faut construire toute une nouvelle infrastructure ce qui n'était pas le cas jusqu'à maintenant c'est rare que ça arrive c'est pas intuitif pour les gens ça arrive pas tous les ans dans les systèmes d'information et donc face à cette nécessité de construire toute une nouvelle infrastructure, pour aller vite, le choix simple, facile, en plus on ne sait pas trop comment ça va évoluer, donc c'est de dire, je sors ma carte bancaire, je vais chez Microsoft, Google, Amazon, j'achète un service et une heure après, c'est prêt à l'emploioi je l'utilise et je fais mes premiers projets et ça a pris une heure et c'est super et donc ça a ses avantages c'est pas le lieu de faire comme ça je veux justement du coup je peux expérimenter tout de suite mais point 1 ça veut dire que je vais envoyer tous mes mails clients et toute ma communication client chez Microsoft, Google ou Amazon. Ils promettent qu'ils ne les regardent pas, mais bon, si demain Trump n'a pas envie de les regarder. Deuxièmement, je ne maîtrise rien de l'évolution de ces outils. J'ai construit tous mes projets sur la version de juin de Chat-GPT, sort la version de septembre, il n'y a plus aucun de mes promptes qui marche, toutes mes applications en production sont par terre, j'ai aucun contrôle là-dessus. Je me lève le matin, il n'y a et après rien qui marche et c'est comme ça tous les deux trois mois et donc troisième point le prix par une fois que je suis prisonnier que j'ai fait toute ma production avec tous les collaborateurs et qui ce truc là c'est déjà cher au début. Et donc, nous, conformément à nos engagements d'entreprise à mission, on n'a pas fait ce choix-là. On se projette dans du long terme. On se préoccupe beaucoup de notre souveraineté. On veut maître de notre propre destin on veut pas dépendre des choix d'informe seurs x ou y on va maîtriser nos coûts aussi et on veut protéger la vie privée de nos clients et la sécurité des données et donc on a fait le choix de déployer ça alors d'une part sur nos propres infrastructures et d'autre part avec des modèles ouverts sur lesquels on puisse un peu maîtriser qu'il soit pas une boîte noire on comprend rien et on subit là encore donc ça c'est un choix quand même un certain nombre d'avantages et mais qu'à l'inconvénient qu'il faut créer une infrastructure et donc ça, ça prend un peu de temps. Deuxième point, c'est que parce que les gens lisent aussi les articles de journaux, en tant que professionnels, experts de l'IA à la Cognitive Factory, il faut aussi qu'on connaisse quand même les principaux fournisseurs. les gens qui sont allés dans le cloud sont branchés chez chats de pt chez claude chez jenny et chez grog je sais pas etc etc chez mistral et donc moi il faut que je les connaisse ces fournisseurs faut que je sois allé sur leur site il faut que je vois à peu près que je sache à un instant donné suivant tous les jours ça a évolué quel est le meilleur quel est moins bon je vois moi je me fasse quand même milliers de tout ça. Je vais passer là-dessus. Et donc, pour évaluer ces fournisseurs, ces différents services, il faut que je sache expliquer pourquoi utiliser Monia plutôt que Microsoft Copilot. Donc, en fait les gens ils ont vu, j'ai vu un benchmark et copilot il est 10% au dessus de tel autre truc. Ça veut dire quoi 10% au dessus? Donc il faut savoir expliquer qu'il y a plein de caractéristiques différentes, il y a plein de facettes différentes dans les capacités d'un modèle, son support de différentes langues, la longueur de phrases, la taille de contexte, de documents qu'il est capable de gérer, sa vitesse de production, les ressources qui consomment, il y en a qui sont plus forts en termes de connaissances qui sont plus forts en termes de raisonnement de maths une action spécialisé dans le code il y en a qui sont spécialisés pour extraire des informations générer des résultats structurés il y en a qui sont spécialisés pour appeler des services dans le système d'information il y en a qui sont plus ou moins alignés j'ai mis censuré vous avez des gens qui sont plus ou moins censurés bon donc et etc etc il y en a qui sont plus bavards qui sont moins bavards et donc on peut pas évaluer un modèle sur une seule c'est là qu'on doit être des professionnels et pas des amateurs tu vois premier niveau c'est je connais rien à l'IA et donc on parle de mistral je sais pas ce que c'est deuxième niveau c'est je suis super content j'arrive à la cognitive factory moi je connais je connais le seul dans la salle je connais mistral et j'ai lu l'article dans le journal qui dit qu'ils ont un modèle qui est plus fort que machin et ça c'est encore j'ai lu femme actuelle le week-end c'est bien mais il faut non mais c'est je il faut pas qu'on reste nous il faut pas qu'on reste à ce niveau là c'est pas possible on est des pros donc il faut qu'on ait regardé faut aller faut aller sur le poste de Mistral, il faut regarder, il y a des tableaux où il va vous mettre dans chaque langue pour les maths, pour la littérature, pour la voilà les performances, etc. Et du coup, il y a une évaluation beaucoup plus nuancée. En général, les gens sont à peu près honnêtes, ces cercles scientifiques, ils ne trichent pas trop les chiffres. De toute façon, n'importe qui peut les reproduire et voir si c'est faux. Et va te dire, tu vois, sur le code aujourd'hui, c'est Dipsicvv2.5 qui est le plus fort, c'est Cuen 2.5 depuis hier soir. Quoi n'un codeur 2.5, c'est aujourd'hui le top dans les modèles de code. Ça change tous les jours. Il faut avoir ce niveau de détail et de granularité. Je vais expliquer aux gens qui n'y a pas que la taille qui compte, mais quand même, qui a un lien entre la taille d'un modèle et ses capacités, évidemment, plus il est puissant, plus il est cher, et donc c'est pas forcément pratique, même rouler en Ferrari tous les jours pour aller faire ses courses, c'est pas forcément une bonne idée, même si on a les moyens la zoé peut-être plus pratique et donc après il faut je reviens un peu toujours sur les mêmes thématiques mais il ya cette idée de les performances de nos cas d'utilisation vont, on les augmente pas forcément en mettant un meilleur modèle. Il y a plein d'autres axes à travailler. Et là, on est d'en connaître les tendances du marché. Donc, dans un premier temps, la première année après ChatGPT, tout le monde était sur un modèle, un chat bot un peu isolé du monde et essayer de le rendre meilleur. Mais aujourd'hui, il faut travailler sur tout ce qu'il y a autour. C'est ça qui progresse et qui va bien. Donc, premier point très important, c'est l'aspect contextualisation, c'est-à-dire ramener au moment où je vais solliciter mon modèle, ramener un maximum d'informations de l'environnement pour qu'ils puissent faire quelque chose de pertinent. Par exemple, si on regarde, je parlais tout à l'heure de la génération de code, de faire intervenir un modèle pour t'aider dans des activités liées au développement en code justement tout l'enjeu des approches des différents outils où les gens travaillent je peux je vais pas pouvoir mettre les 400 millions de lignes de code de renformation en ligne dans mon modèle c'est trop gros il faut que je sélectionne pour qui génère quelque chose de pertinent faut qu'ils comprennent ce qui se passe dans quel contexte je suis quels sont mes normes de développement quel langage j'utilise quels sont les morceaux de code qui existent déjà ce que je suis en train d'essayer de faire que j'essaie de mixer tout ça pour avoir les infos nécessaires pour bien générer le morceau de code qui va bien s'insérer dans l'ensemble qui existe déjà et en fait guitaub copilot curseur enfin enfin tous les gens qui travaillent sur des agents autour du code, il y a un du travail sur les modèles, mais souvent, même pas, ils utilisent un modèle qui prenne sur étagère, donc le 3.5, voilà. Eux, ils ont des boîtes financées à hauteur de centaines de millions de dollars pour travailler sur le problème de comment je ramène le bon contexte quand je ramène des bonnes informations de contexte pour avant de solliciter mon modèle toute l'intelligence est là dedans et il ya des modèles pour sélectionner le contexte enfin donc là pour nous c'est très important pour donc si je reviens sur un truc plus concret dans nos assistants pour les collaborateurs paris si la réponse est décevante c'est parce qu'elle est absolument pas personnalisé par rapport à l'utilisateur. Elle dépend pas de qui tu es, de ce que tu faisais dans l'entreprise, de ce que tu faisais juste avant. Si au moment où je pose une question à l'assistant collaborateur, je savais ta fonction, les appliques que tu utilises régulièrement tous les jours, ce que es en train de faire les tables que tu as ouvert dans ton navigateur le mail que tu viens de recevoir et ton dernier coup de téléphone etc et tu poses une question dans ce contexte là si j'avais toutes ces infos là je pourrais te faire une réponse tellement meilleur que d'essayer de comme ça sans contexte sans rien voir sans rien entendre j'essaye de répondre à ta question c'est super dur c'est impossible on n'arrivera jamais et donc je veux aussi insister sur ce point parce que je ne vois pas tellement dans nos projets travailler dans ce sens là enfin on se le dit mais on le fait le fait pas vraiment. On continue à essayer de mieux entraîner le modèle, d'améliorer le Retriever. On fait des trucs techniques. Mais, je veux dire, il n'y a pas que le modèle dans la vie, à chaque fois, on a ce biais.ie manière d'améliorer nos assistants c'est de ramener du contexte pourquoi google mon expérience je généralise pas vous je vais prendre mon expérience quand je vais chez sur google souvent c'est stratosphérique je lui mets deux mots clés de recherche il me sort le truc je comprends même pas comment il fait c'est impossible il a lu dans mes pensées je vais pas exprimer mon besoin il a exaucé c'est merveilleux je fais la même chose dans bing ça marche pas du tout sur leur moteur de recherche microsoft c'est quoi la différence entre les deux? La différence entre les deux, c'est que Google, il a mon historique, il a mon téléphone Android, qui me suit à la culotte toute la journée, il sait tout sur moi, il a tout le contexte. Donc c'est pour ça qu'il me répond bien. C'est pas parce qu'il a un meilleur moteur de recherche. Vous ne croyez pas que Microsoft, depuis le temps, les milliards qu'ils ont mis, ils ont un moteur de recherche croyez pas que microsoft depuis le temps les milliards qu'ils ont mis ils ont un moteur de recherche qui n'est pas à l'état de là il a juste moins de contexte sur toi donc nous on sait ça dans notre vie personnelle mais on n'applique pas forcément dans nos projets donc premier donc premier point vraiment aller chercher du contexte c'est ça qui est important deuxième point c'est une autre manière de donc là j'ai parlé surtout de contexte sur l'utilisateur donc après bah plutôt que d'essayer de faire répondre un modèle c'est d'aller trouver les bons documents et de faire de génération augmentée basé sur les documents recherches ça c'est plus ça c'est plus rentré déjà dans mon projet donc je passe plus vite ce qu'on fait pas du tout aujourd'hui encore enfin très peu donc ça s'appelle des dialogues personnalisés dans l'assistant client mais on fait pas beaucoup dans le seul endroit en gros gros aussi on le fait maintenant dans l'ESVI aussi, mais pour avoir un assistant, des outils pertinents, il faut s'intégrer au système d'information et donc il faut développer comment on fait cette couche de conversation entre le chatbot, l'assistant dans lequel je me trouve et les services du système d'information donc là on a beaucoup de travaux qui sont faits donc sur les six derniers mois les modèles tous de tous les fournisseurs quand ils les rafraîchissent le principal gain c'est ça tiré les ont tous améliorés sur savoir faire un appel de savoir écrire un appel javascript ou ses chars enfin voilà de savoir disons comprendre qu'ils ont des on leur met dans le contexte une liste de fonctions auxquelles ils ont accès, et du coup, on entraîne les modèles à savoir décomposer la tâche qu'on leur demande dans une série d'appels à ces fonctions, et du coup il y a une boucle interne qui se crée, c'est-à-dire que l'utilisateur demande un truc, et le et le lm au lieu de répondre à l'utilisateur il répond un outil l'outil lui donne la réponse puis il ya des allers-retours qui peuvent se faire avec plusieurs outils etc jusqu'à ce que j'ai accumulé tout ce qu'il faut puis à la fin je prends les éléments je les reformule et je réponds donc ça c'est des choses que les modèles même même les petits, maintenant ça faut vraiment de mieux en mieux faire, parce qu'on a créé des jeux de données énormes, tout le monde travaille à entraîner ça. Mais ça, c'est encore aussi la version, le premier niveau de l'interaction avec le système d'information, parce que c'est quand même une tâche très difficile de passer d'une requête de haut niveau en langage métier à une requête détaillée à des web services en langage technique avec plein d'aïd technique dans le truc et tout c'est quasi impossible à faire en plus c'est dangereux tu sais pas ce que le modèle va appeler et donc il ya ibn qui travaille les travaux les plus avancés que j'ai vu c'est dans mes points mensuels avec robietz qui est chez imbm le responsable de la rd dans la gamme de chez watson depuis le début donc on a ceux qui ne le savent pas on a établi des relations un peu personnelles, je dirais, avec certains chercheurs ou côté IBM, parce que ça fait bientôt 10 ans qu'on bosse ensemble. Et donc là, à titre perso, on a un one-to-one tous les mois avec Rob, où il me montre sur son PC portable la R&D qu'il est en train de faire. Et donc là, ça fait plusieurs mois qu'il bosse vraiment sur ce qu'il a nommé maintenant un AI middleware, c'est-à-dire comment je crée cette couche middleware qui va faire l'intermédiaire entre le LLM et le système d'information, dans lequel l'humain va... On n'essaye pas de comprendre tout le système d'information. Donc l'humain va créer une vue, en gros on crée une vue sur le système d'information, si je veux être pas technique, sur un périmètre très précis. Donc par exemple, une vue, ça ne va pas être la partie RH du système d'information. C'est encore beaucoup trop vaste. C'est par exemple, c'est l'exemple sur lequel il bossait chez l'IBM, c'est la période des augmentations, des primes, etc. Et j'ai besoin d'un assistant pour m'aider à prendre mes décisions et à décider de comment on va faire, de qu'est-ce qu'il se fait ? Donc il faut que j'aille consulter plein de systèmes pour savoir, chez IBM, en tout cas ça a très compliqué. Il y a des systèmes où il y a les entretiens, il y a des systèmes où il y a l'historique de carrière, il y a des femmes. Il faut aller interroger plein de systèmes, faire des rapprochements, c'est un bordel, pas possible. Donc, en fait, moi, avec mon middleware, je fais une vision métier cohérente d'une vue du système RH pour travailler à cette tâche précise. J'expose juste ce qu'il faut, comme il faut, dans le bon sens et tout, pour pouvoir faire cette tâche tâche précise ça je le fais à la main c'est un développeur qui le fait et je l'expose de manière facilement consommable et tout pendant l'élème et ensuite le lm il dialogue avec ça donc on est toujours dans un appel de fonction mais il ya eu un travail qui a été fait en amont pour rendre le fin tu as le truc et là dedans j'ai mis ma sécurité j'ai mis les gard rails j'ai enfin voilà donc il ya ça qui arrive cette notion alors je sais pas chez ibm ça rob me dit que c'est pas très loin d'atterrir mais je sais pas dans quel produit comment sous quelle forme mais il ya et donc il travaille là dessus depuis deux ans peut-être vraiment pour trouver le bon protocole la bonne manière pour que ça soit pratique pour les développeurs pratique pour les gens qui font le chat bot et que donc je reviens donc penser à ramener du contexte penser à s'intégrer avec les documents avec le système d'information. Pensez que maintenant aussi, dans les tendances du marché, on va pouvoir exploiter l'aspect multimodal des choses. Pour l'instant, on est très texte omni. Des fois, un bon dessin, tu vois, d'ailleurs, je ne dessine plus là dessin tu vois d'ailleurs je dessine plus là j'ai images texte voix les trois sont les trois sont intéressants et donc on a des modèles open source qui commencent à être vraiment bon et disponible sur donc là images et texte il ya un modèle instral la piquestral qui est sorti il ya deux semaines qui est déjà intégré dans watsonics et a d'ailleurs guillaume par exemple qui est un modèle open source assez petit, 12 milliards de paramètres, qui fait très bien images plus textes, qui est fort sur plein plein de tâches. Et donc ça, ça peut nous permettre, pour l'instant, on n'a pas d'offres autour de ça, comme on a beaucoup de documents à gérer. Et donc exemple cas d'utilisation donc en fait ce que ce qui se passe aujourd'hui c'est que les différentes modalités donc vous savez vous savez pas mais ça c'est dans la formation plus approfondie mais le texte simplement le découpe en petits morceaux en mots ou sous-mot c'est des tokens et puis on a une représentation pour chaque token en plus on fait nos traitements de deep learning là dessus et en fait dans ces modèles multimodaux l'image elle va être découpée en token comme le texte avec la même représentation sémantique un bout d'image il a un sens comme un bout de mot à un sens quoi et puis en les relations entre elles ont un sens aussi mais et donc en fait quand je vais être dans pitts tral là je vois j'ai mis une image un texte une image un texte ça c'est mon prompt qui mélange images et textes et en fait ça fait une suite de tokens, il y a des tokens d'images, il y a des tokens de texte, mais ils représentent le même monde. Le texte parle des images, les images contiennent du texte, enfin c'est la même chose. Et donc, on peut, de la même manière qu'aujourd'hui, on fait des moteurs de recherche où on a pris des documents, qui étaient une image, on a fait de l'OCR, on a extrait du texte, on indexe le texte. Du coup, on a perdu quand même beaucoup d'infos sur la mise en page, donc au moins on peut chercher dans le texte. Là, aujourd'hui'hui avec ça on va pouvoir faire du moteur de recherche sur directement sur les images du document parce que on pourra chercher le texte qui a l'enlant des documents mais aussi du coup sur la mise en page et tout ça sera directement l'image elle-même avec le texte enfin avec l'infos du texte qui est dedans ça veut dire que par exemple moi je veux faire un ppt pour le prochain comité de pilotage et j'ai en stock des tonnes et des tonnes et des tonnes de ppt et je retrouve jamais je sais que sais que j'ai déjà fait ce slide, mais où il est? En fait, il y avait un dessin avec marqué dessus, etc. Maintenant, on pourra dire, là, mon rêve, c'est que vite, on est ce système que je puisse indexer tous les PPT du secteur. Je vais dire, là où j'avais fait un dessin en trois couches de la plateforme cognitive avec première couche comme ça deuxième couche comme ça et hop le truc me sort le slide j'ai gagné une demi-heure donc ça voilà je vais multimodalité et après il y a la voix donc là il y a alors évidemment il y a toujours la possibilité de faire d'indicterale, d'avoir du texte et après on traite le texte. Mais aujourd'hui, on a des modèles, les premiers qui font des tokens de voix directement mélangés avec le reste, qui ne sont pas, on ne passe pas par une phase texte. On va directement la voie, une représentation sémantique qui du coup encode aussi la tonalité, l'intention de la voie. Donc c'est plus riche. Et là c'est ce matin, ou hier soir aussi, là il y a le... Je ne sais pas si vous avez essayé. Kutai, la start-up française qui fait ça. Qui l'a mis en open source... Ils ont mis tout leur système, Kutai, dont ils avaient fait la démo. En open source, licence la plus permissive du monde. Donc c'est des trucs, pareil, c'est en plus, je crois qu'il fait 7 milliards de paramètres, un truc comme ça. Ça marche pas mal. Donc c'est des choses avec lesquelles on va on va commencer pouvoir jouer et maintenant si je réunis donc toutes ces tendances donc j'ai j'arrive à la notion d'agent donc là si vous n'êtes pas dans les circuits si vous n'êtes pas encore très branché sur les circuits génératives plutôt recherche et tout ça vous avez peut-être encore échappé à la vague mais je vous préviens que dans le journal à partir de la semaine prochaine vous allez pendant un an vous allez vous bouffer des titres avec des agents partout non-stop c'est la vague est là et donc c'est de réunir tous ces morceaux là un agent c'est un chatbot qui peut agir comme son nom l'indique donc j'ai c'est là que je vois les différents usages de différents lm j'ai un lm qui va me faire l'interface de chat j'ai un lm plus gros plus balais ce qui va me faire une planification je vais donner une tâche un peu de haut niveau donc il va me faire une planification en fonction de cette planification il va déterminer les outils du système d'information qu'il doit appeler il va exécuter va faire ses appels et l'orchestrateur au mieux là et ensuite il va avoir un résultat qui lui revient en fonction de ce résultat il va décider de l'action suivante petit à petit va enrichir ou aller chercher des informations dans une mémoire de le court terme de lanterne et donc là ça veut dire que je suis plus dans mes projets dans un système uniquement de chatbaud de chat qui sait vraiment je donne des instructions un agent qui travaille pour mon compte et donc là dessus on a eu or donc on parle des meetings qu'on a un point one to one avec les gens d'ibm donc là dessus on a eu un meeting pas plus tard que hier soir avec avec le chief scientist ibm et qui travaille à faire un agent s'appelle agent toit no et qui travaille à faire un agent, qui s'appelle Agent One en l'instant, et ils travaillent sur un benchmark qui est le suivant. Ils ont ramassé, ils ont constitué un jeu de données, ils ont pris des repository, des bases de code qui sont open source qui sont disponibles, sous GitHub, l'endroit où tous les projets open source sont partagés. Donc ils ont pris une base de code, dont certaines très grosses. Ils ont regardé des tickets, des demandes d'évolution ou des tickets d'incident. Et le but du benchmark, c'est qu'on donne à l'agent le ticket incident et il crée la pool request, il crée les changements de code qu'il faut et les commentaires, à l'explication et la doc pour je corrige, je traite la demande d'évolution ou la demande d'incident donc le réveil pas on n'est pas à 100% de perf sur ce benchmark là mais il ya un taux de performance aujourd'hui de 30% à peu près à l'état de l'art sauf des systèmes de recherche qui sont pas pratiques qu'on peut pas utiliser en pratique mais sur ce benchmark on sait résoudre à peu près 30% c'est l'état de l'art et donc là il ben nous promet pour le mois d'octobre d'avoir en open source un agent complètement open source qui atteint ce niveau de 30% des tickets sur les projets open source. C'est juste pour vous dire que cette notion d'agent, pourquoi on va en parler beaucoup et pourquoi on en parle, c'est que ça prend forme. Ça commence à marcher, ça coûte cher, c'est difficile, etc. Ça commence à marcher, je n'ai pas le temps d'écrire, et donc voilà, et juste pour votre info également, donc ça c'est pas arrêté-conclut, mais on a sur la table, du coup, depuis un jour, une proposition de la part de rougir justement d'ibm d'être le client zéro de cette fonctionnalité de le mettre au point avec ce qui serait très chouette franchement alors encore une fois ça va pas remplacer les développe Sortez pas de cette salle ou de cette vidéo en disant ça y est, on a des agents pour remplacer les développeurs. C'est sur un benchmark artificiel, on le plafonne à 30%, il faut relire, et quand l'agent travaille, l'humain est beaucoup sollicité. C'est-à-dire c'est pas un agent, il part, il fait son truc il finit tout seul il manque il sollicite beaucoup l'humain pour l'aider pour lui donner des infos pour valider son plan et mais voilà c'est lui qui me prend en main les choses quoi on est sur la partie 1 peut-être juste avant de couper tu peux revenir et juste peut-être faire un petit contenant sur la partie 1. Je peux peut-être juste avant de couper le long. Tu peux revenir et peut-être faire une petite compétence à partir une en disant voilà quel était l'objectif derrière cette première partie. Ouais. Je pense que tu viens de défiler. Ouais. Je crois que je revienne... Slate 3. Non mais va pas... Non, pas aller jusqu'à la hausse. Ah non, on va pas aller jusqu'à laute. Je vais revenir. Merci parce que je vais trouver le... Ouais, donc on arrive effectivement à la fin de la première partie. Donc vous voyez qu'elle était particulièrement longue. on le savait quand on l'a préparé c'était pas une surprise c'est vraiment le point les autres parties sont un peu différentes là c'est vraiment on voulait essayer de mettre à un endroit et enregistrer une fois pour toutes plein d'idées sur plein de sujets différents qui ne sont pas, vous voyez, qui ne font pas de vous des spécialistes du sujet, mais qui vous aident, je l'espère, en tout cas, et on va tester si c'est le cas, pour aider en fait tous les collaborateurs de la Cognitive Factory à aborder un nouveau projet qui se présente avec les bons réflexes en se posant les bonnes questions faire en on entame le truc au niveau un peu naïf spontané bon sens et non mais en fait nous on est les professionnels et donc on a quand même regardé là c'est deux heures à peu près de contenu que représente cette première partie. Et voilà, quand on nous parle d'alignement de sécurité, de correction de biais, on le comprend de manière un peu plus approfondie, un peu différente et on voit un petit peu les questions qu'on va se devoir se poser, les études qu'on va devoir faire, peut-être les articles qui seraient intéressants de lire pour se poser des questions qu'en première surface, on ne dit, bah oui, il ne faut pas faire de discrimination, on ne se pose pas de questions. Là, l'idée c'était d'avoir ce premier niveau. Et donc dans la suite, dans les parties suivantes, on va être plus après rentré petit à petit dans les informations à avoir pour pouvoir vraiment réaliser nos projets sans aller jusqu'à trop d'expertise mais pareil dans le même état d'esprit pour balayer un peu les grandes idées et pour savoir pour avoir quand même un certain niveau d'autonomie dire pas toujours se tourner vers les équipes de spécialistes notamment l'équipage des indices spécialiste des algorithmes pour faire son projet il ya un tout un tas de réflexes et de grandes idées qu'on peut avoir soit et on peut faire une première passe avant d'aller chercher\n"
     ]
    }
   ],
   "source": [
    "# result = pipe(\"./audio/2024-09-19 15-03-35.mp3\", return_timestamps=True)\n",
    "result = pipe(\"./audio/2024-09-19 15-03-35.mp3\", chunk_length_s=25, batch_size=128)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d94411-e680-405a-9343-f5069b39ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pipe([\"./audio/2024-09-19 15-03-35.mp3\",\"./audio/2024-09-19 16-32-50.mp3\"], batch_size=2)\n",
    "for result in results: print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5206c4ef-00f8-4950-a039-61073d111d79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T14:42:59.105830Z",
     "iopub.status.busy": "2024-09-29T14:42:59.105423Z",
     "iopub.status.idle": "2024-09-29T14:54:02.698117Z",
     "shell.execute_reply": "2024-09-29T14:54:02.697721Z",
     "shell.execute_reply.started": "2024-09-29T14:42:59.105802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 2024-09-24 12-27-09.mp3 (sequential) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-voice/.venv/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "- 2024-09-24 12-27-09.mp3 (chuncked) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-voice/.venv/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Saved: /workspace/wordslab-voice/audio/2024-09-24 12-27-09_sequential.txt\n",
      "Saved: /workspace/wordslab-voice/audio/2024-09-24 12-27-09_chunked.txt\n",
      "- 2024-09-24 10-46-15.mp3 (sequential) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-voice/.venv/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "- 2024-09-24 10-46-15.mp3 (chuncked) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-voice/.venv/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Saved: /workspace/wordslab-voice/audio/2024-09-24 10-46-15_sequential.txt\n",
      "Saved: /workspace/wordslab-voice/audio/2024-09-24 10-46-15_chunked.txt\n",
      "- 2024-09-24 11-39-13.mp3 (sequential) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-voice/.venv/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "- 2024-09-24 11-39-13.mp3 (chuncked) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-voice/.venv/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Saved: /workspace/wordslab-voice/audio/2024-09-24 11-39-13_sequential.txt\n",
      "Saved: /workspace/wordslab-voice/audio/2024-09-24 11-39-13_chunked.txt\n",
      "- 2024-09-26 15-35-04.mp3 (sequential) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-voice/.venv/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "- 2024-09-26 15-35-04.mp3 (chuncked) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-voice/.venv/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Saved: /workspace/wordslab-voice/audio/2024-09-26 15-35-04_sequential.txt\n",
      "Saved: /workspace/wordslab-voice/audio/2024-09-26 15-35-04_chunked.txt\n",
      "- 2024-09-19 13-34-26.mp3 (sequential) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-voice/.venv/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "- 2024-09-19 13-34-26.mp3 (chuncked) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-voice/.venv/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Saved: /workspace/wordslab-voice/audio/2024-09-19 13-34-26_sequential.txt\n",
      "Saved: /workspace/wordslab-voice/audio/2024-09-19 13-34-26_chunked.txt\n",
      "- 2024-09-19 14-13-55.mp3 (sequential) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-voice/.venv/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "- 2024-09-19 14-13-55.mp3 (chuncked) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-voice/.venv/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Saved: /workspace/wordslab-voice/audio/2024-09-19 14-13-55_sequential.txt\n",
      "Saved: /workspace/wordslab-voice/audio/2024-09-19 14-13-55_chunked.txt\n",
      "- 2024-09-26 15-04-20.mp3 (sequential) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-voice/.venv/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "- 2024-09-26 15-04-20.mp3 (chuncked) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-voice/.venv/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Saved: /workspace/wordslab-voice/audio/2024-09-26 15-04-20_sequential.txt\n",
      "Saved: /workspace/wordslab-voice/audio/2024-09-26 15-04-20_chunked.txt\n",
      "- 2024-09-24 10-10-14.mp3 (sequential) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-voice/.venv/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "- 2024-09-24 10-10-14.mp3 (chuncked) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-voice/.venv/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Saved: /workspace/wordslab-voice/audio/2024-09-24 10-10-14_sequential.txt\n",
      "Saved: /workspace/wordslab-voice/audio/2024-09-24 10-10-14_chunked.txt\n",
      "- 2024-09-24 11-14-23.mp3 (sequential) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-voice/.venv/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "- 2024-09-24 11-14-23.mp3 (chuncked) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-voice/.venv/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Saved: /workspace/wordslab-voice/audio/2024-09-24 11-14-23_sequential.txt\n",
      "Saved: /workspace/wordslab-voice/audio/2024-09-24 11-14-23_chunked.txt\n",
      "- 2024-09-19 15-03-35.mp3 (sequential) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-voice/.venv/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "- 2024-09-19 15-03-35.mp3 (chuncked) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-voice/.venv/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Saved: /workspace/wordslab-voice/audio/2024-09-19 15-03-35_sequential.txt\n",
      "Saved: /workspace/wordslab-voice/audio/2024-09-19 15-03-35_chunked.txt\n",
      "- 2024-09-19 16-32-50.mp3 (sequential) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-voice/.venv/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "- 2024-09-19 16-32-50.mp3 (chuncked) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-voice/.venv/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Saved: /workspace/wordslab-voice/audio/2024-09-19 16-32-50_sequential.txt\n",
      "Saved: /workspace/wordslab-voice/audio/2024-09-19 16-32-50_chunked.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Specify the directory containing mp3 files\n",
    "directory = '/workspace/wordslab-voice/audio'\n",
    "\n",
    "# Use glob to get all .mp3 files in the directory\n",
    "mp3_files = glob.glob(os.path.join(directory, '*.mp3'))\n",
    "\n",
    "# Loop through each mp3 file\n",
    "for mp3_file in mp3_files:\n",
    "    # Get the base name of the file (without directory path)\n",
    "    base_name = os.path.basename(mp3_file)\n",
    "    \n",
    "    # Replace the .mp3 extension with .txt to create a new filename\n",
    "    sequential_txt_file = base_name.replace('.mp3', '_sequential.txt')\n",
    "    chunked_txt_file = base_name.replace('.mp3', '_chunked.txt')\n",
    "    \n",
    "    # Full path of the text file to be written\n",
    "    sequential_txt_file_path = os.path.join(directory, sequential_txt_file)\n",
    "    chunked_txt_file_path = os.path.join(directory, chunked_txt_file)\n",
    "\n",
    "    # Transcribe audio with two methods\n",
    "    print(f\"- {base_name} (sequential) ...\")\n",
    "    sequential_txt = pipe(mp3_file)[\"text\"]\n",
    "    print(\"OK\")\n",
    "    \n",
    "    print(f\"- {base_name} (chunked) ...\")\n",
    "    chunked_txt = pipe(mp3_file, chunk_length_s=25, batch_size=32)[\"text\"]\n",
    "    print(\"OK\")\n",
    "    \n",
    "    # Write a text file with the same name as the mp3 file\n",
    "    with open(sequential_txt_file_path, 'w') as file:\n",
    "        file.write(sequential_txt)\n",
    "    print(f\"Saved: {sequential_txt_file_path}\")\n",
    "    \n",
    "    with open(chunked_txt_file_path, 'w') as file:\n",
    "        file.write(chunked_txt)\n",
    "    print(f\"Saved: {chunked_txt_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bb434f-8120-4943-bb97-1f5d46c32c2f",
   "metadata": {},
   "source": [
    "## Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "756cd710-6f24-4891-83d6-a77fc20cb2cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T19:00:46.572835Z",
     "iopub.status.busy": "2024-09-29T19:00:46.572384Z",
     "iopub.status.idle": "2024-09-29T19:00:46.584300Z",
     "shell.execute_reply": "2024-09-29T19:00:46.583548Z",
     "shell.execute_reply.started": "2024-09-29T19:00:46.572802Z"
    }
   },
   "outputs": [],
   "source": [
    "from importlib.metadata import version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046cf261-6ed1-4537-96e5-ce8a529f1be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ca4bac1-cabf-42ca-b404-a0c477362ea4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T19:01:25.151192Z",
     "iopub.status.busy": "2024-09-29T19:01:25.150803Z",
     "iopub.status.idle": "2024-09-29T19:01:25.175241Z",
     "shell.execute_reply": "2024-09-29T19:01:25.174795Z",
     "shell.execute_reply.started": "2024-09-29T19:01:25.151160Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.6.2'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('vllm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "944988dc-a3e0-4449-8e88-a7f2ae20cc89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T06:24:06.185916Z",
     "iopub.status.busy": "2024-09-30T06:24:06.185541Z",
     "iopub.status.idle": "2024-09-30T06:24:06.191144Z",
     "shell.execute_reply": "2024-09-30T06:24:06.190333Z",
     "shell.execute_reply.started": "2024-09-30T06:24:06.185891Z"
    }
   },
   "outputs": [],
   "source": [
    "test_models = {                                                                    # OpenLLM leaderboard score\n",
    "    \"llama-3.1\" : \"meta-llama/Meta-Llama-3.1-8B-Instruct\",                         # 100.0 %\n",
    "    \"llama-3.1:w8a16\" : \"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a16\",  # 99.8 %    \n",
    "    \"qwen-2.5\" : \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    \"qwen-2.5:w8a16\" : \"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int8\",\n",
    "    \"qwen-2.5-14b:w8a16\" : \"Qwen/Qwen2.5-14B-Instruct-GPTQ-Int8\",\n",
    "    \"qwen-2.5-14b:w4a16\" : \"Qwen/Qwen2.5-14B-Instruct-GPTQ-Int4\",\n",
    "    \"qwen-2.5-32b:w4a16\" : \"Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d09510a5-b598-4049-9b90-0b4cf47f803e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T06:24:07.038957Z",
     "iopub.status.busy": "2024-09-30T06:24:07.038570Z",
     "iopub.status.idle": "2024-09-30T06:24:07.043815Z",
     "shell.execute_reply": "2024-09-30T06:24:07.043366Z",
     "shell.execute_reply.started": "2024-09-30T06:24:07.038930Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def format_prompt(messages, model):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5811ad7e-3e58-454e-8251-81fe7c0464d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T06:24:07.336191Z",
     "iopub.status.busy": "2024-09-30T06:24:07.335814Z",
     "iopub.status.idle": "2024-09-30T06:24:07.342458Z",
     "shell.execute_reply": "2024-09-30T06:24:07.341610Z",
     "shell.execute_reply.started": "2024-09-30T06:24:07.336164Z"
    }
   },
   "outputs": [],
   "source": [
    "# Authenticate VLLM with Huggingface Hub\n",
    "import os\n",
    "\n",
    "with open(\"/workspace/hftoken\", 'r') as file:\n",
    "    myhftoken = file.read().strip()\n",
    "\n",
    "os.environ[\"HF_TOKEN\"]=myhftoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5770c36a-fbb2-4bdf-971f-3500a7a2d496",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T06:24:07.650950Z",
     "iopub.status.busy": "2024-09-30T06:24:07.650394Z",
     "iopub.status.idle": "2024-09-30T06:24:08.187089Z",
     "shell.execute_reply": "2024-09-30T06:24:08.186685Z",
     "shell.execute_reply.started": "2024-09-30T06:24:07.650927Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(test_models[\"llama-3.1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba8156d9-341d-4b7b-94fd-b270ad5a4583",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T06:24:08.187738Z",
     "iopub.status.busy": "2024-09-30T06:24:08.187640Z",
     "iopub.status.idle": "2024-09-30T06:24:08.395269Z",
     "shell.execute_reply": "2024-09-30T06:24:08.394811Z",
     "shell.execute_reply.started": "2024-09-30T06:24:08.187731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read: /workspace/wordslab-voice/audio/2024-09-24 11-14-23_chunked.txt 21446 chars => 5581 tokens\n",
      "Read: /workspace/wordslab-voice/audio/2024-09-24 10-10-14_sequential.txt 32194 chars => 8187 tokens\n",
      "Read: /workspace/wordslab-voice/audio/2024-09-26 15-35-04_sequential.txt 60032 chars => 15458 tokens\n",
      "Read: /workspace/wordslab-voice/audio/2024-09-24 10-46-15_chunked.txt 24866 chars => 6266 tokens\n",
      "Read: /workspace/wordslab-voice/audio/2024-09-24 10-10-14_chunked.txt 32518 chars => 8167 tokens\n",
      "Read: /workspace/wordslab-voice/audio/2024-09-24 11-39-13_sequential.txt 38849 chars => 9949 tokens\n",
      "Read: /workspace/wordslab-voice/audio/2024-09-19 14-13-55_chunked.txt 46405 chars => 11883 tokens\n",
      "Read: /workspace/wordslab-voice/audio/2024-09-19 13-34-26_chunked.txt 33588 chars => 8741 tokens\n",
      "Read: /workspace/wordslab-voice/audio/2024-09-19 15-03-35_chunked.txt 80679 chars => 20942 tokens\n",
      "Read: /workspace/wordslab-voice/audio/2024-09-24 11-14-23_sequential.txt 20560 chars => 5263 tokens\n",
      "Read: /workspace/wordslab-voice/audio/2024-09-24 11-39-13_chunked.txt 40042 chars => 10240 tokens\n",
      "Read: /workspace/wordslab-voice/audio/2024-09-24 12-27-09_chunked.txt 37719 chars => 9654 tokens\n",
      "Read: /workspace/wordslab-voice/audio/2024-09-26 15-04-20_chunked.txt 26344 chars => 6692 tokens\n",
      "Read: /workspace/wordslab-voice/audio/2024-09-19 14-13-55_sequential.txt 44589 chars => 11508 tokens\n",
      "Read: /workspace/wordslab-voice/audio/2024-09-26 15-35-04_chunked.txt 61400 chars => 15618 tokens\n",
      "Read: /workspace/wordslab-voice/audio/2024-09-19 13-34-26_sequential.txt 32892 chars => 8664 tokens\n",
      "Read: /workspace/wordslab-voice/audio/2024-09-24 10-46-15_sequential.txt 24118 chars => 6071 tokens\n",
      "Read: /workspace/wordslab-voice/audio/2024-09-24 12-27-09_sequential.txt 36433 chars => 9335 tokens\n",
      "Read: /workspace/wordslab-voice/audio/2024-09-19 16-32-50_sequential.txt 22992 chars => 6003 tokens\n",
      "Read: /workspace/wordslab-voice/audio/2024-09-26 15-04-20_sequential.txt 25719 chars => 6612 tokens\n",
      "Read: /workspace/wordslab-voice/audio/2024-09-19 15-03-35_sequential.txt 78155 chars => 20479 tokens\n",
      "Read: /workspace/wordslab-voice/audio/2024-09-19 16-32-50_chunked.txt 23829 chars => 6187 tokens\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "\n",
    "directory = '/workspace/wordslab-voice/audio'\n",
    "\n",
    "text_files = glob.glob(os.path.join(directory, '*.txt'))\n",
    "\n",
    "textes = []\n",
    "\n",
    "# Loop through each mp3 file\n",
    "for text_file in text_files:\n",
    "    with open(text_file, 'r') as file:\n",
    "        content = file.read()\n",
    "        textes.append(content)\n",
    "        print(f\"Read: {text_file} {len(content)} chars => {len(tokenizer(content)['input_ids'])} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "013b9474-f5f5-4432-88e3-eff1ec76e74f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T06:24:11.784682Z",
     "iopub.status.busy": "2024-09-30T06:24:11.784284Z",
     "iopub.status.idle": "2024-09-30T06:24:15.238944Z",
     "shell.execute_reply": "2024-09-30T06:24:15.238594Z",
     "shell.execute_reply.started": "2024-09-30T06:24:11.784656Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "def vllm_load(model):    \n",
    "    llm = LLM(model, gpu_memory_utilization=0.99, max_model_len=49152) # kv_cache_dtype=\"fp8\"\n",
    "    llm._model = model\n",
    "    return llm\n",
    "\n",
    "def vllm_generate(instruction, text, llm):    \n",
    "    message = instruction + text\n",
    "    prompt = format_prompt( [{\"role\": \"system\", \"content\": \"Tu es un assistant utile et professionnel qui répond toujours en français. Tu es spécialisé dans la mise en forme de texte pour le rendre plus lisibile et synthétique. Tu maîtrise parfaitement la grammaire et l'expression écrite, et tu es un expert en informatique. Tu n'invente jamais aucun élément, tu t'astreins à toujours reformuler exactement les phrases qu'on te fournit sans rien ajouter ni enlever à leur sens.\"},\n",
    "    {\"role\": \"user\", \"content\": message}], llm._model)\n",
    "    sampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=len(text)+1024)\n",
    "   \n",
    "    start_time = time.time()  # Record the start time\n",
    "    outputs = llm.generate(prompt, sampling_params)\n",
    "    end_time = time.time()  # Record the end time\n",
    "\n",
    "    generated_text = outputs[0].outputs[0].text\n",
    "    tokenscount = len(outputs[0].outputs[0].token_ids)\n",
    "    tokens_per_sec = tokenscount/(end_time-start_time)\n",
    "    \n",
    "    return generated_text,tokens_per_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60f7a391-c90f-4e3c-8735-6a094c7b8b91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T06:24:15.240101Z",
     "iopub.status.busy": "2024-09-30T06:24:15.239587Z",
     "iopub.status.idle": "2024-09-30T06:24:31.865967Z",
     "shell.execute_reply": "2024-09-30T06:24:31.865666Z",
     "shell.execute_reply.started": "2024-09-30T06:24:15.240093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-30 08:24:15 arg_utils.py:930] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 09-30 08:24:15 config.py:1010] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 09-30 08:24:15 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a16', speculative_config=None, tokenizer='neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a16', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=49152, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a16, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 09-30 08:24:16 utils.py:747] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 09-30 08:24:16 model_runner.py:1014] Starting to load model neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a16...\n",
      "INFO 09-30 08:24:16 compressed_tensors_wNa16.py:84] Using MarlinLinearKernel for CompressedTensorsWNA16\n",
      "INFO 09-30 08:24:17 weight_utils.py:242] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "021a5a1662bf414ca8857b305c5ee779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-30 08:24:22 model_runner.py:1025] Loading model weights took 8.4927 GB\n",
      "INFO 09-30 08:24:22 gpu_executor.py:122] # GPU blocks: 7096, # CPU blocks: 2048\n",
      "INFO 09-30 08:24:22 model_runner.py:1329] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-30 08:24:22 model_runner.py:1333] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-30 08:24:31 model_runner.py:1456] Graph capturing finished in 9 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = vllm_load(test_models[\"llama-3.1:w8a16\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78d5a028-82e3-441f-bc89-2087db54f450",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T06:24:31.866487Z",
     "iopub.status.busy": "2024-09-30T06:24:31.866398Z",
     "iopub.status.idle": "2024-09-30T06:24:31.869010Z",
     "shell.execute_reply": "2024-09-30T06:24:31.868510Z",
     "shell.execute_reply.started": "2024-09-30T06:24:31.866480Z"
    }
   },
   "outputs": [],
   "source": [
    "instruction = \"\"\"\n",
    "Le texte ci-dessous est le résultat d'un transcription automatique de la voix du présentateur d'une conférence sur l'intelligence artificielle. \n",
    "Cette transcription est imparfaite : erreurs, mots incomplets, poncutation manquante, hésitations, interruptions ...\n",
    "Ton travail consiste à répéter strictement le texte fourni ci-dessous, mais en corrigeant sa syntaxe et sa mise en forme :\n",
    "- reformulation sous forme de phrases équivalentes mais bien contruites et sans fautes d'orthographe\n",
    "- ajout de sauts de lignes de paragraphes chaque fois que le présentateur change de sujet\n",
    "- génération de titres et sous-titres de chapitres au format Markdown\n",
    "\n",
    "Voici le texte à mettre en forme :\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b22b22cc-b1ba-4474-b5e4-8c47381369bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T06:25:09.113577Z",
     "iopub.status.busy": "2024-09-30T06:25:09.113155Z",
     "iopub.status.idle": "2024-09-30T06:25:22.960921Z",
     "shell.execute_reply": "2024-09-30T06:25:22.960461Z",
     "shell.execute_reply.started": "2024-09-30T06:25:09.113493Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 1/1 [00:13<00:00, 13.45s/it, est. speed input: 438.12 toks/s, output: 76.68 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.5 s, sys: 173 ms, total: 13.7 s\n",
      "Wall time: 13.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%time text, tokens_per_sec = vllm_generate(instruction, textes[0], llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22f27370-e3b5-4099-b6db-c53210486014",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T20:35:16.427926Z",
     "iopub.status.busy": "2024-09-29T20:35:16.427801Z",
     "iopub.status.idle": "2024-09-29T20:35:16.430613Z",
     "shell.execute_reply": "2024-09-29T20:35:16.430339Z",
     "shell.execute_reply.started": "2024-09-29T20:35:16.427915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Introduction à l'Intelligence Artificielle\n",
       "\n",
       "## Les Domaines de Recherche dans l'IA\n",
       "\n",
       "Voilà, alors, sur tous ces sujets, on a vu que chaque phrase qu'on a dite là-dedans, en fait, c'est un domaine de recherche à part entière, encore aujourd'hui. Il n'y a pas de mode opératoire, de règles, etc. Donc, comment aller plus loin, comment faire de la veille ?\n",
       "\n",
       "## La Veille et les Ressources\n",
       "\n",
       "On a prévu au delà de cette formation tronc commun d'utiliser nos réunions innovation hebdomadaires. Robin a été intervenu pour essayer de vous montrer comment faire pour rechercher quand on sait pas. On peut commencer par YouTube, tout bêtement. Il ya plein de vidéos, plein d'explications externes. Quand vous commencez à repérer les gens qui sont intéressants, qui ont le plus de vue là-dessus, alors c'est quand même... En général, il y a une corrélation entre la popularité et la qualité des contenus.\n",
       "\n",
       "## Les Communautés Spontanées\n",
       "\n",
       "Il ya des communautés spontanées qui se créent en ligne. C'est pour moi extrêmement impressionnant. C'est assez incroyable. Les trucs commencent par 3 personnes qui se réunissent et à la fin ils sont plusieurs milliers. Il ya deux exemples de ça.\n",
       "\n",
       "## Le Premier Exemple\n",
       "\n",
       "Un exemple est la formation en début d'année, où les gens se sont dit : \"Voilà, ça fait à peu près un an qu'on fait des LM en production. Qu'est-ce qu'on a appris ?\" Il ya quelques personnes dans la communauté assez connues. Ils sont mis à deux ou trois au départ. Ils ont dit : \"On va faire une formation où c'est un peu dans le même format qu'on fait là, avec très peu de slides et c'est juste j'essaye de restituer un peu dans l'ordre et correctement ce que j'ai appris donné un an de LM.\" Ils ont commencé à prévoir trois ou quatre sessions, je ne sais plus. Je me suis inscrit à ce moment-là. Et après, ça a fini en une conférence où il y a eu plus de 40 sessions et tous les grands noms du domaine sont venus chacun contribuer leur truc.\n",
       "\n",
       "## Le Deuxième Exemple\n",
       "\n",
       "Un autre exemple est la communauté QDA Mode, qui a démarré avec trois personnes et a fini par avoir 9 000 personnes actives. Ils ont créé un serveur Discord et une chaîne YouTube pour partager leurs connaissances et leurs expériences.\n",
       "\n",
       "## L'Importance de la Veille\n",
       "\n",
       "L'importance de la veille est de repérer les acteurs clés et les communautés qui se créent en ligne. Cela nous permet de nous tenir informés et de nous adapter aux évolutions du domaine.\n",
       "\n",
       "## Les Tâches dans le Machine Learning\n",
       "\n",
       "Dans le machine learning, il ya différentes tâches que l'on peut effectuer, comme la classification, la segmentation, la détection d'objets, etc. Il ya une nomenclature de toutes les tâches qu'on peut faire sur l'image, le texte, la voix, etc.\n",
       "\n",
       "## Les Benchmark Académiques\n",
       "\n",
       "Il ya des benchmark académiques qui mesurent les capacités d'un modèle sur une tâche particulière. Cela nous permet de comparer les performances des modèles et de voir comment on a amélioré les performances sur les benchmarkes au fil des années.\n",
       "\n",
       "## L'Importance de Connaître les Tâches\n",
       "\n",
       "Connaître les tâches est important pour faire une bonne formation en IA. Il faut être capable de distinguer les différentes tâches et de les séparer pour faire une conception de projet efficace.\n",
       "\n",
       "## L'Apprentissage par l'Exemple\n",
       "\n",
       "L'apprentissage par l'exemple est un principe fondamental du machine learning. On apprend par l'exemple en ajustant les paramètres d'un modèle pour minimiser l'erreur sur des exemples de données.\n",
       "\n",
       "## L'Ajustement des Paramètres\n",
       "\n",
       "L'ajustement des paramètres est un processus crucial dans l'apprentissage par l'exemple. On ajuste les paramètres pour minimiser l'erreur sur des exemples de données et pour trouver un modèle qui prédit correctement les résultats.\n",
       "\n",
       "## La Construction d'un Modèle\n",
       "\n",
       "La construction d'un modèle est un processus complexe qui implique l'ajustement des paramètres pour minimiser l'erreur sur des exemples de données. Le modèle final est imparfait, mais il est capable de prédire correctement les résultats sur de nouvelles données.\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "En conclusion, la veille et la connaissance des tâches sont essentiels pour faire une bonne formation en IA. L'apprentissage par l'exemple et l'ajustement des paramètres sont des principes fondamentaux du machine learning. La construction d'un modèle est un processus complexe qui implique l'ajustement des paramètres pour minimiser l'erreur sur des exemples de données."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e5918c-d376-4241-84d3-afd14f6a9cf9",
   "metadata": {},
   "source": [
    "## Nvidia Nemo ASR\n",
    "\n",
    "https://developer.nvidia.com/blog/accelerating-leaderboard-topping-asr-models-10x-with-nvidia-nemo/\n",
    "\n",
    "https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/intro.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecfadc0-ad14-4196-8424-e29204fcdd2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-voice",
   "language": "python",
   "name": "wordslab-voice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
