{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e014d6c1-2727-4e4a-83ff-203f8e7b3f2a",
   "metadata": {},
   "source": [
    "# Transcribe audio files as fast as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5768502-483c-43c8-bffc-e1fdd6321236",
   "metadata": {},
   "source": [
    "## Install prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06e0c0f0-aef8-49db-8a19-aeee93632803",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T14:55:55.682402Z",
     "iopub.status.busy": "2025-11-08T14:55:55.682154Z",
     "iopub.status.idle": "2025-11-08T14:55:55.831071Z",
     "shell.execute_reply": "2025-11-08T14:55:55.830436Z",
     "shell.execute_reply.started": "2025-11-08T14:55:55.682386Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg version 6.1.1-3ubuntu5 Copyright (c) 2000-2023 the FFmpeg developers\n",
      "built with gcc 13 (Ubuntu 13.2.0-23ubuntu3)\n",
      "configuration: --prefix=/usr --extra-version=3ubuntu5 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --disable-omx --enable-gnutls --enable-libaom --enable-libass --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libglslang --enable-libgme --enable-libgsm --enable-libharfbuzz --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-openal --enable-opencl --enable-opengl --disable-sndio --enable-libvpl --disable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-ladspa --enable-libbluray --enable-libjack --enable-libpulse --enable-librabbitmq --enable-librist --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libx264 --enable-libzmq --enable-libzvbi --enable-lv2 --enable-sdl2 --enable-libplacebo --enable-librav1e --enable-pocketsphinx --enable-librsvg --enable-libjxl --enable-shared\n",
      "libavutil      58. 29.100 / 58. 29.100\n",
      "libavcodec     60. 31.102 / 60. 31.102\n",
      "libavformat    60. 16.100 / 60. 16.100\n",
      "libavdevice    60.  3.100 / 60.  3.100\n",
      "libavfilter     9. 12.100 /  9. 12.100\n",
      "libswscale      7.  5.100 /  7.  5.100\n",
      "libswresample   4. 12.100 /  4. 12.100\n",
      "libpostproc    57.  3.100 / 57.  3.100\n"
     ]
    }
   ],
   "source": [
    "!ffmpeg -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56df76d7-4856-45fc-9f91-e361b1bb4d80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T14:55:55.831917Z",
     "iopub.status.busy": "2025-11-08T14:55:55.831798Z",
     "iopub.status.idle": "2025-11-08T14:55:55.965611Z",
     "shell.execute_reply": "2025-11-08T14:55:55.964916Z",
     "shell.execute_reply.started": "2025-11-08T14:55:55.831906Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/home/jupyterlab/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m275 packages\u001b[0m \u001b[2min 0.83ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m178 packages\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9d1fda0-bbd8-4b3e-bd9c-57164c860bb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T14:55:55.971912Z",
     "iopub.status.busy": "2025-11-08T14:55:55.971748Z",
     "iopub.status.idle": "2025-11-08T14:55:55.974326Z",
     "shell.execute_reply": "2025-11-08T14:55:55.973844Z",
     "shell.execute_reply.started": "2025-11-08T14:55:55.971866Z"
    }
   },
   "outputs": [],
   "source": [
    "from importlib.metadata import version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11130d56-5a5b-40e8-b85c-d90dfd9577f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T14:55:55.974889Z",
     "iopub.status.busy": "2025-11-08T14:55:55.974772Z",
     "iopub.status.idle": "2025-11-08T14:55:55.982090Z",
     "shell.execute_reply": "2025-11-08T14:55:55.981767Z",
     "shell.execute_reply.started": "2025-11-08T14:55:55.974881Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.57.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ba96ccf-f1b0-4097-aa8f-209eae467360",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T14:55:55.982549Z",
     "iopub.status.busy": "2025-11-08T14:55:55.982463Z",
     "iopub.status.idle": "2025-11-08T14:55:55.985946Z",
     "shell.execute_reply": "2025-11-08T14:55:55.985431Z",
     "shell.execute_reply.started": "2025-11-08T14:55:55.982542Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('accelerate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15303bd-581f-4592-951d-aee3be88fa02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T22:32:03.810913Z",
     "iopub.status.busy": "2025-11-07T22:32:03.810765Z",
     "iopub.status.idle": "2025-11-07T22:32:03.817116Z",
     "shell.execute_reply": "2025-11-07T22:32:03.816771Z",
     "shell.execute_reply.started": "2025-11-07T22:32:03.810904Z"
    }
   },
   "source": [
    "## Convert video files to audio files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131b9633-6e50-4646-bd79-ed2d3f7ed813",
   "metadata": {},
   "source": [
    "Optional step, if you want to extract the audio of a video file: replace the file names below with your own files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fb0b22-0cca-4d3c-97bd-8ef43fe0b3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -y -i \"2024-09-26 15-35-04.mp4\" \"data/2024-09-26 15-35-04.mp3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44703518-d75c-4223-93b7-92457811f4b0",
   "metadata": {},
   "source": [
    "## Choose a Whisper model on Huggingface\n",
    "\n",
    "You saw in the first notebook how to use the official Whisper model to transcribe english speech.\n",
    "\n",
    "https://huggingface.co/openai/whisper-large-v3-turbo\n",
    "\n",
    "If you need to transcribe audio files in another language, you can find optimized models on HuggingFace. For example for french:\n",
    "\n",
    "https://huggingface.co/eustlb/distil-large-v3-fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffcb2fd4-e273-4125-8269-7874e7516d68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T15:08:49.414820Z",
     "iopub.status.busy": "2025-11-08T15:08:49.414727Z",
     "iopub.status.idle": "2025-11-08T15:08:56.056600Z",
     "shell.execute_reply": "2025-11-08T15:08:56.055946Z",
     "shell.execute_reply.started": "2025-11-08T15:08:49.414811Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workspace/wordslab-voice/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cuda:0\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"eustlb/distil-large-v3-fr\"\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, dtype=torch_dtype, \n",
    "    use_safetensors=True, low_cpu_mem_usage=True, device_map=device, \n",
    "    attn_implementation=\"sdpa\"\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=256,\n",
    "    dtype=torch_dtype\n",
    ")\n",
    "\n",
    "# warmup\n",
    "dummy_input = torch.randn( (1, model.config.num_mel_bins, 3000), dtype=torch_dtype, device=device)\n",
    "_ = model.generate(dummy_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd69d22-fb70-4e93-a867-ef37de9d6621",
   "metadata": {},
   "source": [
    "## Choose a long form transcription algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17de1884-931c-4809-a852-21d153d17798",
   "metadata": {},
   "source": [
    "See: https://huggingface.co/openai/whisper-large-v3#chunked-long-form\n",
    "\n",
    "Whisper has a receptive field of 30-seconds. To transcribe audios longer than this, one of two long-form algorithms are required:\n",
    "\n",
    "- Sequential: uses a \"sliding window\" for buffered inference, transcribing 30-second slices one after the other\n",
    "- Chunked: splits long audio files into shorter ones (with a small overlap between segments), transcribes each segment independently, and stitches the resulting transcriptions at the boundaries\n",
    "\n",
    "The sequential long-form algorithm should be used in either of the following scenarios:\n",
    "- Transcription accuracy is the most important factor, and speed is less of a consideration\n",
    "- You are transcribing batches of long audio files, in which case the latency of sequential is comparable to chunked, while being up to 0.5% WER more accurate\n",
    "\n",
    "Conversely, the chunked algorithm should be used when:\n",
    "- Transcription speed is the most important factor\n",
    "- You are transcribing a single long audio file\n",
    "\n",
    "By default, Transformers uses the sequential algorithm. To enable the chunked algorithm, pass the chunk_length_s parameter to the pipeline. To activate batching over long audio files, pass the argument batch_size:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fae4bd4-10f7-4006-b15c-d2d991462777",
   "metadata": {},
   "source": [
    "## Sequential transcription with timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abf08e4-853c-4010-a1ed-4cc5043c3daa",
   "metadata": {},
   "source": [
    "Replace the mp3 audio file names below with your own files uploaded to the ./data directory.\n",
    "\n",
    "In this example, the audio file is 1 hour and 8 minutes long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cc3bd80-d18f-4881-8866-ffaf12ecef07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T14:56:32.897281Z",
     "iopub.status.busy": "2025-11-08T14:56:32.897109Z",
     "iopub.status.idle": "2025-11-08T14:58:23.359103Z",
     "shell.execute_reply": "2025-11-08T14:58:23.358649Z",
     "shell.execute_reply.started": "2025-11-08T14:56:32.897271Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61299,\n",
       " \" dernière partie de notre tronc commun savoir réaliser un projet chez lui donc là jusqu'à maintenant on a vu principalement tous les éléments qui étaient nécessaires pour identifier des projets faire \")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pipe(\"./data/2024-09-26 15-35-04.mp3\", return_timestamps=True)\n",
    "len(result[\"text\"]),result[\"text\"][:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327f85a0-7dc5-43cd-bc7a-99924e9693f3",
   "metadata": {},
   "source": [
    "Performance on RTX 4090 -> 1 hour 8 min transcribed in **1 min 51 sec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38dde4f8-56c0-4a4b-9f1d-5f32a45487ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T14:58:51.682127Z",
     "iopub.status.busy": "2025-11-08T14:58:51.681936Z",
     "iopub.status.idle": "2025-11-08T14:58:51.685542Z",
     "shell.execute_reply": "2025-11-08T14:58:51.685173Z",
     "shell.execute_reply.started": "2025-11-08T14:58:51.682116Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'timestamp': (51.94, 54.06), 'text': ' qui a un vrai challenge,'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"chunks\"][14]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c8510c-1891-4598-9d33-3eb181e2ec44",
   "metadata": {},
   "source": [
    "## Chuncked transcription with batch parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4491a1-b8e0-49c9-b2d4-399d467e5442",
   "metadata": {},
   "source": [
    "Use batch size 16 for a 8 GB GPU, batch size 32 for a 16 GB+ GPU, batch size 128 for a datacenter GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1865cd0b-eb16-4c1c-8064-4a6edac0cf23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T15:11:25.836037Z",
     "iopub.status.busy": "2025-11-08T15:11:25.835590Z",
     "iopub.status.idle": "2025-11-08T15:11:54.603358Z",
     "shell.execute_reply": "2025-11-08T15:11:54.602846Z",
     "shell.execute_reply.started": "2025-11-08T15:11:25.836008Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(62063,\n",
       " \" Dernière partie de notre tronc commun, savoir réaliser un projet chez lui. Donc là, jusqu'à maintenant, on a vu principalement tous les éléments qui étaient nécessaires pour identifier des projets, f\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pipe(\"./data/2024-09-26 15-35-04.mp3\", chunk_length_s=30, batch_size=32)\n",
    "len(result[\"text\"]),result[\"text\"][:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8bfa17-f276-4a58-8678-c3dd3ffd160a",
   "metadata": {},
   "source": [
    "Performance on RTX 4090 -> 1 hour 8 min transcribed in **29 sec**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa32e855-ac9e-47a0-9b72-da1ca3f58f69",
   "metadata": {},
   "source": [
    "## Code examples to transcribe a list of audio files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d9dbfe-11a4-4871-b815-9a7ac60f4340",
   "metadata": {},
   "source": [
    "Batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d94411-e680-405a-9343-f5069b39ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pipe([\"./audio/2024-09-19 15-03-35.mp3\",\"./audio/2024-09-19 16-32-50.mp3\"], batch_size=2)\n",
    "for result in results: print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb5cd3b-28b8-458e-9ec5-c63164e8fb12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T15:16:10.097738Z",
     "iopub.status.busy": "2025-11-08T15:16:10.097448Z",
     "iopub.status.idle": "2025-11-08T15:16:10.105380Z",
     "shell.execute_reply": "2025-11-08T15:16:10.104699Z",
     "shell.execute_reply.started": "2025-11-08T15:16:10.097715Z"
    }
   },
   "source": [
    "Sequential processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5206c4ef-00f8-4950-a039-61073d111d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Specify the directory containing mp3 files\n",
    "directory = '/workspace/wordslab-voice/data'\n",
    "\n",
    "# Use glob to get all .mp3 files in the directory\n",
    "mp3_files = glob.glob(os.path.join(directory, '*.mp3'))\n",
    "\n",
    "# Loop through each mp3 file\n",
    "for mp3_file in mp3_files:\n",
    "    # Get the base name of the file (without directory path)\n",
    "    base_name = os.path.basename(mp3_file)\n",
    "    \n",
    "    # Replace the .mp3 extension with .txt to create a new filename\n",
    "    sequential_txt_file = base_name.replace('.mp3', '_sequential.txt')\n",
    "    chunked_txt_file = base_name.replace('.mp3', '_chunked.txt')\n",
    "    \n",
    "    # Full path of the text file to be written\n",
    "    sequential_txt_file_path = os.path.join(directory, sequential_txt_file)\n",
    "    chunked_txt_file_path = os.path.join(directory, chunked_txt_file)\n",
    "\n",
    "    # Transcribe audio with two methods\n",
    "    print(f\"- {base_name} (sequential) ...\")\n",
    "    sequential_txt = pipe(mp3_file)[\"text\"]\n",
    "    print(\"OK\")\n",
    "    \n",
    "    print(f\"- {base_name} (chunked) ...\")\n",
    "    chunked_txt = pipe(mp3_file, chunk_length_s=25, batch_size=32)[\"text\"]\n",
    "    print(\"OK\")\n",
    "    \n",
    "    # Write a text file with the same name as the mp3 file\n",
    "    with open(sequential_txt_file_path, 'w') as file:\n",
    "        file.write(sequential_txt)\n",
    "    print(f\"Saved: {sequential_txt_file_path}\")\n",
    "    \n",
    "    with open(chunked_txt_file_path, 'w') as file:\n",
    "        file.write(chunked_txt)\n",
    "    print(f\"Saved: {chunked_txt_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bb434f-8120-4943-bb97-1f5d46c32c2f",
   "metadata": {},
   "source": [
    "## Reformatting the transcribed audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78d5a028-82e3-441f-bc89-2087db54f450",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T15:31:46.005037Z",
     "iopub.status.busy": "2025-11-08T15:31:46.004885Z",
     "iopub.status.idle": "2025-11-08T15:31:46.007102Z",
     "shell.execute_reply": "2025-11-08T15:31:46.006628Z",
     "shell.execute_reply.started": "2025-11-08T15:31:46.005028Z"
    }
   },
   "outputs": [],
   "source": [
    "instruction = \"\"\"\n",
    "The text below is the result of an automatic transcription of the voice of a presenter at a conference on artificial intelligence.\n",
    "This transcription is imperfect: errors, incomplete words, missing punctuation, hesitations, interruptions...\n",
    "Your task is to **strictly repeat** the text provided below, but correcting its syntax and formatting:\n",
    "- Rewording into equivalent sentences that are well-constructed and free of spelling errors.\n",
    "- Adding line breaks and paragraphs whenever the presenter changes subject.\n",
    "- Generating chapter titles and subtitles in Markdown format.\n",
    "\n",
    "Here's the text to be formatted:\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8abb6f12-e440-401b-88e6-1b89523eb8ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T15:31:46.007629Z",
     "iopub.status.busy": "2025-11-08T15:31:46.007531Z",
     "iopub.status.idle": "2025-11-08T15:31:46.011631Z",
     "shell.execute_reply": "2025-11-08T15:31:46.011242Z",
     "shell.execute_reply.started": "2025-11-08T15:31:46.007622Z"
    }
   },
   "outputs": [],
   "source": [
    "transcribed_text = result[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "756cd710-6f24-4891-83d6-a77fc20cb2cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T15:31:46.012008Z",
     "iopub.status.busy": "2025-11-08T15:31:46.011924Z",
     "iopub.status.idle": "2025-11-08T15:31:46.015554Z",
     "shell.execute_reply": "2025-11-08T15:31:46.015199Z",
     "shell.execute_reply.started": "2025-11-08T15:31:46.012001Z"
    }
   },
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fc7e84-a344-4f87-82cf-077da2970603",
   "metadata": {},
   "source": [
    "Replace the model below with your default model depending on your GPU VRAM size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "046cf261-6ed1-4537-96e5-ce8a529f1be0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T15:34:22.898351Z",
     "iopub.status.busy": "2025-11-08T15:34:22.897935Z",
     "iopub.status.idle": "2025-11-08T15:37:28.817066Z",
     "shell.execute_reply": "2025-11-08T15:37:28.811039Z",
     "shell.execute_reply.started": "2025-11-08T15:34:22.898324Z"
    }
   },
   "outputs": [],
   "source": [
    "formatted_text = ollama.generate(model='hf.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF:UD-Q4_K_XL', prompt=f\"{instruction} {transcribed_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22f27370-e3b5-4099-b6db-c53210486014",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T15:42:43.654694Z",
     "iopub.status.busy": "2025-11-08T15:42:43.654511Z",
     "iopub.status.idle": "2025-11-08T15:42:43.658980Z",
     "shell.execute_reply": "2025-11-08T15:42:43.658617Z",
     "shell.execute_reply.started": "2025-11-08T15:42:43.654683Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "sser\" des projets : laisser les métiers s'approprier les idées (ex : direction commerciale exige désormais une étude IA avant tout projet).\n",
       "  - Ateliers avec les métiers : animer sans imposer (ex : atelier ACM sans idées concrètes).\n",
       "\n",
       "- **Veille technologique** :\n",
       "  - **Intégrée au projet** : Pas une activité séparée, mais une partie du temps de travail quotidien.\n",
       "  - **Exemples concrets** :\n",
       "    - Connaître les versions récentes de langages (ex : C# 14) pour éviter de réinventer des fonctionnalités existantes.\n",
       "    - Benchmarker les modèles IA disponibles (ex : qualité de rédaction pour un assistant client).\n",
       "  - **Rentabilité** : 1 jour de veille peut économiser 3-4 jours de développement.\n",
       "\n",
       "- **Compétences et formation** :\n",
       "  - Planifier des lignes de veille dans les projets (ex : 7 heures sur 2-3 semaines).\n",
       "  - Former l'équipe sur des outils spécifiques (ex : Camunda, prompting pour LLM).\n",
       "  - Partager les retours d'expérience (ex : document type pour évaluer les besoins en compétences).\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(formatted_text.response[500:1500]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-voice",
   "language": "python",
   "name": "wordslab-voice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
