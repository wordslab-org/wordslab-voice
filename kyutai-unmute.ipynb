{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "610e2a1a-81c4-4be4-862a-8dcb62cde5a3",
   "metadata": {},
   "source": [
    "# Kyutai - Unmute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cb2cc3-26fc-4bc3-8e0c-a8f03181da91",
   "metadata": {},
   "source": [
    "https://unmute.sh/\n",
    "\n",
    "This is a cascaded system made by Kyutai: our speech-to-text transcribes what you say, an LLM (we use Mistral Small 24B) generates the text of the response, and we then use our text-to-speech model to say it out loud.\n",
    "\n",
    "All of the components are open-source: Kyutai STT, Kyutai TTS, and Unmute itself.\n",
    "\n",
    "https://kyutai.org/next/stt\n",
    "\n",
    "https://kyutai.org/next/tts\n",
    "\n",
    "https://kyutai.org/next/unmute\n",
    "\n",
    "Although cascaded systems lose valuable information like emotion, irony, etc., they provide unmatched modularity: since the three parts are separate, you can Unmute any LLM you want without any finetuning or adaptation! In this demo, you can get a feel for this versatility by tuning the system prompt of the LLM to handcraft the personality of your digital interlocutor, and independently changing the voice of the TTS.\n",
    "\n",
    "Both the speech-to-text and text-to-speech models are optimized for low latency. The STT model is streaming and integrates semantic voice activity detection instead of relying on an external model. The TTS is streaming both in audio and in text, meaning it can start speaking before the entire LLM response is generated. You can use a 10-second voice sample to determine the TTS's voice and intonation. Check out the pre-print for details.\n",
    "\n",
    "https://arxiv.org/pdf/2509.08753"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145ec642-52f1-4e51-b827-8d74e02e5ac4",
   "metadata": {},
   "source": [
    "## Research paper - Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling\n",
    "\n",
    "### Key ideas and results\n",
    "\n",
    "The paper introduces **Delayed Streams Modeling (DSM)** - a new approach for streaming sequence-to-sequence learning. The key hypothesis is that by aligning different modalities (like audio and text) to a shared framerate and introducing appropriate delays between them, you can achieve real-time streaming generation without needing complex alignment policies.\n",
    "\n",
    "The main contributions are:\n",
    "- A flexible framework that works for multiple tasks (ASR and TTS) with the same architecture\n",
    "- State-of-the-art performance with low latency (as low as a few hundred milliseconds)\n",
    "- Support for arbitrarily long sequences with batching capability\n",
    "\n",
    "The core idea is surprisingly elegant. Instead of processing all input before generating output (offline) or learning complex policies about when to read/write (traditional streaming), DSM:\n",
    "\n",
    "- **Aligns sequences**: Both audio and text are aligned to the same framerate (12.5 Hz)\n",
    "- **Introduces delays**: The output stream is delayed by τ steps relative to the input, creating a \"lookahead\" window\n",
    "- **Uses parallel streams**: A decoder-only transformer processes multiple token streams simultaneously\n",
    "\n",
    "For **ASR** (speech-to-text): audio is the input stream, text is delayed and predicted\n",
    "For **TTS** (text-to-speech): text is the input, audio is delayed and generated\n",
    "\n",
    "Performance Results\n",
    "\n",
    "**DSM-ASR (Automatic Speech Recognition):**\n",
    "- Achieves **7.9% average WER** (Word Error Rate) on long-form datasets\n",
    "- Competitive with or better than non-streaming models like Whisper-Large-V2 (9.0% WER)\n",
    "- Outperforms other streaming models\n",
    "- Provides word-level timestamps with 80ms precision\n",
    "\n",
    "**DSM-TTS (Text-to-Speech):**\n",
    "- Achieves **1.58-1.71% WER** when evaluated (lower is better)\n",
    "- Speaker similarity score of **0.70-0.71** (comparable to baselines)\n",
    "- Competitive with non-streaming models like F5-TTS and Cosyvoice\n",
    "- Only model providing long-form synthesis in a streaming manner\n",
    "\n",
    "Key Advantages\n",
    "\n",
    "The paper emphasizes DSM provides:\n",
    "- **Low latency**: As low as a few hundred milliseconds\n",
    "- **Batching support**: Unlike most streaming models\n",
    "- **Arbitrary length sequences**: No chunking needed\n",
    "- **Single architecture**: Same model structure for both ASR and TTS\n",
    "\n",
    "### How do you align audio and text on the same framerate\n",
    "\n",
    "Great question! This is a crucial part of making DSM work.\n",
    "\n",
    "For **audio**, they use a neural codec called **Mimi** that compresses the raw waveform:\n",
    "- Takes audio at 24kHz sample rate\n",
    "- Compresses it down to 12.5 Hz (so 12.5 \"frames\" per second)\n",
    "- Each frame represents 80ms of audio\n",
    "- Uses vector quantization to turn it into discrete tokens\n",
    "\n",
    "For **text**, they use **word-level timestamps**:\n",
    "- Each word has a timestamp indicating when it's spoken\n",
    "- They place the word tokens at the corresponding frame position\n",
    "- For example, if a word starts at 0.5 seconds, it goes at frame position 6 (0.5 × 12.5)\n",
    "- They use special tokens: **WORD** (marks word start), **PAD** (empty frames between words)\n",
    "- Frames without words get filled with PAD tokens\n",
    "\n",
    "So if someone says \"Hello world\" where \"Hello\" starts at 0s and \"world\" starts at 0.4s, the aligned sequence might look like:\n",
    "```\n",
    "Frame 0: WORD, H, e, l, l, o\n",
    "Frame 5: WORD, w, o, r, l, d\n",
    "Frames 6+: PAD\n",
    "```\n",
    "\n",
    "The challenge they mention is that most speech datasets only have sentence-level timing, not word-level. \n",
    "\n",
    "### Text tokenization, and how text and audio tokens are aligned\n",
    "\n",
    "Text Tokenization\n",
    "\n",
    "They use a **custom vocabulary** specifically trained on speech transcription data (not a standard text tokenizer). The vocabulary has:\n",
    "- Regular word tokens (vocabulary size 8000)\n",
    "- Two special tokens: **PAD** and **WORD**\n",
    "\n",
    "The Alignment Process\n",
    "\n",
    "Here's how they align text to the 12.5 Hz audio framerate:\n",
    "\n",
    "1. **Start with word-level timestamps**: Each word has a start time (e.g., \"hello\" starts at 0.24 seconds)\n",
    "\n",
    "2. **Convert time to frame index**: Multiply the start time by the framerate\n",
    "   - Example: 0.24s × 12.5 = frame 3\n",
    "\n",
    "3. **Place tokens in the sequence**:\n",
    "   - Put **WORD** token at the start frame\n",
    "   - Follow immediately with the word's sub-tokens (like \"h\", \"e\", \"l\", \"l\", \"o\")\n",
    "   - Fill any empty frames with **PAD**\n",
    "\n",
    "When a word like \"hello\" is tokenized into sub-tokens [h, e, l, l, o], these tokens are placed **consecutively starting from the word's start frame**:\n",
    "\n",
    "```\n",
    "Frame 0: WORD\n",
    "Frame 1: h\n",
    "Frame 2: e\n",
    "Frame 3: l\n",
    "Frame 4: l\n",
    "Frame 5: o\n",
    "Frame 6: PAD (until next word)\n",
    "```\n",
    "\n",
    "So the word's tokens \"flow forward\" in time, occupying consecutive frames. The **WORD** token marks where a new word begins, then its sub-tokens follow in sequence.\n",
    "\n",
    "This means:\n",
    "- Short words might only take 1-2 frames\n",
    "- Longer words could span many frames\n",
    "- The actual pronunciation duration doesn't matter for the text stream - it's just sequential token placement\n",
    "- PAD fills gaps between words\n",
    "\n",
    "The audio stream, meanwhile, has tokens at **every** frame representing the actual sound at that moment.\n",
    "\n",
    "During training, the model learns to predict text tokens that are **delayed by τ frames** relative to the audio. So if τ=16 frames (1.28 seconds), the model sees audio frames 0-16 before predicting text frame 0.\n",
    "\n",
    "The audio stream has its own tokens at every frame (from the Mimi codec). Both streams now have exactly one \"event\" per 80ms time step.\n",
    "\n",
    "### Training phases and datasets\n",
    "\n",
    "For DSM-ASR:\n",
    "\n",
    "**Pretraining:**\n",
    "- 2.5 million hours of publicly available audio (English and French)\n",
    "- Transcribed automatically using whisper-timestamped\n",
    "- Trained on 90-second random segments\n",
    "\n",
    "**Finetuning:**\n",
    "- \"A collection of public datasets with ground-truth transcripts\" totaling 28k hours\n",
    "- The paper mentions details are in \"Appendix A.1\"\n",
    "\n",
    "**Long-form adaptation:**\n",
    "- A special \"long-form mixture\" described in \"Appendix A.2\"\n",
    "\n",
    "For DSM-TTS:\n",
    "\n",
    "**Pretraining:**\n",
    "- 150-second audio extracts from the same 2.5M hour collection\n",
    "\n",
    "### Delay conditioning feature\n",
    "\n",
    "The **delay conditioning** feature is a clever training trick that gives you flexibility at inference time.\n",
    "\n",
    "The Problem\n",
    "\n",
    "Normally, you'd train with a fixed delay (say τ=16 frames). But different use cases need different tradeoffs:\n",
    "- **Low latency** (small delay): Faster response, but lower quality transcription\n",
    "- **High quality** (large delay): Better transcription, but more lag\n",
    "\n",
    "Without delay conditioning, you'd need to train a separate model for each delay value you want to support.\n",
    "\n",
    "The Solution\n",
    "\n",
    "Instead, they train **one model** on random delays:\n",
    "- Each training example uses a different randomly sampled delay\n",
    "- The model receives the delay value as an extra input (using a cosine embedding)\n",
    "- The model learns: \"given delay X, predict text accordingly\"\n",
    "\n",
    "At Inference\n",
    "\n",
    "You simply tell the model what delay you want (e.g., 400ms for low latency, or 2 seconds for high quality), and it adjusts its predictions to match that latency/quality tradeoff.\n",
    "\n",
    "Think of it like training a model that can operate at multiple \"speeds\" rather than just one fixed speed.\n",
    "\n",
    "The delay conditioning feature lets you control the quality/latency tradeoff at inference time without retraining.\n",
    "\n",
    "### Batching support\n",
    "\n",
    "This is one of DSM's key practical advantages.\n",
    "\n",
    "**The core insight:** DSM operates at a **constant framerate** (12.5 Hz). At each time step, the model processes exactly one frame for each stream, regardless of what's in it.\n",
    "\n",
    "This means:\n",
    "- Every example in a batch advances by exactly 1 frame per step\n",
    "- All sequences stay synchronized\n",
    "- You can run multiple audio streams through the model simultaneously\n",
    "\n",
    "**Why other streaming models can't batch:**\n",
    "\n",
    "Traditional streaming models use **policies** that decide \"should I read more input or write output?\" These decisions vary per example:\n",
    "- Example 1 might need 3 input frames before writing\n",
    "- Example 2 might write immediately\n",
    "- They get out of sync, so you have to process them one at a time\n",
    "\n",
    "**DSM's advantage:**\n",
    "\n",
    "Since everything moves in lockstep (one frame per step for all streams), you can stack multiple examples and process them together efficiently on a GPU.\n",
    "\n",
    "The paper notes this is \"a feature rarely provided by streaming models\" and helps with throughput.\n",
    "\n",
    "### Speaker voices\n",
    "\n",
    "This is specific to the TTS model and how it controls whose voice to generate.\n",
    "\n",
    "Speaker Encoding Process\n",
    "\n",
    "The model can handle **up to 5 different speakers** in a conversation. For each speaker:\n",
    "\n",
    "1. **Extract a 10-second audio sample** of that speaker (from outside the training segment)\n",
    "2. **Pass it through a speaker encoder** that produces a fixed-dimension embedding\n",
    "3. The speaker encoder uses the same architecture as the Mimi codec encoder\n",
    "4. Convolutional layers are frozen, but Transformer layers are fine-tuned\n",
    "\n",
    "Conditioning the Model\n",
    "\n",
    "The speaker embeddings are fed to the model through **cross-attention layers**:\n",
    "- Concatenate embeddings from all speakers (up to 5)\n",
    "- Add positional embeddings to distinguish which speaker is which\n",
    "- Feed through cross-attention to the main backbone\n",
    "\n",
    "If there are fewer than 5 speakers, they pad with learned embeddings. If more than 5, they randomly select 5.\n",
    "\n",
    "Controlling Turns in Dialogue\n",
    "\n",
    "They use special tokens to control who's speaking:\n",
    "- **MAIN**: Marks when the primary speaker starts talking\n",
    "- **OTHER**: Marks when another speaker takes over\n",
    "\n",
    "At inference, you provide speaker embeddings for each person, then insert MAIN/OTHER tokens to orchestrate the conversation.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "The paper identifies a few key limitations:\n",
    "\n",
    "1. Need for Aligned Domains\n",
    "\n",
    "The biggest limitation they mention is that **DSM requires aligned domains** - meaning you need data where audio and text have word-level timestamps. This \"reduces the amount of gold-standard ground-truth data that can be used for training.\"\n",
    "\n",
    "Most speech datasets only have sentence-level alignment, so they had to:\n",
    "- Use pseudo-labels from Whisper for pretraining\n",
    "- Apply Dynamic Time Warping to derive word alignments for finetuning\n",
    "\n",
    "2. Independence Assumption\n",
    "\n",
    "They note that \"perfect independence is hard to achieve\" - meaning the output at time t isn't truly independent of future input beyond the delay window. For example, in ASR, a named entity might be ambiguous without seeing more context.\n",
    "\n",
    "3. Safety Concerns (TTS)\n",
    "\n",
    "For their TTS model, they kept the speaker encoder closed-source due to impersonation risks. They acknowledge that voice cloning \"opens up both opportunities in inclusive human-machine interactions and risks of fraudulent impersonation.\"\n",
    "\n",
    "The paper mentions they'll extend DSM to more tasks in future work, suggesting the current scope (ASR and TTS) is somewhat limited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354df4cc-d132-49ea-bd33-71554522e68d",
   "metadata": {},
   "source": [
    "## System implementation - Unmute\n",
    "\n",
    "Great! Now I can give you a complete picture of how the full Unmute system works.\n",
    "\n",
    "System Overview\n",
    "\n",
    "**Unmute** is a cascaded system that lets you have voice conversations with any text-based LLM. It's called \"cascaded\" because it chains three separate components:\n",
    "\n",
    "1. **DSM-ASR** (Speech-to-Text) - transcribes what you say\n",
    "2. **LLM** (any text model) - generates text response\n",
    "3. **DSM-TTS** (Text-to-Speech) - speaks the response\n",
    "\n",
    "Architecture\n",
    "\n",
    "```\n",
    "User Browser ←→ Backend ←→ STT Server\n",
    "                         ↓\n",
    "                         LLM Server\n",
    "                         ↓\n",
    "                         TTS Server\n",
    "```\n",
    "\n",
    "The user's browser connects via WebSocket to a backend server, which orchestrates everything:\n",
    "\n",
    "**Flow:**\n",
    "1. Browser sends audio → Backend → STT transcribes it\n",
    "2. STT detects end of speech (semantic VAD)\n",
    "3. Backend sends transcript → LLM generates response\n",
    "4. As LLM generates tokens → TTS starts speaking immediately (streaming in text!)\n",
    "5. Audio streams back to browser\n",
    "\n",
    "Key Innovation: Streaming in Text\n",
    "\n",
    "Unlike other TTS systems that need the complete text first, DSM-TTS can start generating audio as soon as it receives the first few tokens from the LLM. This dramatically reduces latency.\n",
    "\n",
    "The system uses a \"flush trick\" to reduce latency further - when speech ends, it processes remaining audio at 4x speed.\n",
    "\n",
    "### Frontend\n",
    "\n",
    "Frontend Technology\n",
    "\n",
    "The frontend is a **Next.js app** (React-based framework) located in the `frontend/` directory. It runs on port 3000 by default.\n",
    "\n",
    "Communication Protocol\n",
    "\n",
    "The frontend and backend communicate via **WebSocket** using a protocol based on the **OpenAI Realtime API** (ORA). However, Unmute makes some modifications:\n",
    "- Some extra message types were added\n",
    "- Some parameters are simplified\n",
    "- Not fully compatible with ORA yet (but they're working toward it)\n",
    "\n",
    "The protocol details are defined in `unmute/openai_realtime_api_events.py`.\n",
    "\n",
    "Audio Processing\n",
    "\n",
    "The browser:\n",
    "- Captures audio from the user's microphone\n",
    "- Sends it over WebSocket to the backend in real-time\n",
    "- Receives audio back from the TTS\n",
    "- Plays it to the user\n",
    "\n",
    "#UI Features\n",
    "\n",
    "**Keyboard shortcuts:**\n",
    "- Press **S** for subtitles (shows transcription for both user and chatbot)\n",
    "- Press **D** for dev mode (debug view with extra info)\n",
    "\n",
    "**User controls:**\n",
    "- Can interrupt the AI mid-response\n",
    "- Can change voices and system prompts\n",
    "- Voice activity detection shows \"End of speech detected\"\n",
    "\n",
    "There's also a Python client implementation in `unmute/loadtest/loadtest_client.py` that demonstrates the protocol from a different angle - it's used for benchmarking.\n",
    "\n",
    "### How the interruption by the user mid-reponse works\n",
    "\n",
    "The interruption mechanism uses the **word-level timestamps** from DSM-TTS.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "**During generation:**\n",
    "- DSM-TTS outputs audio chunks along with precise timestamps for each word\n",
    "- The system tracks exactly which words have been spoken and when\n",
    "\n",
    "**When user interrupts:**\n",
    "- The frontend detects the user starting to speak (via voice activity detection)\n",
    "- It signals the backend to stop the current TTS generation\n",
    "- Because the system knows the exact timestamp where it stopped, it knows which part of the LLM's response was actually spoken and which wasn't\n",
    "\n",
    "**The clever part:**\n",
    "The backend can then inform the LLM context about what was actually said vs. what was cut off. This means the conversation can continue naturally - the AI knows what the user heard and what they didn't.\n",
    "\n",
    "The paper mentions: \"If you interrupt mid-way through an explanation to ask a follow-up question, Unmute will know exactly where it got interrupted and which part of the explanation still remains to be said later.\"\n",
    "\n",
    "### How the voice activity detection works\n",
    "\n",
    "The Voice Activity Detection (VAD) in Unmute is particularly clever - it's **semantic** rather than just acoustic.\n",
    "\n",
    "Traditional VAD Problem\n",
    "\n",
    "Most voice systems use a separate VAD model that detects if someone is speaking or not, then waits a fixed time (like 500ms) after silence before deciding \"they're done talking.\"\n",
    "\n",
    "The problem: People naturally pause mid-sentence! A fixed timeout causes either:\n",
    "- False positives (cutting people off mid-thought)\n",
    "- Or long delays (waiting too long to be safe)\n",
    "\n",
    "Kyutai's Semantic VAD Solution\n",
    "\n",
    "Instead of a separate model, **DSM-ASR itself predicts the probability that the user is done talking**. It's built right into the speech-to-text model.\n",
    "\n",
    "The key insight: The delay adapts based on **content and intonation**. The model can tell the difference between:\n",
    "- \"I went to the store...\" (pause, more coming)\n",
    "- \"I went to the store.\" (done, falling intonation)\n",
    "\n",
    "How It Works\n",
    "\n",
    "The STT model outputs both:\n",
    "1. Text transcription\n",
    "2. End-of-speech probability\n",
    "\n",
    "When this probability crosses a threshold, the system triggers the LLM response.\n",
    "\n",
    "This is what you see in the UI when it shows \"End of speech detected.\"\n",
    "\n",
    "### Protocol and messages used between frontend and backend\n",
    "\n",
    "**Message format:**\n",
    "- Based on OpenAI Realtime API format\n",
    "- Defined in `unmute/openai_realtime_api_events.py`\n",
    "- Contains both standard ORA messages and custom Unmute extensions\n",
    "\n",
    "**Audio handling:**\n",
    "- Browser sends raw audio data over WebSocket\n",
    "- Backend streams audio back for playback\n",
    "- Real-time bidirectional communication\n",
    "\n",
    "**Debug info:**\n",
    "- Backend populates `self.debug_dict` in `unmute_handler.py`\n",
    "- This gets sent to frontend for the dev mode view\n",
    "\n",
    "WebSocket Connection\n",
    "\n",
    "**Endpoint:** `/v1/realtime` using the `realtime` subprotocol\n",
    "**Port:** 8000 (dev), or 80/443 through Traefik (production)\n",
    "\n",
    "Audio Format\n",
    "\n",
    "All audio uses:\n",
    "- **Codec:** Opus\n",
    "- **Sample rate:** 24kHz  \n",
    "- **Channels:** Mono\n",
    "- **Encoding:** Base64-encoded bytes\n",
    "\n",
    "Key Message Types\n",
    "\n",
    "**Client → Server:**\n",
    "\n",
    "1. **`input_audio_buffer.append`** - Streams microphone audio to backend\n",
    "2. **`session.update`** - Configures voice character and instructions (required to start!)\n",
    "\n",
    "**Server → Client:**\n",
    "\n",
    "1. **`response.audio.delta`** - Streams generated speech back\n",
    "2. **`conversation.item.input_audio_transcription.delta`** - Real-time user transcription\n",
    "3. **`response.text.delta`** - Text of what AI is saying (for display)\n",
    "4. **`input_audio_buffer.speech_started/stopped`** - VAD events (currently ignored)\n",
    "5. **`response.created`** - Signals assistant is generating a response\n",
    "6. **`error`** - Error/warning messages\n",
    "\n",
    "All messages are JSON with a common structure defined in `openai_realtime_api_events.py`.\n",
    "\n",
    "The connection lifecycle starts with a health check at `/v1/health`, then establishes the WebSocket, sends session config, and starts bidirectional streaming.\n",
    "\n",
    "### How the audio streaming works\n",
    "\n",
    "Browser → Backend (Input Audio)\n",
    "\n",
    "The browser continuously captures audio from your microphone and sends it in chunks:\n",
    "\n",
    "1. Browser records audio in **Opus format** (24kHz, mono)\n",
    "2. Encodes chunks as **base64**\n",
    "3. Sends `input_audio_buffer.append` messages via WebSocket\n",
    "4. Backend forwards this to the STT server\n",
    "5. STT transcribes in real-time and detects when you stop speaking\n",
    "\n",
    "Backend → Browser (Output Audio)\n",
    "\n",
    "Once the LLM starts generating text:\n",
    "\n",
    "1. Backend feeds text tokens to TTS as they arrive\n",
    "2. TTS generates audio chunks (also Opus, 24kHz)\n",
    "3. Backend sends `response.audio.delta` messages with base64 audio\n",
    "4. Browser decodes and plays audio immediately\n",
    "5. Continues until LLM finishes generating\n",
    "\n",
    "Key Feature: Streaming in Text\n",
    "\n",
    "The magic is that TTS doesn't wait for complete text - it starts generating audio from the first few tokens. Combined with the 220ms TTS latency, this creates very responsive conversations.\n",
    "\n",
    "The browser also receives `response.text.delta` messages showing what's being said, which enables subtitles.\n",
    "\n",
    "### Backend implementation\n",
    "\n",
    "That's a broad question! The backend has quite a few components. Let me break down what I know, and you can tell me which parts you want to dive deeper into.\n",
    "\n",
    "Backend Overview\n",
    "\n",
    "**Main entry point:** `unmute/main_websocket.py` - handles WebSocket connections\n",
    "\n",
    "**Key handler:** `unmute_handler.py` - orchestrates the conversation flow and populates `self.debug_dict` for debugging\n",
    "\n",
    "Major Components\n",
    "\n",
    "1. **WebSocket server** - receives audio, sends responses\n",
    "2. **STT client** - connects to speech-to-text server via WebSocket\n",
    "3. **LLM client** - queries text generation (supports any OpenAI-compatible API)\n",
    "4. **TTS client** - connects to text-to-speech server via WebSocket\n",
    "5. **Session management** - handles voice configs from `voices.yaml`\n",
    "6. **System prompts** - defined in `unmute/llm/system_prompt.py`\n",
    "\n",
    "Technology Stack\n",
    "\n",
    "- **FastAPI** for the web server\n",
    "- **WebSocket** for real-time communication\n",
    "- **Python** with `uv` for dependency management\n",
    "\n",
    "### Backend orchestration logic\n",
    "\n",
    "The orchestration logic is the heart of how the backend coordinates everything. Here's what happens:\n",
    "\n",
    "Main Flow\n",
    "\n",
    "The backend acts as a **central coordinator** managing three concurrent streams:\n",
    "\n",
    "**1. Audio Input Stream (You → STT)**\n",
    "- Receives audio chunks from browser\n",
    "- Forwards to STT server via WebSocket\n",
    "- STT transcribes and watches for end-of-speech\n",
    "\n",
    "**2. Text Generation (STT → LLM → TTS)**\n",
    "- When STT detects you're done speaking\n",
    "- Backend sends transcript to LLM\n",
    "- LLM starts generating response tokens\n",
    "- Tokens immediately stream to TTS (no waiting!)\n",
    "\n",
    "**3. Audio Output Stream (TTS → You)**\n",
    "- TTS generates audio from incoming text tokens\n",
    "- Backend forwards audio chunks to browser\n",
    "- Continues until LLM finishes\n",
    "\n",
    "State Management\n",
    "\n",
    "The backend must track:\n",
    "- Current conversation state (listening vs. responding)\n",
    "- Active sessions and their configurations\n",
    "- Debug information in `self.debug_dict`\n",
    "- Word timestamps for interruption handling\n",
    "\n",
    "The \"Flush Trick\"\n",
    "\n",
    "When end-of-speech is detected, the backend tells STT to process remaining audio at **4x speed** to reduce the 500ms delay to ~125ms.\n",
    "\n",
    "### Concurrent connections\n",
    "\n",
    "Batching Support\n",
    "\n",
    "The key to handling concurrent connections is **batching** - remember how DSM operates at a constant framerate? This means:\n",
    "\n",
    "- Multiple user conversations can be processed **simultaneously** on the same GPU\n",
    "- All streams advance in lockstep (one frame per step)\n",
    "- No need for complex scheduling or per-user logic\n",
    "\n",
    "Throughput Numbers\n",
    "\n",
    "From the documentation:\n",
    "\n",
    "**STT (Rust server on L40S GPU):**\n",
    "- 64 simultaneous connections at 3x real-time factor\n",
    "- On H100: up to 400 concurrent streams!\n",
    "\n",
    "**TTS (Rust server on L40S GPU):**\n",
    "- 16 simultaneous connections at 2x real-time factor\n",
    "- 32 connections with batching at 350ms latency\n",
    "\n",
    "Implementation\n",
    "\n",
    "The actual STT and TTS servers are **Rust-based** for performance, providing WebSocket endpoints that the Python backend connects to. The backend itself uses **FastAPI with WebSocket support** to handle multiple browser connections.\n",
    "\n",
    "Each browser connection gets its own handler instance that manages that conversation's state independently.\n",
    "\n",
    "### Rust servers\n",
    "\n",
    "The Rust servers handle the actual STT and TTS inference. Here's what I know:\n",
    "\n",
    "Purpose\n",
    "\n",
    "The STT and TTS models run in **separate Rust servers** for performance reasons. They expose WebSocket endpoints that the Python backend connects to.\n",
    "\n",
    "Performance Benefits\n",
    "\n",
    "Rust provides:\n",
    "- Lower latency than Python\n",
    "- Better memory management\n",
    "- Efficient GPU utilization for batching\n",
    "\n",
    "Key Features\n",
    "\n",
    "**STT Rust Server:**\n",
    "- Processes audio faster than real-time (4x speed for flush trick)\n",
    "- Handles batching of multiple streams\n",
    "- Implements the semantic VAD\n",
    "\n",
    "**TTS Rust Server:**\n",
    "- Streams audio generation\n",
    "- Handles the action stream for word timing\n",
    "- Manages speaker embeddings\n",
    "\n",
    "Repository\n",
    "\n",
    "Both are in the `delayed-streams-modeling` GitHub repo (mentioned in the docs). The Unmute repo references this for running the servers.\n",
    "\n",
    "The servers use WebSocket for communication, accepting audio/text input and streaming results back.\n",
    "\n",
    "Multi-GPU Setup\n",
    "\n",
    "The system can distribute components across different GPUs:\n",
    "\n",
    "**Single GPU (16GB minimum):**\n",
    "- All three components (STT, TTS, LLM) share one GPU\n",
    "- Works but has higher latency (~750ms for TTS on L40S)\n",
    "\n",
    "**Three GPU setup (recommended):**\n",
    "- STT on GPU 0\n",
    "- TTS on GPU 1  \n",
    "- LLM on GPU 2\n",
    "- Reduces TTS latency to ~450ms\n",
    "\n",
    "Configuration\n",
    "\n",
    "In Docker Compose, you can assign specific GPUs using:\n",
    "```yaml\n",
    "deploy:\n",
    "  resources:\n",
    "    reservations:\n",
    "      devices:\n",
    "        - driver: nvidia\n",
    "          count: 1\n",
    "          capabilities: [gpu]\n",
    "```\n",
    "\n",
    "Why This Helps\n",
    "\n",
    "Each component runs independently without competing for GPU memory or compute. The Rust servers handle their own GPU batching efficiently.\n",
    "\n",
    "On unmute.sh, they use this three-GPU setup for optimal performance.\n",
    "\n",
    "### Docker compose file\n",
    "\n",
    "#### Overview\n",
    "\n",
    "This file defines 5 services that work together to run Unmute:\n",
    "1. **traefik** - Reverse proxy/router\n",
    "2. **frontend** - Next.js web interface\n",
    "3. **backend** - Python orchestration service\n",
    "4. **tts** - Text-to-speech Rust server\n",
    "5. **stt** - Speech-to-text Rust server\n",
    "6. **llm** - Language model (vLLM)\n",
    "\n",
    "#### Traefik (Reverse Proxy)\n",
    "\n",
    "This routes incoming HTTP requests to the right service:\n",
    "- Listens on port 80\n",
    "- Routes `/api/*` requests → backend\n",
    "- Routes everything else → frontend\n",
    "- Currently HTTP only (no HTTPS)\n",
    "\n",
    "The priority system ensures API calls go to backend first (priority 100) before falling through to frontend (priority 10).\n",
    "\n",
    "#### Frontend Service\n",
    "\n",
    "**What it does:**\n",
    "- Builds the Next.js frontend from the `frontend/` directory\n",
    "- Uses a special hot-reloading Dockerfile for development\n",
    "- Mounts the source code so changes appear instantly without rebuilding\n",
    "\n",
    "**Traefik routing:**\n",
    "- Catches all requests that don't match other routes\n",
    "- Internally runs on port 3000\n",
    "- Lowest priority (so backend API routes take precedence)\n",
    "\n",
    "The volume mounting means you can edit frontend code and see changes immediately without restarting the container.\n",
    "\n",
    "#### Backend Service\n",
    "\n",
    "**What it does:**\n",
    "- Builds from the root directory (contains Python code)\n",
    "- Uses hot-reloading for development\n",
    "- Mounts the `unmute/` directory for live code changes\n",
    "\n",
    "**Environment variables:**\n",
    "```yaml\n",
    "environment:\n",
    "  - KYUTAI_STT_URL=ws://stt:8080\n",
    "  - KYUTAI_TTS_URL=ws://tts:8080\n",
    "  - KYUTAI_LLM_URL=http://llm:8000\n",
    "```\n",
    "\n",
    "These tell the backend how to connect to the other services. Notice Docker's internal networking - `stt`, `tts`, and `llm` are service names that resolve automatically.\n",
    "\n",
    "**Traefik routing:**\n",
    "\n",
    "Requests to `/api/something` get routed here, and the `/api` prefix is stripped before reaching the backend (so it sees `/something`).\n",
    "\n",
    "#### STT and TTS Services\n",
    "\n",
    "Both have very similar configurations:\n",
    "\n",
    "**Key points:**\n",
    "- Both use the same Rust-based `moshi-server` image\n",
    "- Different config files specify STT vs TTS behavior\n",
    "- Need HuggingFace token to download models\n",
    "\n",
    "**GPU access:**\n",
    "```yaml\n",
    "deploy:\n",
    "  resources:\n",
    "    reservations:\n",
    "      devices:\n",
    "        - driver: nvidia\n",
    "          count: all\n",
    "```\n",
    "\n",
    "Currently set to use **all GPUs**. For multi-GPU setups, you'd change `count: all` to `count: 1` to dedicate one GPU per service.\n",
    "\n",
    "#### LLM Service\n",
    "\n",
    "```yaml\n",
    "llm:\n",
    "  image: vllm/vllm-openai:v0.9.1\n",
    "  command:\n",
    "    [\n",
    "      \"--model=meta-llama/Llama-3.2-1B-Instruct\",\n",
    "      \"--max-model-len=1536\",\n",
    "      \"--dtype=bfloat16\",\n",
    "      \"--gpu-memory-utilization=0.4\",\n",
    "    ]\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Runs vLLM (fast LLM inference server)\n",
    "- Uses Llama 3.2 1B by default (small, fits in 16GB GPU)\n",
    "- Exposes OpenAI-compatible API on port 8000\n",
    "\n",
    "**Key parameters:**\n",
    "\n",
    "`--max-model-len=1536` - Maximum conversation length (tokens). Higher = longer conversations but more memory.\n",
    "\n",
    "`--gpu-memory-utilization=0.4` - Uses 40% of GPU memory. You can increase this if running LLM on dedicated GPU.\n",
    "\n",
    "`--dtype=bfloat16` - Uses 16-bit precision for efficiency\n",
    "\n",
    "**Volumes:**\n",
    "Same caching strategy as STT/TTS to avoid re-downloading models.\n",
    "\n",
    "**NOTE comments** in the file suggest places you might customize (different model, more memory, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28c44c5-689e-464f-bf47-c9025defcb67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-voice",
   "language": "python",
   "name": "wordslab-voice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
