{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70137d90-bacc-4baa-a5db-93ac5516db2c",
   "metadata": {},
   "source": [
    "## System implementation - Unmute\n",
    "\n",
    "Great! Now I can give you a complete picture of how the full Unmute system works.\n",
    "\n",
    "System Overview\n",
    "\n",
    "**Unmute** is a cascaded system that lets you have voice conversations with any text-based LLM. It's called \"cascaded\" because it chains three separate components:\n",
    "\n",
    "1. **DSM-ASR** (Speech-to-Text) - transcribes what you say\n",
    "2. **LLM** (any text model) - generates text response\n",
    "3. **DSM-TTS** (Text-to-Speech) - speaks the response\n",
    "\n",
    "Architecture\n",
    "\n",
    "```\n",
    "User Browser ←→ Backend ←→ STT Server\n",
    "                         ↓\n",
    "                         LLM Server\n",
    "                         ↓\n",
    "                         TTS Server\n",
    "```\n",
    "\n",
    "The user's browser connects via WebSocket to a backend server, which orchestrates everything:\n",
    "\n",
    "**Flow:**\n",
    "1. Browser sends audio → Backend → STT transcribes it\n",
    "2. STT detects end of speech (semantic VAD)\n",
    "3. Backend sends transcript → LLM generates response\n",
    "4. As LLM generates tokens → TTS starts speaking immediately (streaming in text!)\n",
    "5. Audio streams back to browser\n",
    "\n",
    "Key Innovation: Streaming in Text\n",
    "\n",
    "Unlike other TTS systems that need the complete text first, DSM-TTS can start generating audio as soon as it receives the first few tokens from the LLM. This dramatically reduces latency.\n",
    "\n",
    "The system uses a \"flush trick\" to reduce latency further - when speech ends, it processes remaining audio at 4x speed.\n",
    "\n",
    "### Frontend\n",
    "\n",
    "Frontend Technology\n",
    "\n",
    "The frontend is a **Next.js app** (React-based framework) located in the `frontend/` directory. It runs on port 3000 by default.\n",
    "\n",
    "Communication Protocol\n",
    "\n",
    "The frontend and backend communicate via **WebSocket** using a protocol based on the **OpenAI Realtime API** (ORA). However, Unmute makes some modifications:\n",
    "- Some extra message types were added\n",
    "- Some parameters are simplified\n",
    "- Not fully compatible with ORA yet (but they're working toward it)\n",
    "\n",
    "The protocol details are defined in `unmute/openai_realtime_api_events.py`.\n",
    "\n",
    "Audio Processing\n",
    "\n",
    "The browser:\n",
    "- Captures audio from the user's microphone\n",
    "- Sends it over WebSocket to the backend in real-time\n",
    "- Receives audio back from the TTS\n",
    "- Plays it to the user\n",
    "\n",
    "#UI Features\n",
    "\n",
    "**Keyboard shortcuts:**\n",
    "- Press **S** for subtitles (shows transcription for both user and chatbot)\n",
    "- Press **D** for dev mode (debug view with extra info)\n",
    "\n",
    "**User controls:**\n",
    "- Can interrupt the AI mid-response\n",
    "- Can change voices and system prompts\n",
    "- Voice activity detection shows \"End of speech detected\"\n",
    "\n",
    "There's also a Python client implementation in `unmute/loadtest/loadtest_client.py` that demonstrates the protocol from a different angle - it's used for benchmarking.\n",
    "\n",
    "### How the interruption by the user mid-reponse works\n",
    "\n",
    "The interruption mechanism uses the **word-level timestamps** from DSM-TTS.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "**During generation:**\n",
    "- DSM-TTS outputs audio chunks along with precise timestamps for each word\n",
    "- The system tracks exactly which words have been spoken and when\n",
    "\n",
    "**When user interrupts:**\n",
    "- The frontend detects the user starting to speak (via voice activity detection)\n",
    "- It signals the backend to stop the current TTS generation\n",
    "- Because the system knows the exact timestamp where it stopped, it knows which part of the LLM's response was actually spoken and which wasn't\n",
    "\n",
    "**The clever part:**\n",
    "The backend can then inform the LLM context about what was actually said vs. what was cut off. This means the conversation can continue naturally - the AI knows what the user heard and what they didn't.\n",
    "\n",
    "The paper mentions: \"If you interrupt mid-way through an explanation to ask a follow-up question, Unmute will know exactly where it got interrupted and which part of the explanation still remains to be said later.\"\n",
    "\n",
    "### How the voice activity detection works\n",
    "\n",
    "The Voice Activity Detection (VAD) in Unmute is particularly clever - it's **semantic** rather than just acoustic.\n",
    "\n",
    "Traditional VAD Problem\n",
    "\n",
    "Most voice systems use a separate VAD model that detects if someone is speaking or not, then waits a fixed time (like 500ms) after silence before deciding \"they're done talking.\"\n",
    "\n",
    "The problem: People naturally pause mid-sentence! A fixed timeout causes either:\n",
    "- False positives (cutting people off mid-thought)\n",
    "- Or long delays (waiting too long to be safe)\n",
    "\n",
    "Kyutai's Semantic VAD Solution\n",
    "\n",
    "Instead of a separate model, **DSM-ASR itself predicts the probability that the user is done talking**. It's built right into the speech-to-text model.\n",
    "\n",
    "The key insight: The delay adapts based on **content and intonation**. The model can tell the difference between:\n",
    "- \"I went to the store...\" (pause, more coming)\n",
    "- \"I went to the store.\" (done, falling intonation)\n",
    "\n",
    "How It Works\n",
    "\n",
    "The STT model outputs both:\n",
    "1. Text transcription\n",
    "2. End-of-speech probability\n",
    "\n",
    "When this probability crosses a threshold, the system triggers the LLM response.\n",
    "\n",
    "This is what you see in the UI when it shows \"End of speech detected.\"\n",
    "\n",
    "### Protocol and messages used between frontend and backend\n",
    "\n",
    "**Message format:**\n",
    "- Based on OpenAI Realtime API format\n",
    "- Defined in `unmute/openai_realtime_api_events.py`\n",
    "- Contains both standard ORA messages and custom Unmute extensions\n",
    "\n",
    "**Audio handling:**\n",
    "- Browser sends raw audio data over WebSocket\n",
    "- Backend streams audio back for playback\n",
    "- Real-time bidirectional communication\n",
    "\n",
    "**Debug info:**\n",
    "- Backend populates `self.debug_dict` in `unmute_handler.py`\n",
    "- This gets sent to frontend for the dev mode view\n",
    "\n",
    "WebSocket Connection\n",
    "\n",
    "**Endpoint:** `/v1/realtime` using the `realtime` subprotocol\n",
    "**Port:** 8000 (dev), or 80/443 through Traefik (production)\n",
    "\n",
    "Audio Format\n",
    "\n",
    "All audio uses:\n",
    "- **Codec:** Opus\n",
    "- **Sample rate:** 24kHz  \n",
    "- **Channels:** Mono\n",
    "- **Encoding:** Base64-encoded bytes\n",
    "\n",
    "Key Message Types\n",
    "\n",
    "**Client → Server:**\n",
    "\n",
    "1. **`input_audio_buffer.append`** - Streams microphone audio to backend\n",
    "2. **`session.update`** - Configures voice character and instructions (required to start!)\n",
    "\n",
    "**Server → Client:**\n",
    "\n",
    "1. **`response.audio.delta`** - Streams generated speech back\n",
    "2. **`conversation.item.input_audio_transcription.delta`** - Real-time user transcription\n",
    "3. **`response.text.delta`** - Text of what AI is saying (for display)\n",
    "4. **`input_audio_buffer.speech_started/stopped`** - VAD events (currently ignored)\n",
    "5. **`response.created`** - Signals assistant is generating a response\n",
    "6. **`error`** - Error/warning messages\n",
    "\n",
    "All messages are JSON with a common structure defined in `openai_realtime_api_events.py`.\n",
    "\n",
    "The connection lifecycle starts with a health check at `/v1/health`, then establishes the WebSocket, sends session config, and starts bidirectional streaming.\n",
    "\n",
    "### How the audio streaming works\n",
    "\n",
    "Browser → Backend (Input Audio)\n",
    "\n",
    "The browser continuously captures audio from your microphone and sends it in chunks:\n",
    "\n",
    "1. Browser records audio in **Opus format** (24kHz, mono)\n",
    "2. Encodes chunks as **base64**\n",
    "3. Sends `input_audio_buffer.append` messages via WebSocket\n",
    "4. Backend forwards this to the STT server\n",
    "5. STT transcribes in real-time and detects when you stop speaking\n",
    "\n",
    "Backend → Browser (Output Audio)\n",
    "\n",
    "Once the LLM starts generating text:\n",
    "\n",
    "1. Backend feeds text tokens to TTS as they arrive\n",
    "2. TTS generates audio chunks (also Opus, 24kHz)\n",
    "3. Backend sends `response.audio.delta` messages with base64 audio\n",
    "4. Browser decodes and plays audio immediately\n",
    "5. Continues until LLM finishes generating\n",
    "\n",
    "Key Feature: Streaming in Text\n",
    "\n",
    "The magic is that TTS doesn't wait for complete text - it starts generating audio from the first few tokens. Combined with the 220ms TTS latency, this creates very responsive conversations.\n",
    "\n",
    "The browser also receives `response.text.delta` messages showing what's being said, which enables subtitles.\n",
    "\n",
    "### Backend implementation\n",
    "\n",
    "That's a broad question! The backend has quite a few components. Let me break down what I know, and you can tell me which parts you want to dive deeper into.\n",
    "\n",
    "Backend Overview\n",
    "\n",
    "**Main entry point:** `unmute/main_websocket.py` - handles WebSocket connections\n",
    "\n",
    "**Key handler:** `unmute_handler.py` - orchestrates the conversation flow and populates `self.debug_dict` for debugging\n",
    "\n",
    "Major Components\n",
    "\n",
    "1. **WebSocket server** - receives audio, sends responses\n",
    "2. **STT client** - connects to speech-to-text server via WebSocket\n",
    "3. **LLM client** - queries text generation (supports any OpenAI-compatible API)\n",
    "4. **TTS client** - connects to text-to-speech server via WebSocket\n",
    "5. **Session management** - handles voice configs from `voices.yaml`\n",
    "6. **System prompts** - defined in `unmute/llm/system_prompt.py`\n",
    "\n",
    "Technology Stack\n",
    "\n",
    "- **FastAPI** for the web server\n",
    "- **WebSocket** for real-time communication\n",
    "- **Python** with `uv` for dependency management\n",
    "\n",
    "### Backend orchestration logic\n",
    "\n",
    "The orchestration logic is the heart of how the backend coordinates everything. Here's what happens:\n",
    "\n",
    "Main Flow\n",
    "\n",
    "The backend acts as a **central coordinator** managing three concurrent streams:\n",
    "\n",
    "**1. Audio Input Stream (You → STT)**\n",
    "- Receives audio chunks from browser\n",
    "- Forwards to STT server via WebSocket\n",
    "- STT transcribes and watches for end-of-speech\n",
    "\n",
    "**2. Text Generation (STT → LLM → TTS)**\n",
    "- When STT detects you're done speaking\n",
    "- Backend sends transcript to LLM\n",
    "- LLM starts generating response tokens\n",
    "- Tokens immediately stream to TTS (no waiting!)\n",
    "\n",
    "**3. Audio Output Stream (TTS → You)**\n",
    "- TTS generates audio from incoming text tokens\n",
    "- Backend forwards audio chunks to browser\n",
    "- Continues until LLM finishes\n",
    "\n",
    "State Management\n",
    "\n",
    "The backend must track:\n",
    "- Current conversation state (listening vs. responding)\n",
    "- Active sessions and their configurations\n",
    "- Debug information in `self.debug_dict`\n",
    "- Word timestamps for interruption handling\n",
    "\n",
    "The \"Flush Trick\"\n",
    "\n",
    "When end-of-speech is detected, the backend tells STT to process remaining audio at **4x speed** to reduce the 500ms delay to ~125ms.\n",
    "\n",
    "### Concurrent connections\n",
    "\n",
    "Batching Support\n",
    "\n",
    "The key to handling concurrent connections is **batching** - remember how DSM operates at a constant framerate? This means:\n",
    "\n",
    "- Multiple user conversations can be processed **simultaneously** on the same GPU\n",
    "- All streams advance in lockstep (one frame per step)\n",
    "- No need for complex scheduling or per-user logic\n",
    "\n",
    "Throughput Numbers\n",
    "\n",
    "From the documentation:\n",
    "\n",
    "**STT (Rust server on L40S GPU):**\n",
    "- 64 simultaneous connections at 3x real-time factor\n",
    "- On H100: up to 400 concurrent streams!\n",
    "\n",
    "**TTS (Rust server on L40S GPU):**\n",
    "- 16 simultaneous connections at 2x real-time factor\n",
    "- 32 connections with batching at 350ms latency\n",
    "\n",
    "Implementation\n",
    "\n",
    "The actual STT and TTS servers are **Rust-based** for performance, providing WebSocket endpoints that the Python backend connects to. The backend itself uses **FastAPI with WebSocket support** to handle multiple browser connections.\n",
    "\n",
    "Each browser connection gets its own handler instance that manages that conversation's state independently.\n",
    "\n",
    "### Rust servers\n",
    "\n",
    "The Rust servers handle the actual STT and TTS inference. Here's what I know:\n",
    "\n",
    "Purpose\n",
    "\n",
    "The STT and TTS models run in **separate Rust servers** for performance reasons. They expose WebSocket endpoints that the Python backend connects to.\n",
    "\n",
    "Performance Benefits\n",
    "\n",
    "Rust provides:\n",
    "- Lower latency than Python\n",
    "- Better memory management\n",
    "- Efficient GPU utilization for batching\n",
    "\n",
    "Key Features\n",
    "\n",
    "**STT Rust Server:**\n",
    "- Processes audio faster than real-time (4x speed for flush trick)\n",
    "- Handles batching of multiple streams\n",
    "- Implements the semantic VAD\n",
    "\n",
    "**TTS Rust Server:**\n",
    "- Streams audio generation\n",
    "- Handles the action stream for word timing\n",
    "- Manages speaker embeddings\n",
    "\n",
    "Repository\n",
    "\n",
    "Both are in the `delayed-streams-modeling` GitHub repo (mentioned in the docs). The Unmute repo references this for running the servers.\n",
    "\n",
    "The servers use WebSocket for communication, accepting audio/text input and streaming results back.\n",
    "\n",
    "Multi-GPU Setup\n",
    "\n",
    "The system can distribute components across different GPUs:\n",
    "\n",
    "**Single GPU (16GB minimum):**\n",
    "- All three components (STT, TTS, LLM) share one GPU\n",
    "- Works but has higher latency (~750ms for TTS on L40S)\n",
    "\n",
    "**Three GPU setup (recommended):**\n",
    "- STT on GPU 0\n",
    "- TTS on GPU 1  \n",
    "- LLM on GPU 2\n",
    "- Reduces TTS latency to ~450ms\n",
    "\n",
    "Configuration\n",
    "\n",
    "In Docker Compose, you can assign specific GPUs using:\n",
    "```yaml\n",
    "deploy:\n",
    "  resources:\n",
    "    reservations:\n",
    "      devices:\n",
    "        - driver: nvidia\n",
    "          count: 1\n",
    "          capabilities: [gpu]\n",
    "```\n",
    "\n",
    "Why This Helps\n",
    "\n",
    "Each component runs independently without competing for GPU memory or compute. The Rust servers handle their own GPU batching efficiently.\n",
    "\n",
    "On unmute.sh, they use this three-GPU setup for optimal performance.\n",
    "\n",
    "### Docker compose file\n",
    "\n",
    "#### Overview\n",
    "\n",
    "This file defines 5 services that work together to run Unmute:\n",
    "1. **traefik** - Reverse proxy/router\n",
    "2. **frontend** - Next.js web interface\n",
    "3. **backend** - Python orchestration service\n",
    "4. **tts** - Text-to-speech Rust server\n",
    "5. **stt** - Speech-to-text Rust server\n",
    "6. **llm** - Language model (vLLM)\n",
    "\n",
    "#### Traefik (Reverse Proxy)\n",
    "\n",
    "This routes incoming HTTP requests to the right service:\n",
    "- Listens on port 80\n",
    "- Routes `/api/*` requests → backend\n",
    "- Routes everything else → frontend\n",
    "- Currently HTTP only (no HTTPS)\n",
    "\n",
    "The priority system ensures API calls go to backend first (priority 100) before falling through to frontend (priority 10).\n",
    "\n",
    "#### Frontend Service\n",
    "\n",
    "**What it does:**\n",
    "- Builds the Next.js frontend from the `frontend/` directory\n",
    "- Uses a special hot-reloading Dockerfile for development\n",
    "- Mounts the source code so changes appear instantly without rebuilding\n",
    "\n",
    "**Traefik routing:**\n",
    "- Catches all requests that don't match other routes\n",
    "- Internally runs on port 3000\n",
    "- Lowest priority (so backend API routes take precedence)\n",
    "\n",
    "The volume mounting means you can edit frontend code and see changes immediately without restarting the container.\n",
    "\n",
    "#### Backend Service\n",
    "\n",
    "**What it does:**\n",
    "- Builds from the root directory (contains Python code)\n",
    "- Uses hot-reloading for development\n",
    "- Mounts the `unmute/` directory for live code changes\n",
    "\n",
    "**Environment variables:**\n",
    "```yaml\n",
    "environment:\n",
    "  - KYUTAI_STT_URL=ws://stt:8080\n",
    "  - KYUTAI_TTS_URL=ws://tts:8080\n",
    "  - KYUTAI_LLM_URL=http://llm:8000\n",
    "```\n",
    "\n",
    "These tell the backend how to connect to the other services. Notice Docker's internal networking - `stt`, `tts`, and `llm` are service names that resolve automatically.\n",
    "\n",
    "**Traefik routing:**\n",
    "\n",
    "Requests to `/api/something` get routed here, and the `/api` prefix is stripped before reaching the backend (so it sees `/something`).\n",
    "\n",
    "#### STT and TTS Services\n",
    "\n",
    "Both have very similar configurations:\n",
    "\n",
    "**Key points:**\n",
    "- Both use the same Rust-based `moshi-server` image\n",
    "- Different config files specify STT vs TTS behavior\n",
    "- Need HuggingFace token to download models\n",
    "\n",
    "**GPU access:**\n",
    "```yaml\n",
    "deploy:\n",
    "  resources:\n",
    "    reservations:\n",
    "      devices:\n",
    "        - driver: nvidia\n",
    "          count: all\n",
    "```\n",
    "\n",
    "Currently set to use **all GPUs**. For multi-GPU setups, you'd change `count: all` to `count: 1` to dedicate one GPU per service.\n",
    "\n",
    "#### LLM Service\n",
    "\n",
    "```yaml\n",
    "llm:\n",
    "  image: vllm/vllm-openai:v0.9.1\n",
    "  command:\n",
    "    [\n",
    "      \"--model=meta-llama/Llama-3.2-1B-Instruct\",\n",
    "      \"--max-model-len=1536\",\n",
    "      \"--dtype=bfloat16\",\n",
    "      \"--gpu-memory-utilization=0.4\",\n",
    "    ]\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Runs vLLM (fast LLM inference server)\n",
    "- Uses Llama 3.2 1B by default (small, fits in 16GB GPU)\n",
    "- Exposes OpenAI-compatible API on port 8000\n",
    "\n",
    "**Key parameters:**\n",
    "\n",
    "`--max-model-len=1536` - Maximum conversation length (tokens). Higher = longer conversations but more memory.\n",
    "\n",
    "`--gpu-memory-utilization=0.4` - Uses 40% of GPU memory. You can increase this if running LLM on dedicated GPU.\n",
    "\n",
    "`--dtype=bfloat16` - Uses 16-bit precision for efficiency\n",
    "\n",
    "**Volumes:**\n",
    "Same caching strategy as STT/TTS to avoid re-downloading models.\n",
    "\n",
    "**NOTE comments** in the file suggest places you might customize (different model, more memory, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597b78af-2d4c-4aa1-8bea-e7454e66bce7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
