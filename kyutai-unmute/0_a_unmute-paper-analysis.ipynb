{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c39ff10-d6e2-4a35-86ef-4395ff1b18e8",
   "metadata": {},
   "source": [
    "## Research paper - Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling\n",
    "\n",
    "### Key ideas and results\n",
    "\n",
    "The paper introduces **Delayed Streams Modeling (DSM)** - a new approach for streaming sequence-to-sequence learning. The key hypothesis is that by aligning different modalities (like audio and text) to a shared framerate and introducing appropriate delays between them, you can achieve real-time streaming generation without needing complex alignment policies.\n",
    "\n",
    "The main contributions are:\n",
    "- A flexible framework that works for multiple tasks (ASR and TTS) with the same architecture\n",
    "- State-of-the-art performance with low latency (as low as a few hundred milliseconds)\n",
    "- Support for arbitrarily long sequences with batching capability\n",
    "\n",
    "The core idea is surprisingly elegant. Instead of processing all input before generating output (offline) or learning complex policies about when to read/write (traditional streaming), DSM:\n",
    "\n",
    "- **Aligns sequences**: Both audio and text are aligned to the same framerate (12.5 Hz)\n",
    "- **Introduces delays**: The output stream is delayed by τ steps relative to the input, creating a \"lookahead\" window\n",
    "- **Uses parallel streams**: A decoder-only transformer processes multiple token streams simultaneously\n",
    "\n",
    "For **ASR** (speech-to-text): audio is the input stream, text is delayed and predicted\n",
    "For **TTS** (text-to-speech): text is the input, audio is delayed and generated\n",
    "\n",
    "Performance Results\n",
    "\n",
    "**DSM-ASR (Automatic Speech Recognition):**\n",
    "- Achieves **7.9% average WER** (Word Error Rate) on long-form datasets\n",
    "- Competitive with or better than non-streaming models like Whisper-Large-V2 (9.0% WER)\n",
    "- Outperforms other streaming models\n",
    "- Provides word-level timestamps with 80ms precision\n",
    "\n",
    "**DSM-TTS (Text-to-Speech):**\n",
    "- Achieves **1.58-1.71% WER** when evaluated (lower is better)\n",
    "- Speaker similarity score of **0.70-0.71** (comparable to baselines)\n",
    "- Competitive with non-streaming models like F5-TTS and Cosyvoice\n",
    "- Only model providing long-form synthesis in a streaming manner\n",
    "\n",
    "Key Advantages\n",
    "\n",
    "The paper emphasizes DSM provides:\n",
    "- **Low latency**: As low as a few hundred milliseconds\n",
    "- **Batching support**: Unlike most streaming models\n",
    "- **Arbitrary length sequences**: No chunking needed\n",
    "- **Single architecture**: Same model structure for both ASR and TTS\n",
    "\n",
    "### How do you align audio and text on the same framerate\n",
    "\n",
    "Great question! This is a crucial part of making DSM work.\n",
    "\n",
    "For **audio**, they use a neural codec called **Mimi** that compresses the raw waveform:\n",
    "- Takes audio at 24kHz sample rate\n",
    "- Compresses it down to 12.5 Hz (so 12.5 \"frames\" per second)\n",
    "- Each frame represents 80ms of audio\n",
    "- Uses vector quantization to turn it into discrete tokens\n",
    "\n",
    "For **text**, they use **word-level timestamps**:\n",
    "- Each word has a timestamp indicating when it's spoken\n",
    "- They place the word tokens at the corresponding frame position\n",
    "- For example, if a word starts at 0.5 seconds, it goes at frame position 6 (0.5 × 12.5)\n",
    "- They use special tokens: **WORD** (marks word start), **PAD** (empty frames between words)\n",
    "- Frames without words get filled with PAD tokens\n",
    "\n",
    "So if someone says \"Hello world\" where \"Hello\" starts at 0s and \"world\" starts at 0.4s, the aligned sequence might look like:\n",
    "```\n",
    "Frame 0: WORD, H, e, l, l, o\n",
    "Frame 5: WORD, w, o, r, l, d\n",
    "Frames 6+: PAD\n",
    "```\n",
    "\n",
    "The challenge they mention is that most speech datasets only have sentence-level timing, not word-level. \n",
    "\n",
    "### Text tokenization, and how text and audio tokens are aligned\n",
    "\n",
    "Text Tokenization\n",
    "\n",
    "They use a **custom vocabulary** specifically trained on speech transcription data (not a standard text tokenizer). The vocabulary has:\n",
    "- Regular word tokens (vocabulary size 8000)\n",
    "- Two special tokens: **PAD** and **WORD**\n",
    "\n",
    "The Alignment Process\n",
    "\n",
    "Here's how they align text to the 12.5 Hz audio framerate:\n",
    "\n",
    "1. **Start with word-level timestamps**: Each word has a start time (e.g., \"hello\" starts at 0.24 seconds)\n",
    "\n",
    "2. **Convert time to frame index**: Multiply the start time by the framerate\n",
    "   - Example: 0.24s × 12.5 = frame 3\n",
    "\n",
    "3. **Place tokens in the sequence**:\n",
    "   - Put **WORD** token at the start frame\n",
    "   - Follow immediately with the word's sub-tokens (like \"h\", \"e\", \"l\", \"l\", \"o\")\n",
    "   - Fill any empty frames with **PAD**\n",
    "\n",
    "When a word like \"hello\" is tokenized into sub-tokens [h, e, l, l, o], these tokens are placed **consecutively starting from the word's start frame**:\n",
    "\n",
    "```\n",
    "Frame 0: WORD\n",
    "Frame 1: h\n",
    "Frame 2: e\n",
    "Frame 3: l\n",
    "Frame 4: l\n",
    "Frame 5: o\n",
    "Frame 6: PAD (until next word)\n",
    "```\n",
    "\n",
    "So the word's tokens \"flow forward\" in time, occupying consecutive frames. The **WORD** token marks where a new word begins, then its sub-tokens follow in sequence.\n",
    "\n",
    "This means:\n",
    "- Short words might only take 1-2 frames\n",
    "- Longer words could span many frames\n",
    "- The actual pronunciation duration doesn't matter for the text stream - it's just sequential token placement\n",
    "- PAD fills gaps between words\n",
    "\n",
    "The audio stream, meanwhile, has tokens at **every** frame representing the actual sound at that moment.\n",
    "\n",
    "During training, the model learns to predict text tokens that are **delayed by τ frames** relative to the audio. So if τ=16 frames (1.28 seconds), the model sees audio frames 0-16 before predicting text frame 0.\n",
    "\n",
    "The audio stream has its own tokens at every frame (from the Mimi codec). Both streams now have exactly one \"event\" per 80ms time step.\n",
    "\n",
    "### Training phases and datasets\n",
    "\n",
    "For DSM-ASR:\n",
    "\n",
    "**Pretraining:**\n",
    "- 2.5 million hours of publicly available audio (English and French)\n",
    "- Transcribed automatically using whisper-timestamped\n",
    "- Trained on 90-second random segments\n",
    "\n",
    "**Finetuning:**\n",
    "- \"A collection of public datasets with ground-truth transcripts\" totaling 28k hours\n",
    "- The paper mentions details are in \"Appendix A.1\"\n",
    "\n",
    "**Long-form adaptation:**\n",
    "- A special \"long-form mixture\" described in \"Appendix A.2\"\n",
    "\n",
    "For DSM-TTS:\n",
    "\n",
    "**Pretraining:**\n",
    "- 150-second audio extracts from the same 2.5M hour collection\n",
    "\n",
    "### Delay conditioning feature\n",
    "\n",
    "The **delay conditioning** feature is a clever training trick that gives you flexibility at inference time.\n",
    "\n",
    "The Problem\n",
    "\n",
    "Normally, you'd train with a fixed delay (say τ=16 frames). But different use cases need different tradeoffs:\n",
    "- **Low latency** (small delay): Faster response, but lower quality transcription\n",
    "- **High quality** (large delay): Better transcription, but more lag\n",
    "\n",
    "Without delay conditioning, you'd need to train a separate model for each delay value you want to support.\n",
    "\n",
    "The Solution\n",
    "\n",
    "Instead, they train **one model** on random delays:\n",
    "- Each training example uses a different randomly sampled delay\n",
    "- The model receives the delay value as an extra input (using a cosine embedding)\n",
    "- The model learns: \"given delay X, predict text accordingly\"\n",
    "\n",
    "At Inference\n",
    "\n",
    "You simply tell the model what delay you want (e.g., 400ms for low latency, or 2 seconds for high quality), and it adjusts its predictions to match that latency/quality tradeoff.\n",
    "\n",
    "Think of it like training a model that can operate at multiple \"speeds\" rather than just one fixed speed.\n",
    "\n",
    "The delay conditioning feature lets you control the quality/latency tradeoff at inference time without retraining.\n",
    "\n",
    "### Batching support\n",
    "\n",
    "This is one of DSM's key practical advantages.\n",
    "\n",
    "**The core insight:** DSM operates at a **constant framerate** (12.5 Hz). At each time step, the model processes exactly one frame for each stream, regardless of what's in it.\n",
    "\n",
    "This means:\n",
    "- Every example in a batch advances by exactly 1 frame per step\n",
    "- All sequences stay synchronized\n",
    "- You can run multiple audio streams through the model simultaneously\n",
    "\n",
    "**Why other streaming models can't batch:**\n",
    "\n",
    "Traditional streaming models use **policies** that decide \"should I read more input or write output?\" These decisions vary per example:\n",
    "- Example 1 might need 3 input frames before writing\n",
    "- Example 2 might write immediately\n",
    "- They get out of sync, so you have to process them one at a time\n",
    "\n",
    "**DSM's advantage:**\n",
    "\n",
    "Since everything moves in lockstep (one frame per step for all streams), you can stack multiple examples and process them together efficiently on a GPU.\n",
    "\n",
    "The paper notes this is \"a feature rarely provided by streaming models\" and helps with throughput.\n",
    "\n",
    "### Speaker voices\n",
    "\n",
    "This is specific to the TTS model and how it controls whose voice to generate.\n",
    "\n",
    "Speaker Encoding Process\n",
    "\n",
    "The model can handle **up to 5 different speakers** in a conversation. For each speaker:\n",
    "\n",
    "1. **Extract a 10-second audio sample** of that speaker (from outside the training segment)\n",
    "2. **Pass it through a speaker encoder** that produces a fixed-dimension embedding\n",
    "3. The speaker encoder uses the same architecture as the Mimi codec encoder\n",
    "4. Convolutional layers are frozen, but Transformer layers are fine-tuned\n",
    "\n",
    "Conditioning the Model\n",
    "\n",
    "The speaker embeddings are fed to the model through **cross-attention layers**:\n",
    "- Concatenate embeddings from all speakers (up to 5)\n",
    "- Add positional embeddings to distinguish which speaker is which\n",
    "- Feed through cross-attention to the main backbone\n",
    "\n",
    "If there are fewer than 5 speakers, they pad with learned embeddings. If more than 5, they randomly select 5.\n",
    "\n",
    "Controlling Turns in Dialogue\n",
    "\n",
    "They use special tokens to control who's speaking:\n",
    "- **MAIN**: Marks when the primary speaker starts talking\n",
    "- **OTHER**: Marks when another speaker takes over\n",
    "\n",
    "At inference, you provide speaker embeddings for each person, then insert MAIN/OTHER tokens to orchestrate the conversation.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "The paper identifies a few key limitations:\n",
    "\n",
    "1. Need for Aligned Domains\n",
    "\n",
    "The biggest limitation they mention is that **DSM requires aligned domains** - meaning you need data where audio and text have word-level timestamps. This \"reduces the amount of gold-standard ground-truth data that can be used for training.\"\n",
    "\n",
    "Most speech datasets only have sentence-level alignment, so they had to:\n",
    "- Use pseudo-labels from Whisper for pretraining\n",
    "- Apply Dynamic Time Warping to derive word alignments for finetuning\n",
    "\n",
    "2. Independence Assumption\n",
    "\n",
    "They note that \"perfect independence is hard to achieve\" - meaning the output at time t isn't truly independent of future input beyond the delay window. For example, in ASR, a named entity might be ambiguous without seeing more context.\n",
    "\n",
    "3. Safety Concerns (TTS)\n",
    "\n",
    "For their TTS model, they kept the speaker encoder closed-source due to impersonation risks. They acknowledge that voice cloning \"opens up both opportunities in inclusive human-machine interactions and risks of fraudulent impersonation.\"\n",
    "\n",
    "The paper mentions they'll extend DSM to more tasks in future work, suggesting the current scope (ASR and TTS) is somewhat limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a7412b-8305-4e42-a810-aa03bab7c18d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
