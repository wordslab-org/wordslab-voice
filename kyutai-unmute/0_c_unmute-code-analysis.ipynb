{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fd8806e-a500-4d1f-a637-8cd0c64d340a",
   "metadata": {},
   "source": [
    "The `main_websocket.py` file implements a **FastAPI-based WebSocket server** that acts as the backend for a real-time audio interaction system, integrating voice-to-text (STT), text-to-speech (TTS), and large language models (LLM) â€” all orchestrated through a custom WebSocket-based protocol inspired by OpenAIâ€™s Realtime API. Here's a detailed breakdown of **how the server works**, including its architecture, communication flow, and state management.\n",
    "\n",
    "---\n",
    "\n",
    "Most important section of code in this file to scale the service:\n",
    "\n",
    "```python\n",
    "# We prefer to scale this by running more instances of the server than having a single\n",
    "# server handle more. This is to avoid the GIL.\n",
    "MAX_CLIENTS = 4\n",
    "SEMAPHORE = asyncio.Semaphore(MAX_CLIENTS)\n",
    "```\n",
    "\n",
    "Number of replicas in the refrence swarm-deploy.yaml\n",
    "- frontend: 5\n",
    "- backend: 16 (cpus: \"1.5\", memory: 1G)\n",
    "- stt: 1 (cpus: \"8\", memory: 16G, kind: gpu value: 1)\n",
    "  - batch_size = 64  (stt-prod.toml)\n",
    "- tts: 3 (cpus: \"8\", memory: 16G, kind: gpu value: 1)\n",
    "  - batch_size = 4  (tts-prod.toml)\n",
    "\n",
    "64 clients\n",
    "- 16 backend x 4 max clients\n",
    "- 1 x 64 batch size pour STT\n",
    "- 3 x 4 batch size pour TTS (6x real time generation ?)\n",
    "  \n",
    "### ðŸ§  High-Level Overview\n",
    "\n",
    "This server serves real-time voice conversations between a user and an AI model:\n",
    "\n",
    "- Accepts live **Opus-encoded audio data over WebSocket**\n",
    "- Converts it into text (**STT**: speech-to-text)\n",
    "- Sends to an LLM for generating responses\n",
    "- Converts the response into synthetic speech (**TTS**: text-to-speech)\n",
    "- Streams back Opus-encoded responses in real time\n",
    "\n",
    "It also supports additional endpoints like `/v1/voices`, `/v1/voice-donation`, etc., to support voice cloning or donations.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”„ Main Steps in Handling a Connection\n",
    "\n",
    "When a WebSocket connects to `/v1/realtime`, here's what happens step-by-step:\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Step 1: Upgrade to WebSocket\n",
    "```python\n",
    "@app.websocket(\"/v1/realtime\")\n",
    "async def websocket_route(websocket: WebSocket):\n",
    "```\n",
    "Accepts a new incoming connection, ensuring the correct subprotocol `\"realtime\"` is used (OpenAI compatibility).\n",
    "\n",
    "Also limits concurrent active sessions (`SEMAPHORE`) to cap resource usage per instance.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Step 2: Initialize Session Handler\n",
    "```python\n",
    "handler = UnmuteHandler()\n",
    "await handler.start_up()\n",
    "```\n",
    "Initializes components such as:\n",
    "- STT/TTS clients\n",
    "- Quest manager (story/game tracking)\n",
    "- Audio buffer, recorder (for logs), event queues\n",
    "\n",
    "All services communicate with external servers via WebSockets or HTTP calls as needed.\n",
    "\n",
    "Note: Service availability is pre-checked before accepting this connection.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Step 3: Start Two Async Loops\n",
    "Inside `_run_route`:\n",
    "```python\n",
    "tg.create_task(receive_loop(...), name=\"receive_loop()\")\n",
    "tg.create_task(emit_loop(...), name=\"emit_loop()\")\n",
    "```\n",
    "\n",
    "Two independent async loops run concurrently inside a `TaskGroup()`:\n",
    "- **Receive Loop**: Handles messages sent by the client\n",
    "- **Emit Loop**: Processes and transmits events generated internally\n",
    "\n",
    "They coordinate using shared `UnmuteHandler` and an event queue (`asyncio.Queue`).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’¬ Communication Protocol â€“ Based on OpenAI Realtime Events\n",
    "\n",
    "Communication follows a lightweight JSON-based messaging pattern where both sides exchange predefined structures called â€œeventsâ€.\n",
    "\n",
    "### ðŸ‘¤ Client â†’ Server\n",
    "Events sent **by the client** include:\n",
    "| Event Type                    | Purpose |\n",
    "|------------------------------|---------|\n",
    "| `input_audio_buffer.append`  | Add new chunk of encoded (Opus) audio. |\n",
    "| `session.update`             | Update configuration of current session. |\n",
    "\n",
    "These are validated and dispatched accordingly.\n",
    "\n",
    "#### Special Logic:\n",
    "- Only valid Opus chunks marked with â€œfirst packetâ€ flag after reconnection accepted\n",
    "- Invalid/unrecognized events trigger error feedback\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ¤– Server â†’ Client\n",
    "Events produced **by the server/UI handler** include:\n",
    "\n",
    "| Event Type                  | Description |\n",
    "|----------------------------|-------------|\n",
    "| `conversation.item.created`| LLM generates output segment or tool invocation item |\n",
    "| `response.audio.delta`     | Live synthesized TTS stream in base64-encoded Opus |\n",
    "| `session.updated`          | Acknowledgement after session config change |\n",
    "| `error`                     | Any kind of validation/runtime fault |\n",
    "| `close`                     | End conversation cleanly |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ˆ Key Transitions During Session Flow\n",
    "\n",
    "Hereâ€™s how various stages transition during a full cycle of a call:\n",
    "\n",
    "1. **Client Connect + Accept Handshake**\n",
    "2. **Session Setup (`session.update`)**\n",
    "   - Configure desired model parameters like voice, turn detection, instructions, etc.\n",
    "3. **Audio Input (`input_audio_buffer.append`)**\n",
    "   - Stream microphone audio continuously for transcription and processing\n",
    "4. **STT â†’ Text Transcription**\n",
    "   - Server sends recognized transcription segments as items (`item.created`)\n",
    "5. **Text Processed By Model**\n",
    "   - AI responds, generating text replies or actions\n",
    "6. **Synthesis Output Streaming**\n",
    "   - Asynchronously produces small playable audio fragments (`audio.delta`) until EOF\n",
    "7. **Close Stream Signal**\n",
    "   - When response finishes, the internal logic emits a `CloseStream`\n",
    "8. **Clean Disconnect / Reconnect Gracefully**\n",
    "\n",
    "All these steps maintain bi-directional communication via structured payloads conforming to schema defined under `openai_realtime_api_events`.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”§ Internals Behind Message Handling\n",
    "\n",
    "Letâ€™s break down each loop component more clearly.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“¥ Receive Loop (`receive_loop`)\n",
    "\n",
    "Responsible for handling inbound traffic.\n",
    "\n",
    "Logic includes:\n",
    "- Reading one message per frame (via `websocket.receive_text()`)\n",
    "- Parsing JSON to validate against known Event schemas (using Pydantic's `TypeAdapter`)\n",
    "- Dispatch based on message type:\n",
    "  - If it's **audio data**, decode Opus â†’ PCM, forward to handler\n",
    "  - Else update session or log unknown/metrics\n",
    "\n",
    "Additionally ensures clean cleanup upon disconnects and prevents stale messages (from prior connections) being applied post-reconnect.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“¤ Emit Loop (`emit_loop`)\n",
    "\n",
    "Handles sending outputs back toward the frontend client:\n",
    "\n",
    "Flow:\n",
    "1. Checks if any pending message exists already in the queue\n",
    "   - Used mostly for immediate user alerts like errors\n",
    "2. Otherwise pulls next output (either generated audio OR structured event) from `handler.emit()`\n",
    "3. Encodes raw PCM samples using `OpusStreamWriter` â†’ sends delta events containing base64 Opus-encoded audio\n",
    "4. Sends textual events directly as JSON payloads with `.model_dump_json()`\n",
    "5. Logs every outgoing server event to the recorder if applicable\n",
    "\n",
    "If network disconnection detected early, will raise `WebSocketClosedError` to signal graceful shutdown across handlers/tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ›  Lifecycle Management with Cleanup\n",
    "\n",
    "Everything wraps up when:\n",
    "- One side closes the socket voluntarily or unexpectedly\n",
    "- Error conditions are hit (`RuntimeError`, timeouts, validation failures)\n",
    "- Hard faults result in error reporting followed by disconnect code\n",
    "\n",
    "All resources including audio encoders, task groups, timers, service handles are cleaned up gracefully in `finally` clauses.\n",
    "\n",
    "```python\n",
    "finally:\n",
    "    await handler.cleanup()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Backend Dependencies Overview\n",
    "\n",
    "### External Services Connected:\n",
    "| Name                      | Role |\n",
    "|--------------------------|------|\n",
    "| TTS_SERVER               | Speech synthesis (Moshi, possibly Parler-TTS, etc.) |\n",
    "| STT_SERVER               | Speech recognition (e.g., Whisperstream variant) |\n",
    "| LLM_SERVER               | Generates conversational text (Kyutai, or OpenAI proxy) |\n",
    "| VOICE_CLONING_SERVER      | Clones user voice for personalization |\n",
    "\n",
    "Service health is periodically assessed at `/v1/health`\n",
    "\n",
    "### Metrics Tracking:\n",
    "Uses Prometheus instrumentation (via `prometheus_fastapi_instrumentator`)\n",
    "\n",
    "Exposed metric examples:\n",
    "- Sessions count\n",
    "- Active sessions gauge\n",
    "- Duration histogram\n",
    "- Errors encountered\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ—‚ Middleware Layers Present\n",
    "\n",
    "Several important pieces of middleware enhance behavior:\n",
    "\n",
    "| Middleware                         | Use Case |\n",
    "|----------------------------------|----------|\n",
    "| `CORSMiddleware`                 | Allow cross-site JS access during dev |\n",
    "| `LimitUploadSizeForPath`         | Cap file upload sizes for endpoints like `/v1/voices` |\n",
    "| Custom Exception Handlers        | Ensure proper CORS headers even if exceptions happen |\n",
    "| Optional profiling (`PROFILER`)  | Debug performance bottlenecks |\n",
    "\n",
    "---\n",
    "\n",
    "## â± Performance Considerations\n",
    "\n",
    "Key design choices optimize scalability without burning CPU:\n",
    "- Uses threading carefully with GIL-aware `asyncio.to_thread()` only where native libraries require synchronous execution (like decoding Opus).\n",
    "- Caps total simultaneous connections via semaphore (`MAX_CLIENTS = 4`).\n",
    "- Leverages FastAPI's concurrency model efficiently with task grouping for parallelism.\n",
    "- Logs and metrics help monitor live activity and debug issues quickly.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ¨ Summary Diagram\n",
    "\n",
    "```\n",
    "Client â†” Websocket (/v1/realtime)\n",
    "          â†“\n",
    "WebSocket.accept(subprotocol=\"realtime\")\n",
    "          â†“\n",
    "Create UnmuteHandler() with STT/TTS/LLM clients\n",
    "          â†“\n",
    "Spawn two looping tasks:\n",
    "    â† ReceiveLoop: Read client input_audio_buffer.append & session.update\n",
    "    â†’ EmitLoop: Write server speech deltas (audio/text) and responses\n",
    "          â†“\n",
    "Internal engine orchestrates voice input/output chain with model inference\n",
    "          â†“\n",
    "Metrics collected, recordings stored optionally, safe disconnection handled\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# unmute_handler.py\n",
    "\n",
    "Looking at `unmute_handler.py`, this is the core logic handler for managing real-time audio conversations. It orchestrates the flow between speech-to-text (STT), large language model (LLM), and text-to-speech (TTS) components. Let me break down how it works:\n",
    "\n",
    "## **Core Architecture**\n",
    "\n",
    "The `UnmuteHandler` class extends `AsyncStreamHandler` and serves as the central coordinator for a single conversation session. It manages:\n",
    "\n",
    "1. **Audio processing pipeline** (STT â†’ LLM â†’ TTS)\n",
    "2. **Conversation state management**\n",
    "3. **Service lifecycle coordination**\n",
    "4. **Real-time audio streaming**\n",
    "\n",
    "## **Key Components & Workflow**\n",
    "\n",
    "### **1. Initialization & State Management**\n",
    "```python\n",
    "def __init__(self) -> None:\n",
    "    super().__init__(input_sample_rate=SAMPLE_RATE, output_frame_size=480, output_sample_rate=SAMPLE_RATE)\n",
    "    self.n_samples_received = 0  # Audio time tracking\n",
    "    self.output_queue: asyncio.Queue[HandlerOutput] = asyncio.Queue()  # Output buffer\n",
    "    self.chatbot = Chatbot()  # Manages conversation history\n",
    "    self.quest_manager = QuestManager()  # Service lifecycle management\n",
    "```\n",
    "\n",
    "### **2. Service Management via QuestManager**\n",
    "The handler uses a `QuestManager` to manage service lifecycles:\n",
    "- **STT Quest**: Handles speech recognition\n",
    "- **TTS Quest**: Handles text-to-speech synthesis  \n",
    "- **LLM Quest**: Handles language model responses\n",
    "\n",
    "### **3. Main Audio Processing Flow**\n",
    "\n",
    "#### **Audio Reception (`receive` method)**\n",
    "```python\n",
    "async def receive(self, frame: tuple[int, np.ndarray]) -> None:\n",
    "```\n",
    "1. **Audio Input**: Receives PCM audio frames from the WebSocket\n",
    "2. **STT Processing**: Sends audio to speech-to-text service\n",
    "3. **Pause Detection**: Monitors for speech pauses/endings\n",
    "4. **Interruption Handling**: Detects when user interrupts bot\n",
    "\n",
    "#### **Key Detection Logic**:\n",
    "- **Voice Activity Detection (VAD)**: Uses STT pause predictions\n",
    "- **Silence Timeout**: Triggers after `USER_SILENCE_TIMEOUT` seconds\n",
    "- **Interruption**: User can interrupt bot speech\n",
    "\n",
    "### **4. Conversation Flow States**\n",
    "\n",
    "#### **State Transitions**:\n",
    "1. **Waiting for User** â†’ **User Speaking** (audio detected)\n",
    "2. **User Speaking** â†’ **Processing** (pause detected)\n",
    "3. **Processing** â†’ **Bot Speaking** (LLM response + TTS)\n",
    "4. **Bot Speaking** â†’ **Waiting for User** (response complete)\n",
    "\n",
    "#### **State Management**:\n",
    "```python\n",
    "# Conversation states managed in Chatbot:\n",
    "# - \"waiting_for_user\": Bot waiting for user input\n",
    "# - \"user_speaking\": User is talking\n",
    "# - \"bot_speaking\": Bot is generating/responding\n",
    "```\n",
    "\n",
    "### **5. Response Generation Pipeline**\n",
    "\n",
    "#### **Step 1: STT Processing** (`_stt_loop`)\n",
    "```python\n",
    "async def _stt_loop(self, stt: SpeechToText):\n",
    "    async for data in stt:\n",
    "        # Transcribe audio chunks\n",
    "        await self.output_queue.put(ora.ConversationItemInputAudioTranscriptionDelta(...))\n",
    "        # Update chat history with transcribed text\n",
    "```\n",
    "\n",
    "#### **Step 2: LLM Generation** (`_generate_response_task`)\n",
    "```python\n",
    "async def _generate_response_task(self):\n",
    "    # Send conversation history to LLM\n",
    "    async for delta in rechunk_to_words(llm.chat_completion(messages)):\n",
    "        # Stream response words to TTS\n",
    "        await tts.send(delta)\n",
    "```\n",
    "\n",
    "#### **Step 3: TTS Synthesis** (`_tts_loop`)\n",
    "```python\n",
    "async def _tts_loop(self, tts: TextToSpeech, generating_message_i: int):\n",
    "    async for message in tts:\n",
    "        if isinstance(message, TTSAudioMessage):\n",
    "            # Send audio frames to client\n",
    "            await output_queue.put((SAMPLE_RATE, audio))\n",
    "        elif isinstance(message, TTSTextMessage):\n",
    "            # Send text deltas to client\n",
    "            await output_queue.put(ora.ResponseTextDelta(delta=message.text))\n",
    "```\n",
    "\n",
    "## **Key Features**\n",
    "\n",
    "### **Real-time Interruption**\n",
    "```python\n",
    "async def interrupt_bot(self):\n",
    "    # Clear audio queues\n",
    "    # Stop current TTS/LLM tasks\n",
    "    # Prepare for new user input\n",
    "```\n",
    "\n",
    "### **Debugging & Monitoring**\n",
    "- Tracks timing metrics (TTFT, RTF)\n",
    "- Maintains debug plots and dictionaries\n",
    "- Records session data if enabled\n",
    "\n",
    "### **Error Handling & Recovery**\n",
    "- Graceful service shutdown\n",
    "- Queue management during interruptions\n",
    "- Connection error recovery\n",
    "\n",
    "## **Communication Flow**\n",
    "\n",
    "```\n",
    "User Audio â†’ WebSocket â†’ receive() â†’ STT Service â†’ \n",
    "Transcription â†’ Chatbot History â†’ LLM Request â†’ \n",
    "LLM Response â†’ TTS Service â†’ Audio Response â†’ \n",
    "WebSocket â†’ Client\n",
    "```\n",
    "\n",
    "## **Service Lifecycle**\n",
    "\n",
    "### **Startup Sequence**:\n",
    "1. Initialize STT service (`start_up_stt`)\n",
    "2. Wait for STT readiness\n",
    "3. Begin audio processing\n",
    "\n",
    "### **Dynamic Service Management**:\n",
    "- Services created/deleted as needed\n",
    "- Error recovery with retry logic\n",
    "- Clean shutdown procedures\n",
    "\n",
    "The handler essentially acts as a **real-time conversation orchestrator** that manages the complex timing and state transitions required for natural voice interactions, while handling interruptions, timeouts, and service coordination.\n",
    "\n",
    "# quest_manager.py\n",
    "\n",
    "Looking at `quest_manager.py`, this is a resource management system that provides **RAII (Resource Acquisition Is Initialization)** patterns in Python for handling asynchronous services with proper cleanup. Let me break down how it works:\n",
    "\n",
    "## **Core Concept**\n",
    "\n",
    "The `QuestManager` implements a **structured concurrency pattern** for managing the lifecycle of asynchronous services (STT, TTS, LLM connections) with guaranteed cleanup, similar to try/finally blocks but more sophisticated.\n",
    "\n",
    "## **Key Components**\n",
    "\n",
    "### **Quest[T] Class**\n",
    "A `Quest` represents a unit of async work with three phases:\n",
    "1. **Init**: Initialize the resource (e.g., connect to STT service)\n",
    "2. **Run**: Use the resource (e.g., process audio stream)\n",
    "3. **Close**: Clean up the resource (e.g., disconnect)\n",
    "\n",
    "```python\n",
    "class Quest[T]:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        init: Callable[[], Awaitable[T]],  # Resource initialization\n",
    "        run: Callable[[T], Awaitable[None]],  # Resource usage\n",
    "        close: Callable[[T], Awaitable[None]] | None = None,  # Cleanup\n",
    "    ):\n",
    "```\n",
    "\n",
    "### **QuestManager Class**\n",
    "Manages multiple quests with automatic conflict resolution and cleanup:\n",
    "\n",
    "```python\n",
    "class QuestManager:\n",
    "    def __init__(self):\n",
    "        self.quests: dict[str, Quest] = {}  # Named quests\n",
    "        self._future: asyncio.Future | None = None  # Aggregates exceptions\n",
    "```\n",
    "\n",
    "## **How It Works**\n",
    "\n",
    "### **1. Quest Lifecycle**\n",
    "\n",
    "#### **Creation & Initialization**\n",
    "```python\n",
    "# Example: Creating an STT quest\n",
    "async def _init() -> SpeechToText:\n",
    "    return await find_instance(\"stt\", SpeechToText)  # Connect to service\n",
    "\n",
    "async def _run(stt: SpeechToText):\n",
    "    await self._stt_loop(stt)  # Process audio stream\n",
    "\n",
    "async def _close(stt: SpeechToText):\n",
    "    await stt.shutdown()  # Clean disconnect\n",
    "\n",
    "quest = Quest(\"stt\", _init, _run, _close)\n",
    "```\n",
    "\n",
    "#### **Starting a Quest**\n",
    "```python\n",
    "async def __aenter__(self) -> asyncio.Future[None]:\n",
    "    self.task = asyncio.create_task(self._run())  # Start the quest\n",
    "    return asyncio.ensure_future(self.task)\n",
    "```\n",
    "\n",
    "### **2. Quest Management**\n",
    "\n",
    "#### **Adding Quests**\n",
    "```python\n",
    "async def add(self, quest: Quest[T]) -> Quest[T]:\n",
    "    name = quest.name\n",
    "    # Cancel existing quest with same name\n",
    "    if name in self.quests:\n",
    "        await self.quests[name].remove()\n",
    "    \n",
    "    self.quests[name] = quest\n",
    "    # Start the quest and monitor for exceptions\n",
    "    future = await quest.__aenter__()\n",
    "    future.add_done_callback(partial(self._one_is_done, name, self._future))\n",
    "    return quest\n",
    "```\n",
    "\n",
    "#### **Automatic Replacement**\n",
    "If you add a quest with a name that already exists, the old one is automatically cancelled and cleaned up.\n",
    "\n",
    "### **3. Exception Handling**\n",
    "\n",
    "#### **Aggregated Error Monitoring**\n",
    "```python\n",
    "@staticmethod\n",
    "def _one_is_done(name: str, agg_future: asyncio.Future, future: asyncio.Future):\n",
    "    try:\n",
    "        future.result()  # Check for exceptions\n",
    "    except asyncio.CancelledError:\n",
    "        logger.debug(\"Quest %s was cancelled.\", name)\n",
    "    except Exception as exc:\n",
    "        logger.debug(\"Quest %s failed with %r.\", name, exc)\n",
    "        if not agg_future.done():\n",
    "            agg_future.set_exception(exc)  # Bubble up to main task\n",
    "```\n",
    "\n",
    "#### **Usage Pattern**\n",
    "```python\n",
    "async with asyncio.TaskGroup() as tg:\n",
    "    tg.create_task(handler.quest_manager.wait())  # Catches quest exceptions\n",
    "```\n",
    "\n",
    "### **4. Cleanup Guarantees**\n",
    "\n",
    "#### **Context Manager Pattern**\n",
    "```python\n",
    "async def __aexit__(self, exc_type, exc_val, exc_tb):\n",
    "    # Clean shutdown of all quests\n",
    "    for name, quest in self.quests.items():\n",
    "        try:\n",
    "            await quest.remove()  # Calls close() and cancels task\n",
    "        except Exception:\n",
    "            logger.exception(f\"Error shutting down quest {name}\")\n",
    "    \n",
    "    self.quests.clear()\n",
    "    if not self._future.done():\n",
    "        self._future.set_result(None)  # Signal completion\n",
    "```\n",
    "\n",
    "## **Real-World Usage Example**\n",
    "\n",
    "In `unmute_handler.py`, quests manage services:\n",
    "\n",
    "```python\n",
    "# STT Quest\n",
    "async def start_up_stt(self):\n",
    "    async def _init() -> SpeechToText:\n",
    "        return await find_instance(\"stt\", SpeechToText)\n",
    "    \n",
    "    async def _run(stt: SpeechToText):\n",
    "        await self._stt_loop(stt)  # Process audio\n",
    "    \n",
    "    async def _close(stt: SpeechToText):\n",
    "        await stt.shutdown()\n",
    "    \n",
    "    # Add quest - automatically handles replacement/cleanup\n",
    "    quest = await self.quest_manager.add(Quest(\"stt\", _init, _run, _close))\n",
    "```\n",
    "\n",
    "## **Key Benefits**\n",
    "\n",
    "1. **Automatic Cleanup**: Resources are always cleaned up, even on errors\n",
    "2. **Conflict Resolution**: Adding a quest with existing name cancels the old one\n",
    "3. **Exception Propagation**: Errors in any quest bubble up to main task\n",
    "4. **Structured Lifecycle**: Clear init/run/close phases prevent resource leaks\n",
    "5. **Cancellation Safety**: Proper task cancellation without leaving resources hanging\n",
    "\n",
    "This system provides robust service lifecycle management essential for real-time audio processing where services need to be dynamically created, replaced, and cleaned up without leaks or zombie processes.\n",
    "\n",
    "# STT protocol\n",
    "\n",
    "Based on the code structure and patterns in your files, the STT (Speech-to-Text) service communication follows a WebSocket-based protocol with custom message types. Let me trace through the detailed protocol:\n",
    "\n",
    "## **STT Service Communication Protocol**\n",
    "\n",
    "### **1. Connection Establishment**\n",
    "The STT service is discovered and connected via:\n",
    "```python\n",
    "# In `quest_manager.py` and `unmute_handler.py`\n",
    "async def _init() -> SpeechToText:\n",
    "    return await find_instance(\"stt\", SpeechToText)\n",
    "```\n",
    "\n",
    "This uses service discovery to find and connect to the STT WebSocket server.\n",
    "\n",
    "### **2. Message Flow**\n",
    "\n",
    "#### **Client â†’ STT Service Messages**\n",
    "The main client-to-STT communication happens through audio streaming:\n",
    "```python\n",
    "# In `unmute_handler.py` receive() method\n",
    "await stt.send_audio(array)\n",
    "```\n",
    "\n",
    "This sends PCM audio chunks to the STT service. The audio is likely wrapped in a protocol like:\n",
    "- Raw PCM frames\n",
    "- Timestamped audio packets\n",
    "- Session/context identifiers\n",
    "\n",
    "#### **STT Service â†’ Client Messages**\n",
    "The STT service sends responses through async iteration:\n",
    "```python\n",
    "# In `unmute_handler.py` _stt_loop method\n",
    "async for data in stt:\n",
    "    if isinstance(data, STTMarkerMessage):\n",
    "        # Ignore marker messages\n",
    "        continue\n",
    "    \n",
    "    # Send transcription to client\n",
    "    await self.output_queue.put(\n",
    "        ora.ConversationItemInputAudioTranscriptionDelta(\n",
    "            delta=data.text,\n",
    "            start_time=data.start_time,\n",
    "        )\n",
    "    )\n",
    "```\n",
    "\n",
    "### **3. Message Types**\n",
    "\n",
    "Based on the code, the STT service likely uses these message types:\n",
    "\n",
    "#### **Input Messages (Client â†’ STT)**:\n",
    "- **Audio Data**: PCM audio frames\n",
    "- **Session Control**: Start/stop transcription signals\n",
    "- **Configuration**: Language, model selection, etc.\n",
    "\n",
    "#### **Output Messages (STT â†’ Client)**:\n",
    "```python\n",
    "# From `stt/speech_to_text.py` (inferred structure):\n",
    "class STTMarkerMessage:\n",
    "    type: str = \"marker\"\n",
    "    # Internal synchronization markers\n",
    "\n",
    "class STTTranscriptionMessage:\n",
    "    type: str = \"transcription\"\n",
    "    text: str\n",
    "    start_time: float\n",
    "    end_time: float\n",
    "    confidence: float\n",
    "```\n",
    "\n",
    "### **4. Protocol Flow**\n",
    "\n",
    "#### **Connection Phase**:\n",
    "1. **Service Discovery**: `find_instance(\"stt\", SpeechToText)`\n",
    "2. **WebSocket Connection**: Establish connection to STT server\n",
    "3. **Session Initialization**: Send configuration (sample rate, language, etc.)\n",
    "4. **Ready Signal**: STT service acknowledges readiness\n",
    "\n",
    "#### **Streaming Phase**:\n",
    "```python\n",
    "# Continuous loop in _stt_loop\n",
    "async for data in stt:\n",
    "    # Real-time transcription results\n",
    "    # Partial results streamed as they become available\n",
    "    # Final results marked with confidence scores\n",
    "```\n",
    "\n",
    "#### **Audio Streaming**:\n",
    "```python\n",
    "# From unmute_handler.py\n",
    "await stt.send_audio(array)  # Send PCM audio chunks\n",
    "```\n",
    "\n",
    "The protocol likely batches audio into frames and sends them continuously.\n",
    "\n",
    "### **5. Control Mechanisms**\n",
    "\n",
    "#### **Pause Detection**:\n",
    "```python\n",
    "# In unmute_handler.py\n",
    "if stt.pause_prediction.value > 0.6:\n",
    "    # Voice Activity Detection triggers pause detection\n",
    "    return True\n",
    "```\n",
    "\n",
    "The STT service likely sends VAD (Voice Activity Detection) scores along with transcriptions.\n",
    "\n",
    "#### **Flushing Mechanism**:\n",
    "```python\n",
    "# When pause detected, send silence to flush buffers\n",
    "num_frames = int(math.ceil(stt.delay_sec / FRAME_TIME_SEC)) + 1\n",
    "zero = np.zeros(SAMPLES_PER_FRAME, dtype=np.float32)\n",
    "for _ in range(num_frames):\n",
    "    await stt.send_audio(zero)\n",
    "```\n",
    "\n",
    "### **6. Error Handling Protocol**\n",
    "\n",
    "#### **Connection Errors**:\n",
    "```python\n",
    "# In _stt_loop\n",
    "except websockets.ConnectionClosed:\n",
    "    logger.info(\"STT connection closed while receiving messages.\")\n",
    "```\n",
    "\n",
    "#### **Service Unavailability**:\n",
    "```python\n",
    "# In main_websocket.py via QuestManager\n",
    "except MissingServiceAtCapacity:\n",
    "    # Handle service overload\n",
    "except MissingServiceTimeout:\n",
    "    # Handle connection timeouts\n",
    "```\n",
    "\n",
    "### **7. Session Management**\n",
    "\n",
    "#### **Start Session**:\n",
    "- Connect to WebSocket\n",
    "- Send initial configuration\n",
    "- Begin audio streaming\n",
    "\n",
    "#### **End Session**:\n",
    "```python\n",
    "# In Quest cleanup\n",
    "async def _close(stt: SpeechToText):\n",
    "    await stt.shutdown()  # Graceful shutdown\n",
    "```\n",
    "\n",
    "### **8. Timing and Synchronization**\n",
    "\n",
    "#### **Delay Compensation**:\n",
    "```python\n",
    "# Account for processing delays\n",
    "self.stt_end_of_flush_time = stt.current_time + stt.delay_sec\n",
    "```\n",
    "\n",
    "#### **Real-time Factor (RTF) Monitoring**:\n",
    "```python\n",
    "rtf = stt.delay_sec / elapsed  # Processing speed vs real-time\n",
    "```\n",
    "\n",
    "### **9. Inferred Protocol Message Structure**\n",
    "\n",
    "While the exact STT protocol isn't shown, based on the patterns, it likely uses JSON messages:\n",
    "\n",
    "```json\n",
    "// Audio packet\n",
    "{\n",
    "  \"type\": \"audio\",\n",
    "  \"data\": \"<base64_encoded_pcm>\",\n",
    "  \"timestamp\": 1234567890.123,\n",
    "  \"sample_rate\": 24000\n",
    "}\n",
    "\n",
    "// Transcription result (partial)\n",
    "{\n",
    "  \"type\": \"transcription\",\n",
    "  \"text\": \"hello world\",\n",
    "  \"start_time\": 1.234,\n",
    "  \"end_time\": 2.345,\n",
    "  \"confidence\": 0.95,\n",
    "  \"is_final\": false\n",
    "}\n",
    "\n",
    "// Transcription result (final)\n",
    "{\n",
    "  \"type\": \"transcription\",\n",
    "  \"text\": \"hello world\",\n",
    "  \"start_time\": 1.234,\n",
    "  \"end_time\": 2.345,\n",
    "  \"confidence\": 0.98,\n",
    "  \"is_final\": true\n",
    "}\n",
    "\n",
    "// VAD update\n",
    "{\n",
    "  \"type\": \"vad\",\n",
    "  \"probability\": 0.85,\n",
    "  \"speaking\": true\n",
    "}\n",
    "\n",
    "// Session control\n",
    "{\n",
    "  \"type\": \"control\",\n",
    "  \"action\": \"flush\",\n",
    "  \"duration\": 0.1\n",
    "}\n",
    "```\n",
    "\n",
    "### **10. Key Protocol Features**\n",
    "\n",
    "1. **Real-time Streaming**: Continuous audio input with immediate partial results\n",
    "2. **VAD Integration**: Voice activity detection for pause/speech boundaries\n",
    "3. **Confidence Scoring**: Quality indicators for transcription accuracy\n",
    "4. **Session Boundaries**: Clear start/end of transcription sessions\n",
    "5. **Error Recovery**: Automatic reconnection and error handling\n",
    "6. **Timing Synchronization**: Timestamps for accurate alignment with other services\n",
    "\n",
    "The protocol is designed for low-latency, real-time transcription with robust error handling and synchronization with the overall conversation flow managed by the UnmuteHandler.\n",
    "\n",
    "# TTS protocol\n",
    "\n",
    "Based on the code in `unmute_handler.py` and the patterns shown, the TTS (Text-to-Speech) service communication follows a more complex bidirectional protocol than STT. Let me trace through the detailed protocol:\n",
    "\n",
    "## **TTS Service Communication Protocol**\n",
    "\n",
    "### **1. Connection Establishment**\n",
    "```python\n",
    "# In `unmute_handler.py` start_up_tts method\n",
    "async def _init() -> TextToSpeech:\n",
    "    factory = partial(\n",
    "        TextToSpeech,\n",
    "        recorder=self.recorder,\n",
    "        get_time=self.audio_received_sec,\n",
    "        voice=self.tts_voice,\n",
    "    )\n",
    "    tts = await find_instance(\"tts\", factory)\n",
    "```\n",
    "\n",
    "This creates a `TextToSpeech` instance that handles the WebSocket communication.\n",
    "\n",
    "### **2. Message Flow**\n",
    "\n",
    "#### **Client â†’ TTS Service Messages**\n",
    "The main client-to-TTS communication sends text for synthesis:\n",
    "\n",
    "```python\n",
    "# In `_generate_response_task` method\n",
    "await tts.send(delta)  # Send text chunks to TTS\n",
    "\n",
    "# At end of LLM response\n",
    "await tts.send(TTSClientEosMessage())  # End of stream signal\n",
    "```\n",
    "\n",
    "#### **TTS Service â†’ Client Messages**\n",
    "The TTS service sends responses through async iteration:\n",
    "```python\n",
    "# In `_tts_loop` method\n",
    "async for message in tts:\n",
    "    if isinstance(message, TTSAudioMessage):\n",
    "        # Send audio frames to client\n",
    "        await output_queue.put((SAMPLE_RATE, audio))\n",
    "    elif isinstance(message, TTSTextMessage):\n",
    "        # Send text deltas (for display)\n",
    "        await output_queue.put(ora.ResponseTextDelta(delta=message.text))\n",
    "```\n",
    "\n",
    "### **3. Message Types**\n",
    "\n",
    "Based on the code, the TTS service uses specific message types:\n",
    "\n",
    "#### **Input Messages (Client â†’ TTS)**:\n",
    "```python\n",
    "# From `tts/text_to_speech.py` (inferred):\n",
    "class TTSClientTextMessage:\n",
    "    type: str = \"text\"\n",
    "    text: str  # Text to synthesize\n",
    "\n",
    "class TTSClientEosMessage:\n",
    "    type: str = \"eos\"  # End of stream\n",
    "```\n",
    "\n",
    "#### **Output Messages (TTS â†’ Client)**:\n",
    "```python\n",
    "class TTSAudioMessage:\n",
    "    type: str = \"audio\"\n",
    "    pcm: list[float]  # PCM audio samples\n",
    "\n",
    "class TTSTextMessage:\n",
    "    type: str = \"text\"\n",
    "    text: str  # Text being synthesized (for display)\n",
    "```\n",
    "\n",
    "### **4. Protocol Flow**\n",
    "\n",
    "#### **Connection Phase**:\n",
    "1. **Service Discovery**: `find_instance(\"tts\", factory)`\n",
    "2. **WebSocket Connection**: Establish connection to TTS server\n",
    "3. **Session Initialization**: Send voice configuration, sample rate, etc.\n",
    "4. **Ready Signal**: TTS service acknowledges readiness\n",
    "\n",
    "#### **Synthesis Phase**:\n",
    "```python\n",
    "# In `_tts_loop` method\n",
    "async for message in tts:\n",
    "    # Stream audio and text responses back to client\n",
    "    # Audio messages are PCM samples\n",
    "    # Text messages are for displaying current synthesis\n",
    "```\n",
    "\n",
    "#### **Text Streaming**:\n",
    "```python\n",
    "# From _generate_response_task\n",
    "async for delta in rechunk_to_words(llm.chat_completion(messages)):\n",
    "    # Stream response words to TTS as they arrive\n",
    "    await tts.send(delta)\n",
    "```\n",
    "\n",
    "### **5. Control Mechanisms**\n",
    "\n",
    "#### **End of Stream Signaling**:\n",
    "```python\n",
    "# Signal end of LLM response to TTS\n",
    "await tts.send(TTSClientEosMessage())\n",
    "```\n",
    "\n",
    "#### **Real-time Timing**:\n",
    "```python\n",
    "# Timing measurements for throughput monitoring\n",
    "time_since_start = self.audio_received_sec() - audio_started\n",
    "time_received = tts.received_samples / self.input_sample_rate\n",
    "self.debug_dict[\"tts_throughput\"] = {\n",
    "    \"time_received\": round(time_received, 2),\n",
    "    \"time_since_start\": round(time_since_start, 2),\n",
    "    \"ratio\": round(time_received_yielded / (time_since_start + 0.01), 2),\n",
    "}\n",
    "```\n",
    "\n",
    "### **6. Error Handling Protocol**\n",
    "\n",
    "#### **Connection Errors**:\n",
    "```python\n",
    "# In _tts_loop\n",
    "except websockets.ConnectionClosedError as e:\n",
    "    logger.error(f\"TTS connection closed with an error: {e}\")\n",
    "```\n",
    "\n",
    "#### **Retry Logic**:\n",
    "```python\n",
    "# In start_up_tts with retry mechanism\n",
    "for trial in range(trials):\n",
    "    try:\n",
    "        tts = await find_instance(\"tts\", factory)\n",
    "    except Exception:\n",
    "        if trial == trials - 1:\n",
    "            raise\n",
    "        await asyncio.sleep(sleep_time)\n",
    "        # Send warning to client about latency\n",
    "        error = make_ora_error(\n",
    "            type=\"warning\",\n",
    "            message=\"Looking for the resources, expect some latency.\",\n",
    "        )\n",
    "        await self.output_queue.put(error)\n",
    "```\n",
    "\n",
    "### **7. Session Management**\n",
    "\n",
    "#### **Start Session**:\n",
    "```python\n",
    "# In start_up_tts method\n",
    "quest = await self.quest_manager.add(Quest(\"tts\", _init, _run, _close))\n",
    "```\n",
    "\n",
    "#### **End Session**:\n",
    "```python\n",
    "# In Quest cleanup\n",
    "async def _close(tts: TextToSpeech):\n",
    "    await tts.shutdown()  # Graceful shutdown\n",
    "```\n",
    "\n",
    "### **8. Interruption Handling**\n",
    "\n",
    "#### **Bot Interruption**:\n",
    "```python\n",
    "# In interrupt_bot method\n",
    "async def interrupt_bot(self):\n",
    "    # Clear TTS queue and stop current synthesis\n",
    "    self.output_queue = asyncio.Queue()  # Clear our own queue\n",
    "    await self.quest_manager.remove(\"tts\")  # Stop and cleanup TTS quest\n",
    "    await self.quest_manager.remove(\"llm\")   # Stop and cleanup LLM quest\n",
    "```\n",
    "\n",
    "### **9. Timing and Synchronization**\n",
    "\n",
    "#### **Audio Timing**:\n",
    "```python\n",
    "# Track when audio synthesis starts\n",
    "if audio_started is None:\n",
    "    audio_started = self.audio_received_sec()\n",
    "\n",
    "# Calculate real-time factor for performance monitoring\n",
    "self.debug_dict[\"tts_throughput\"] = {\n",
    "    \"ratio\": round(time_received_yielded / (time_since_start + 0.01), 2),\n",
    "}\n",
    "```\n",
    "\n",
    "### **10. Protocol Message Structure**\n",
    "\n",
    "Based on the patterns in the code, the TTS protocol likely uses JSON messages:\n",
    "\n",
    "```json\n",
    "// Client sends text for synthesis\n",
    "{\n",
    "  \"type\": \"text\",\n",
    "  \"text\": \"Hello, how can I help you today?\",\n",
    "  \"timestamp\": 1234567890.123\n",
    "}\n",
    "\n",
    "// Client signals end of input\n",
    "{\n",
    "  \"type\": \"eos\",\n",
    "  \"timestamp\": 1234567895.456\n",
    "}\n",
    "\n",
    "// TTS service sends audio response\n",
    "{\n",
    "  \"type\": \"audio\",\n",
    "  \"pcm\": [0.1, -0.2, 0.3, ...],  // PCM samples\n",
    "  \"sample_rate\": 24000,\n",
    "  \"timestamp\": 1234567890.789\n",
    "}\n",
    "\n",
    "// TTS service sends text for display (as it's being synthesized)\n",
    "{\n",
    "  \"type\": \"text\",\n",
    "  \"text\": \"Hello, how can I help\",\n",
    "  \"timestamp\": 1234567890.789\n",
    "}\n",
    "\n",
    "// Configuration message (on connection)\n",
    "{\n",
    "  \"type\": \"config\",\n",
    "  \"voice\": \"alloy\",\n",
    "  \"sample_rate\": 24000,\n",
    "  \"language\": \"en\"\n",
    "}\n",
    "\n",
    "// Error message\n",
    "{\n",
    "  \"type\": \"error\",\n",
    "  \"code\": \"VOICE_NOT_AVAILABLE\",\n",
    "  \"message\": \"Requested voice is not available\"\n",
    "}\n",
    "```\n",
    "\n",
    "### **11. Key Protocol Features**\n",
    "\n",
    "1. **Bidirectional Streaming**: \n",
    "   - Client streams text chunks to TTS\n",
    "   - TTS streams audio and text responses back\n",
    "\n",
    "2. **Real-time Synthesis**: \n",
    "   - Audio generated as text arrives\n",
    "   - Low-latency streaming output\n",
    "\n",
    "3. **Voice Selection**: \n",
    "   - Voice configuration sent on connection\n",
    "   - Support for multiple voice options\n",
    "\n",
    "4. **Flow Control**: \n",
    "   - EOS signaling for clean session ending\n",
    "   - Interruption handling for dynamic conversations\n",
    "\n",
    "5. **Synchronization**: \n",
    "   - Timestamps for accurate timing\n",
    "   - Sample counting for throughput monitoring\n",
    "\n",
    "6. **Error Recovery**: \n",
    "   - Retry mechanisms for service discovery\n",
    "   - Graceful degradation with user warnings\n",
    "\n",
    "7. **Interruption Support**: \n",
    "   - Ability to cancel ongoing synthesis\n",
    "   - Queue clearing for immediate response changes\n",
    "\n",
    "The TTS protocol is designed for real-time, low-latency speech synthesis with support for dynamic interruptions and robust error handling, making it suitable for conversational AI applications.\n",
    "\n",
    "# LLM protocol\n",
    "\n",
    "Based on the code in `unmute_handler.py` and related files, the LLM (Large Language Model) communication follows the **OpenAI API protocol** with some custom extensions. Let me break down the detailed protocol:\n",
    "\n",
    "## **LLM Service Communication Protocol**\n",
    "\n",
    "### **1. Connection & Client Setup**\n",
    "```python\n",
    "# In `unmute_handler.py` __init__ method\n",
    "self.openai_client = get_openai_client()\n",
    "\n",
    "# In `llm_utils.py` (inferred)\n",
    "def get_openai_client():\n",
    "    return AsyncOpenAI(\n",
    "        base_url=LLM_SERVER,  # e.g., \"http://localhost:8000/v1\"\n",
    "        api_key=KYUTAI_LLM_API_KEY,\n",
    "    )\n",
    "```\n",
    "\n",
    "The system uses the standard OpenAI Python client connecting to a vLLM-compatible server.\n",
    "\n",
    "### **2. Message Flow**\n",
    "\n",
    "#### **Client â†’ LLM Service Messages**\n",
    "The communication uses the OpenAI Chat Completions API format:\n",
    "\n",
    "```python\n",
    "# In `_generate_response_task` method\n",
    "messages = self.chatbot.preprocessed_messages()\n",
    "llm = VLLMStream(self.openai_client, temperature=0.7)\n",
    "\n",
    "async for delta in rechunk_to_words(llm.chat_completion(messages)):\n",
    "    # Stream response tokens/words to TTS\n",
    "    await tts.send(delta)\n",
    "```\n",
    "\n",
    "#### **LLM Service â†’ Client Messages**\n",
    "Streaming responses using Server-Sent Events (SSE) format typical of OpenAI API:\n",
    "\n",
    "```python\n",
    "# From llm_utils.py (inferred structure)\n",
    "class VLLMStream:\n",
    "    async def chat_completion(self, messages):\n",
    "        stream = await self.client.chat.completions.create(\n",
    "            model=\"default\",  # or configured model\n",
    "            messages=messages,\n",
    "            stream=True,\n",
    "            temperature=self.temperature,\n",
    "            # other OpenAI parameters\n",
    "        )\n",
    "        \n",
    "        async for chunk in stream:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                yield chunk.choices[0].delta.content\n",
    "```\n",
    "\n",
    "### **3. Message Types & Structure**\n",
    "\n",
    "#### **Input Messages (Client â†’ LLM)**:\n",
    "Standard OpenAI Chat API format:\n",
    "```python\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant...\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Hello, how are you?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"I'm doing well, thank you for asking!\"\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "#### **Parameters**:\n",
    "```python\n",
    "# From _generate_response_task\n",
    "llm.chat_completion(\n",
    "    messages=messages,\n",
    "    model=\"default\",  # Configurable\n",
    "    temperature=FIRST_MESSAGE_TEMPERATURE,  # 0.7 for first, 0.3 for follow-ups\n",
    "    stream=True,  # Always streaming\n",
    "    # Potentially other OpenAI params like max_tokens, top_p, etc.\n",
    ")\n",
    "```\n",
    "\n",
    "### **4. Protocol Flow**\n",
    "\n",
    "#### **Connection Phase**:\n",
    "1. **HTTP Connection**: REST API call to `/v1/chat/completions`\n",
    "2. **Authentication**: Bearer token in Authorization header\n",
    "3. **Session Setup**: Send conversation history and parameters\n",
    "\n",
    "#### **Streaming Phase**:\n",
    "```python\n",
    "# In _generate_response_task\n",
    "async for delta in rechunk_to_words(llm.chat_completion(messages)):\n",
    "    # Process each token/word as it arrives\n",
    "    await self.output_queue.put(\n",
    "        ora.UnmuteResponseTextDeltaReady(delta=delta)\n",
    "    )\n",
    "    \n",
    "    # Send to TTS for real-time synthesis\n",
    "    await tts.send(delta)\n",
    "```\n",
    "\n",
    "#### **Word Chunking**:\n",
    "```python\n",
    "# From llm_utils.py (inferred)\n",
    "async def rechunk_to_words(stream):\n",
    "    buffer = \"\"\n",
    "    async for token in stream:\n",
    "        buffer += token\n",
    "        # Split on word boundaries for better TTS streaming\n",
    "        words = buffer.split()\n",
    "        if len(words) > 1:  # At least one complete word\n",
    "            for word in words[:-1]:  # Send all but last (possibly incomplete)\n",
    "                yield word + \" \"\n",
    "            buffer = words[-1]  # Keep last word for next iteration\n",
    "    \n",
    "    # Send remaining buffer\n",
    "    if buffer:\n",
    "        yield buffer\n",
    "```\n",
    "\n",
    "### **5. Control Mechanisms**\n",
    "\n",
    "#### **Temperature Control**:\n",
    "```python\n",
    "# Different temperatures for conversation flow\n",
    "temperature=FIRST_MESSAGE_TEMPERATURE if generating_message_i == 2 else FURTHER_MESSAGES_TEMPERATURE\n",
    "# FIRST_MESSAGE_TEMPERATURE = 0.7 (more creative)\n",
    "# FURTHER_MESSAGES_TEMPERATURE = 0.3 (more focused)\n",
    "```\n",
    "\n",
    "#### **Interruption Handling**:\n",
    "```python\n",
    "# In _generate_response_task\n",
    "if len(self.chatbot.chat_history) > generating_message_i:\n",
    "    break  # We've been interrupted\n",
    "```\n",
    "\n",
    "#### **Cancellation Support**:\n",
    "```python\n",
    "# Task cancellation propagates to LLM stream\n",
    "except asyncio.CancelledError:\n",
    "    mt.VLLM_INTERRUPTS.inc()\n",
    "    raise\n",
    "```\n",
    "\n",
    "### **6. Error Handling Protocol**\n",
    "\n",
    "#### **Connection Errors**:\n",
    "The OpenAI client handles standard HTTP errors, timeouts, etc.\n",
    "\n",
    "#### **Service Unavailability**:\n",
    "```python\n",
    "# In main_websocket.py health check\n",
    "llm_up = tg.create_task(\n",
    "    asyncio.to_thread(\n",
    "        _check_server_status,\n",
    "        _ws_to_http(LLM_SERVER) + \"/v1/models\",\n",
    "        headers={\"Authorization\": f\"Bearer {KYUTAI_LLM_API_KEY}\"},\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "### **7. Session Management**\n",
    "\n",
    "#### **Conversation Context**:\n",
    "```python\n",
    "# Full conversation history sent with each request\n",
    "messages = self.chatbot.preprocessed_messages()\n",
    "\n",
    "# Example structure:\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"System instructions...\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n",
    "    {\"role\": \"user\", \"content\": \"How are you?\"},\n",
    "    # Current assistant message being generated\n",
    "    {\"role\": \"assistant\", \"content\": \"\"}\n",
    "]\n",
    "```\n",
    "\n",
    "#### **Message Preprocessing**:\n",
    "```python\n",
    "# In Chatbot class (inferred)\n",
    "def preprocessed_messages(self):\n",
    "    # Apply any preprocessing rules\n",
    "    # Handle special markers like USER_SILENCE_MARKER\n",
    "    # Apply conversation context rules\n",
    "    return processed_messages\n",
    "```\n",
    "\n",
    "### **8. Timing and Metrics**\n",
    "\n",
    "#### **Performance Monitoring**:\n",
    "```python\n",
    "# Time to First Token tracking\n",
    "llm_stopwatch = Stopwatch()\n",
    "# ... start streaming\n",
    "if time_to_first_token is None:\n",
    "    time_to_first_token = llm_stopwatch.time()\n",
    "    self.debug_dict[\"timing\"][\"to_first_token\"] = time_to_first_token\n",
    "    mt.VLLM_TTFT.observe(time_to_first_token)  # Prometheus metrics\n",
    "```\n",
    "\n",
    "#### **Response Analysis**:\n",
    "```python\n",
    "# Track response length and generation time\n",
    "mt.VLLM_REPLY_LENGTH.observe(len(response_words))\n",
    "mt.VLLM_GEN_DURATION.observe(llm_stopwatch.time())\n",
    "mt.VLLM_REQUEST_LENGTH.observe(num_words_sent)  # Input length\n",
    "```\n",
    "\n",
    "### **9. Protocol Message Examples**\n",
    "\n",
    "#### **HTTP Request**:\n",
    "```http\n",
    "POST /v1/chat/completions\n",
    "Authorization: Bearer sk-...\n",
    "Content-Type: application/json\n",
    "\n",
    "{\n",
    "  \"model\": \"default\",\n",
    "  \"messages\": [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather like?\"}\n",
    "  ],\n",
    "  \"temperature\": 0.7,\n",
    "  \"stream\": true\n",
    "}\n",
    "```\n",
    "\n",
    "#### **Streaming Response (SSE)**:\n",
    "```http\n",
    "HTTP/1.1 200 OK\n",
    "Content-Type: text/event-stream\n",
    "\n",
    "data: {\"id\":\"cmpl-1\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\"},\"index\":0}]}\n",
    "\n",
    "data: {\"id\":\"cmpl-1\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"content\":\"The\"},\"index\":0}]}\n",
    "\n",
    "data: {\"id\":\"cmpl-1\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"content\":\" weather\"},\"index\":0}]}\n",
    "\n",
    "data: {\"id\":\"cmpl-1\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"content\":\" today\"},\"index\":0}]}\n",
    "\n",
    "data: {\"id\":\"cmpl-1\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{},\"finish_reason\":\"stop\",\"index\":0}]}\n",
    "\n",
    "data: [DONE]\n",
    "```\n",
    "\n",
    "### **10. Custom Extensions**\n",
    "\n",
    "#### **Special Markers Handling**:\n",
    "```python\n",
    "# In Chatbot preprocessing\n",
    "if message.get(\"content\") == USER_SILENCE_MARKER:\n",
    "    # Handle long silence scenario with special prompt instructions\n",
    "```\n",
    "\n",
    "#### **Response Wrapping**:\n",
    "```python\n",
    "# In _generate_response_task\n",
    "await self.output_queue.put(\n",
    "    ora.ResponseCreated(\n",
    "        response=ora.Response(\n",
    "            status=\"in_progress\",\n",
    "            voice=self.tts_voice or \"missing\",\n",
    "            chat_history=self.chatbot.chat_history,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "### **11. Key Protocol Features**\n",
    "\n",
    "1. **OpenAI-Compatible**: Uses standard Chat Completions API\n",
    "2. **Streaming**: Real-time token streaming with SSE\n",
    "3. **Contextual**: Full conversation history maintained\n",
    "4. **Configurable**: Temperature and other parameters adjusted per message\n",
    "5. **Interruption-Aware**: Stops generation when interrupted\n",
    "6. **Metrics-Rich**: Detailed timing and performance tracking\n",
    "7. **Error-Resilient**: Standard HTTP error handling\n",
    "8. **Word-Boundary Optimized**: `rechunk_to_words` for better TTS streaming\n",
    "\n",
    "The protocol leverages the widely-adopted OpenAI API standard while adding custom streaming optimizations and conversation management features specific to real-time voice interactions.\n",
    "\n",
    "# Mutiple users and sessions\n",
    "\n",
    "Looking at the code, the system uses several mechanisms to manage distinct sessions for multiple users while sharing some resources appropriately. Let me break down how session isolation and resource management work:\n",
    "\n",
    "## **Session Isolation Mechanisms**\n",
    "\n",
    "### **1. Per-Connection Handler Instances**\n",
    "\n",
    "Each WebSocket connection gets its own **dedicated `UnmuteHandler` instance**:\n",
    "\n",
    "```python\n",
    "# In main_websocket.py\n",
    "async def websocket_route(websocket: WebSocket):\n",
    "    async with SEMAPHORE:\n",
    "        try:\n",
    "            await websocket.accept(subprotocol=\"realtime\")\n",
    "            \n",
    "            # NEW INSTANCE PER CONNECTION\n",
    "            handler = UnmuteHandler()  # â† Fresh instance for each user\n",
    "            async with handler:\n",
    "                await handler.start_up()\n",
    "                await _run_route(websocket, handler)\n",
    "```\n",
    "\n",
    "This ensures complete isolation - each user has their own:\n",
    "- Conversation state (`chatbot.chat_history`)\n",
    "- Audio processing queues\n",
    "- Service connections (STT, TTS, LLM)\n",
    "- Timing trackers and debug data\n",
    "\n",
    "### **2. Semaphore-Based Connection Limiting**\n",
    "\n",
    "```python\n",
    "# Global semaphore limits concurrent sessions\n",
    "MAX_CLIENTS = 4\n",
    "SEMAPHORE = asyncio.Semaphore(MAX_CLIENTS)\n",
    "\n",
    "# In websocket_route\n",
    "async with SEMAPHORE:  # Limits total concurrent users\n",
    "    # ... create handler and process session\n",
    "```\n",
    "\n",
    "This prevents resource exhaustion while ensuring each session gets dedicated resources.\n",
    "\n",
    "### **3. Independent Service Connections**\n",
    "\n",
    "Each handler creates **independent connections** to backend services:\n",
    "\n",
    "```python\n",
    "# In UnmuteHandler.start_up_stt()\n",
    "async def _init() -> SpeechToText:\n",
    "    return await find_instance(\"stt\", SpeechToText)  # â† Unique STT connection\n",
    "\n",
    "# In UnmuteHandler.start_up_tts()  \n",
    "async def _init() -> TextToSpeech:\n",
    "    factory = partial(TextToSpeech, ...)  # â† Unique TTS connection\n",
    "    return await find_instance(\"tts\", factory)\n",
    "```\n",
    "\n",
    "Services like STT/TTS are allocated independently per session via service discovery.\n",
    "\n",
    "## **What is SHARED Between Users**\n",
    "\n",
    "### **1. Configuration Constants**\n",
    "```python\n",
    "# Shared across all sessions\n",
    "SAMPLE_RATE = 24000\n",
    "FRAME_TIME_SEC = 0.02\n",
    "MAX_CLIENTS = 4\n",
    "CORS_ALLOW_ORIGINS = [\"http://localhost\", \"http://localhost:3000\"]\n",
    "```\n",
    "\n",
    "### **2. Service Discovery Mechanism**\n",
    "```python\n",
    "# Shared service discovery pool\n",
    "@async_ttl_cached(ttl_sec=0.5)  \n",
    "async def _get_health(_none: None):\n",
    "    # Shared health checking but separate actual connections per session\n",
    "```\n",
    "\n",
    "### **3. Global Metrics Collection**\n",
    "```python\n",
    "# Shared Prometheus metrics (with proper atomic operations)\n",
    "mt.SESSIONS.inc()        # Global counter\n",
    "mt.ACTIVE_SESSIONS.inc() # Global gauge\n",
    "mt.HEALTH_OK.observe()   # Shared histogram\n",
    "```\n",
    "\n",
    "### **4. Cached Static Data**\n",
    "```python\n",
    "# Shared across sessions (immutable)\n",
    "@app.get(\"/v1/voices\")\n",
    "@cache\n",
    "def voices():  # Voice list cached globally\n",
    "    return good_voices\n",
    "\n",
    "# Health checks also shared with caching\n",
    "@partial(async_ttl_cached, ttl_sec=0.5)\n",
    "async def _get_health(_none: None):\n",
    "```\n",
    "\n",
    "## **What is SPECIFIC Per User Session**\n",
    "\n",
    "### **1. Complete Handler State**\n",
    "Each `UnmuteHandler` instance maintains independent:\n",
    "```python\n",
    "class UnmuteHandler:\n",
    "    def __init__(self) -> None:\n",
    "        self.n_samples_received = 0           # Per-session timing\n",
    "        self.output_queue = asyncio.Queue()   # Per-session output queue\n",
    "        self.chatbot = Chatbot()              # Per-session conversation\n",
    "        self.quest_manager = QuestManager()   # Per-session service manager\n",
    "        self.recorder = Recorder(...)         # Per-session recording (if enabled)\n",
    "        self.debug_dict = {}                  # Per-session debug data\n",
    "```\n",
    "\n",
    "### **2. Independent Service Quests**\n",
    "```python\n",
    "# Each session has its own QuestManager with isolated services\n",
    "self.quest_manager = QuestManager()\n",
    "# Creates independent STT, TTS, LLM connections\n",
    "```\n",
    "\n",
    "### **3. Separate Audio Processing**\n",
    "```python\n",
    "# Per-session audio state tracking\n",
    "self.n_samples_received = 0\n",
    "self.stt_last_message_time = 0\n",
    "self.waiting_for_user_start_time = self.audio_received_sec()\n",
    "```\n",
    "\n",
    "### **4. Individual WebSocket Connection**\n",
    "```python\n",
    "# Each session uses its own WebSocket\n",
    "async def _run_route(websocket: WebSocket, handler: UnmuteHandler):\n",
    "    # websocket is unique per connection\n",
    "    # handler is unique per session\n",
    "```\n",
    "\n",
    "## **Session Lifecycle Isolation**\n",
    "\n",
    "### **Creation**\n",
    "```python\n",
    "# New handler instance per connection\n",
    "handler = UnmuteHandler()  # Completely fresh state\n",
    "\n",
    "# Dedicated service connections\n",
    "await handler.start_up()  # Creates STT connection for THIS session only\n",
    "```\n",
    "\n",
    "### **Execution**\n",
    "```python\n",
    "# Independent processing loops\n",
    "tg.create_task(receive_loop(websocket, handler, emit_queue))\n",
    "tg.create_task(emit_loop(websocket, handler, emit_queue))\n",
    "tg.create_task(handler.quest_manager.wait())  # Independent service management\n",
    "```\n",
    "\n",
    "### **Destruction**\n",
    "```python\n",
    "# Guaranteed cleanup per session\n",
    "finally:\n",
    "    await handler.cleanup()  # Closes session-specific resources\n",
    "    mt.ACTIVE_SESSIONS.dec()  # Updates global metric only\n",
    "```\n",
    "\n",
    "## **Resource Sharing Strategy**\n",
    "\n",
    "### **Shared Read-Only Resources**:\n",
    "- Configuration constants\n",
    "- Voice lists\n",
    "- Health check utilities\n",
    "- Metrics collectors\n",
    "- Static file serving endpoints\n",
    "\n",
    "### **Isolated Mutable Resources**:\n",
    "- Conversation state (`chatbot.chat_history`)\n",
    "- Audio processing state (`n_samples_received`, timers)\n",
    "- Service connections (STT, TTS, LLM)\n",
    "- Recording files and handles\n",
    "- Debug information and logs\n",
    "\n",
    "### **Controlled Resource Pooling**:\n",
    "- **Service instances**: Pooled at the infrastructure level but allocated uniquely per session\n",
    "- **Connections**: Each session gets dedicated connections to avoid cross-talk\n",
    "- **Memory**: Session-scoped to prevent leaks\n",
    "- **CPU/GIL**: Semaphore limiting prevents excessive concurrency\n",
    "\n",
    "## **Race Condition Prevention**\n",
    "\n",
    "### **Per-Session Locks**:\n",
    "```python\n",
    "# Turn transition lock is per-handler (per-session)\n",
    "self.turn_transition_lock = asyncio.Lock()\n",
    "```\n",
    "\n",
    "### **Independent Queues**:\n",
    "```python\n",
    "# Each session has its own output queue\n",
    "self.output_queue: asyncio.Queue[HandlerOutput] = asyncio.Queue()\n",
    "```\n",
    "\n",
    "### **Session-Specific State**:\n",
    "All mutable state is contained within the handler instance, preventing cross-session contamination.\n",
    "\n",
    "## **Scalability Design**\n",
    "\n",
    "The architecture is designed for horizontal scaling:\n",
    "- **Vertical scaling limit**: Semaphore limits sessions per instance\n",
    "- **Horizontal scaling**: Run multiple backend instances behind load balancer\n",
    "- **Resource isolation**: No session-to-session interference\n",
    "- **Stateless core**: Most shared resources are read-only configuration\n",
    "\n",
    "This design ensures that each user gets a completely independent conversation experience while efficiently sharing immutable resources and infrastructure.\n",
    "\n",
    "# Concurrent requests for STT\n",
    "\n",
    "Looking at `batched_asr.py`, the system manages concurrent requests and GPU batching through a sophisticated batched processing approach. Here's how it works:\n",
    "\n",
    "## Core Architecture\n",
    "\n",
    "### 1. **Fixed-Size Batch Processing**\n",
    "The system operates on **fixed batch sizes** established at initialization:\n",
    "```python\n",
    "service = ASRService(batch_size=batch_size, ...)\n",
    "```\n",
    "\n",
    "All operations process exactly `batch_size` items simultaneously, regardless of how many active clients there are.\n",
    "\n",
    "### 2. **Client Slot Management**\n",
    "Each batch slot represents a \"client slot\" managed by `ClientState`:\n",
    "```python\n",
    "@dataclass\n",
    "class ClientState:\n",
    "    is_complete: bool = False\n",
    "    active: bool = False\n",
    "    offset: int = 0\n",
    "    real_end: int = 0\n",
    "\n",
    "def __post_init__(self):\n",
    "    self.clients = [ClientState() for _ in range(self.batch_size)]\n",
    "```\n",
    "\n",
    "Each slot can be in different states:\n",
    "- **Inactive**: No active request\n",
    "- **Active**: Processing audio data\n",
    "- **Complete**: Waiting for final processing after EOS marker\n",
    "\n",
    "### 3. **Dynamic Execution Masking**\n",
    "The key innovation is **execution masking** that enables efficient sparse batching:\n",
    "\n",
    "```python\n",
    "def step(self, ..., updates: list[int]) -> None:\n",
    "    # Build execution mask based on active clients\n",
    "    exec_mask = torch.tensor(\n",
    "        [client.active for client in self.clients],\n",
    "        dtype=torch.bool,\n",
    "        device=self.device,\n",
    "    )\n",
    "    \n",
    "    # Only process active slots\n",
    "    self.lm_gen.set_exec_mask(exec_mask)\n",
    "    self.mimi.set_exec_mask(exec_mask)\n",
    "```\n",
    "\n",
    "This means:\n",
    "- **Inactive slots** consume minimal GPU resources (masked out)\n",
    "- **Only active slots** perform actual computation\n",
    "- **Batch operations** still run on full tensors, but masked portions do minimal work\n",
    "\n",
    "### 4. **State Management for Concurrent Requests**\n",
    "The system handles concurrent requests through:\n",
    "\n",
    "**Request Lifecycle:**\n",
    "1. **New Request**: Slot marked as `ACTIVE`\n",
    "2. **Ongoing Processing**: Continuous audio chunks processed\n",
    "3. **End Signal**: Client sends marker, system schedules final processing\n",
    "4. **Completion**: After delay (`asr_delay_in_tokens`), slot marked as `IS_EOS`\n",
    "\n",
    "```python\n",
    "# Handle different update types\n",
    "if update > 0:  # End marker received\n",
    "    self.clients[batch_idx].is_complete = True\n",
    "    self.clients[batch_idx].real_end = (\n",
    "        self.clients[batch_idx].offset \n",
    "        + update + self.asr_delay_in_tokens + 2\n",
    "    )\n",
    "```\n",
    "\n",
    "### 5. **GPU Memory and Performance Optimization**\n",
    "```python\n",
    "# Warmup pre-allocates GPU memory for batched operations\n",
    "def __post_init__(self):\n",
    "    self.lm_gen.streaming_forever(self.batch_size)\n",
    "    self.mimi.streaming_forever(self.batch_size)\n",
    "    \n",
    "    # Pre-warm models with dummy data\n",
    "    for _ in range(3):\n",
    "        batch_zeros = torch.zeros((self.batch_size, 1, self.mimi.frame_size))\n",
    "        audio_tokens = self.mimi.encode(batch_zeros)\n",
    "        frame = self.lm_gen.step(audio_tokens)\n",
    "```\n",
    "\n",
    "This ensures:\n",
    "- **Consistent GPU memory usage** (fixed batch size)\n",
    "- **Eliminates dynamic allocation** during runtime\n",
    "- **Optimal tensor operations** on GPU\n",
    "\n",
    "## Request Flow Example\n",
    "\n",
    "1. **Batch arrives** with mixed active/inactive slots\n",
    "2. **Execution mask** filters to only active slots\n",
    "3. **Mimi model** encodes audio for all slots simultaneously:\n",
    "   ```python\n",
    "   batch_pcm = batch_pcm.view(self.batch_size, frame_size).unsqueeze(1)\n",
    "   audio_tokens = self.mimi.encode(batch_pcm)  # Full batch operation\n",
    "   ```\n",
    "4. **LM model** generates text with execution masking:\n",
    "   ```python\n",
    "   text_tokens, extra_heads = self.lm_gen.step_with_extra_heads(audio_tokens)\n",
    "   ```\n",
    "5. **State updates** for each slot based on progress\n",
    "6. **Results returned** for all slots simultaneously\n",
    "\n",
    "## Key Benefits\n",
    "\n",
    "- **Maximized GPU Utilization**: Batched operations even with sparse activity\n",
    "- **Predictable Performance**: Fixed memory and computation patterns\n",
    "- **Scalable Concurrency**: Batch size determines maximum concurrent requests\n",
    "- **Efficient Resource Use**: Inactive slots consume minimal GPU resources\n",
    "\n",
    "The system essentially trades some theoretical efficiency (processing empty slots) for practical benefits like predictable performance and simplified GPU memory management.\n",
    "\n",
    "# Concurrent requests for TTS\n",
    "\n",
    "Looking at `tts.rs`, the GPU batching and concurrent request handling is quite different from the ASR system. Here's how it works:\n",
    "\n",
    "## Single Request Processing Model\n",
    "\n",
    "### 1. **Mutex-Based Concurrency Control**\n",
    "The TTS system uses a simple mutex to ensure **only one inference can run at a time**:\n",
    "\n",
    "```rust\n",
    "pub struct Model {\n",
    "    // ... other fields\n",
    "    pub(crate) mutex: tokio::sync::Mutex<()>,  // Dummy way to ensure only single inference\n",
    "}\n",
    "\n",
    "pub async fn handle_socket(&self, socket: ws::WebSocket, query: crate::TtsStreamingQuery) -> Result<()> {\n",
    "    let _guard = self.mutex.lock().await;  // Only one request processed at a time\n",
    "    // ... processing logic\n",
    "}\n",
    "```\n",
    "\n",
    "This means:\n",
    "- **No batched parallel processing** like in ASR\n",
    "- **Sequential request handling** - each request blocks others\n",
    "- **Single GPU context** used per request\n",
    "\n",
    "### 2. **Per-Request GPU State Management**\n",
    "Each TTS request creates its own independent state:\n",
    "\n",
    "```rust\n",
    "let mut state = moshi::tts_streaming::State::new(\n",
    "    self.lm.clone(),                    // Shared model weights\n",
    "    Some(moshi::transformer::CaSrc::Tokens(ca_src)),  // Per-request voice conditioning\n",
    "    max_seq_len,\n",
    "    audio_lp, text_lp,\n",
    "    query.cfg_alpha,\n",
    "    config.clone(),\n",
    ");\n",
    "```\n",
    "\n",
    "The model weights are shared, but each request has:\n",
    "- **Independent generation state**\n",
    "- **Independent voice conditioning** (ca_src)\n",
    "- **Independent logits processors** (sampling parameters)\n",
    "\n",
    "### 3. **Streaming Pipeline Architecture**\n",
    "For WebSocket streaming, the system uses a **three-thread pipeline**:\n",
    "\n",
    "```rust\n",
    "let recv_loop = tokio::task::spawn(async move { /* Socket input */ });\n",
    "let process_loop = tokio::task::spawn_blocking(move { /* TTS generation */ });\n",
    "let send_loop = tokio::task::spawn(async move { /* Socket output */ });\n",
    "```\n",
    "\n",
    "This allows:\n",
    "- **Non-blocking socket I/O** (recv/send loops)\n",
    "- **Blocking computation** in process loop\n",
    "- **Independent timing** for each component\n",
    "\n",
    "### 4. **GPU Memory Management**\n",
    "The system handles GPU memory through:\n",
    "\n",
    "```rust\n",
    "// Voice conditioning can load custom voice files\n",
    "let ca_src = self.voice_ca_src(query.voice.as_ref(), query.voices.as_ref())?;\n",
    "\n",
    "// Model weights shared but state is per-request\n",
    "let mut state = moshi::tts_streaming::State::new(...);\n",
    "\n",
    "// Audio tokenizer reset per request\n",
    "let mut audio_tokenizer = self.audio_tokenizer.clone();\n",
    "audio_tokenizer.reset_state();\n",
    "```\n",
    "\n",
    "## Key Differences from ASR Batched System\n",
    "\n",
    "| Aspect | ASR System | TTS System |\n",
    "|--------|------------|------------|\n",
    "| **Concurrency** | Batched parallel processing | Sequential with mutex |\n",
    "| **GPU Utilization** | Fixed-size batch tensors | Single request tensors |\n",
    "| **State Management** | Slot-based client states | Per-request independent state |\n",
    "| **Throughput** | High (batched) | Low (sequential) |\n",
    "| **Latency** | Higher per request | Lower per request |\n",
    "\n",
    "## Request Flow\n",
    "\n",
    "1. **Request Arrives**: Mutex ensures only one runs\n",
    "2. **State Initialization**: Creates independent TTS generation state\n",
    "3. **Voice Conditioning**: Loads/encodes speaker voice if needed\n",
    "4. **Generation Loop**: Processes tokens one by one in sequence\n",
    "5. **Audio Decoding**: Converts tokens to PCM audio chunks\n",
    "6. **Streaming Output**: Sends audio chunks back via WebSocket\n",
    "\n",
    "## Performance Characteristics\n",
    "\n",
    "- **GPU Underutilization**: Single requests don't fully utilize GPU capacity\n",
    "- **Higher Latency for Concurrent Users**: Requests queue behind each other\n",
    "- **Consistent Per-Request Performance**: No interference between requests\n",
    "- **Simpler Error Handling**: Failures are isolated to single requests\n",
    "\n",
    "The design prioritizes **simplicity and quality** over throughput - typical for TTS systems where generation quality is more critical than handling many concurrent users.\n",
    "\n",
    "# TTS batch size meaning\n",
    "\n",
    "Looking at the code more carefully, I need to correct my analysis. The `batch_size` parameter in the TTS config likely refers to **internal model batching** rather than concurrent request handling.\n",
    "\n",
    "## What the `batch_size` Parameter Actually Controls\n",
    "\n",
    "The `batch_size` parameter in TTS configuration most likely controls:\n",
    "\n",
    "### 1. **Internal Model Batch Processing**\n",
    "Even though requests are processed sequentially, the underlying models (LM and Mimi) may still use batching internally for efficiency:\n",
    "\n",
    "```rust\n",
    "// In the model initialization, we see references to batched operations\n",
    "let mut audio_tokenizer = self.audio_tokenizer.clone();\n",
    "audio_tokenizer.reset_state();\n",
    "// The tokenizer is likely configured for a specific batch size\n",
    "```\n",
    "\n",
    "### 2. **Voice Conditioning Batching**\n",
    "When processing multiple voices or speakers:\n",
    "\n",
    "```rust\n",
    "pub fn voice_ca_src(\n",
    "    &self,\n",
    "    voice: Option<&String>,\n",
    "    voices: Option<&Vec<String>>,  // Multiple voices processed together\n",
    ") -> Result<Tensor> {\n",
    "    // When voices is Some(Vec), multiple voice embeddings are batched together\n",
    "    for voice in voices.iter() {\n",
    "        // Process each voice...\n",
    "    }\n",
    "    Ok(self.speaker_encoder.encode(&pcms)?)  // Batch encoding\n",
    "}\n",
    "```\n",
    "\n",
    "### 3. **Tensor Operations Batch Dimensions**\n",
    "The batch size affects how tensors are shaped and processed:\n",
    "\n",
    "```rust\n",
    "// Operations likely use the configured batch size for tensor dimensions\n",
    "let audio_tokens = candle::Tensor::from_vec(audio_tokens, (1, cb, 1), state.device())?;\n",
    "// The first dimension (1) would be batch_size in batched operations\n",
    "```\n",
    "\n",
    "## Real Purpose of `batch_size` in TTS Config\n",
    "\n",
    "The `batch_size` parameter is used for:\n",
    "- **Configuring model internal states** to expect certain batch dimensions\n",
    "- **Voice embedding batching** when processing multiple speakers\n",
    "- **Memory allocation planning** for GPU tensors\n",
    "- **Consistency with shared model infrastructure** that expects batched operations\n",
    "\n",
    "## Why Requests Are Still Sequential\n",
    "\n",
    "The sequential processing (mutex) is separate from the internal batch size configuration because:\n",
    "- **TTS Quality**: Real-time TTS often requires consistent, low-latency responses per user\n",
    "- **Voice Personalization**: Each request may use different voice conditioning\n",
    "- **State Management Complexity**: Managing multiple concurrent generation states is complex\n",
    "\n",
    "## Example Usage\n",
    "\n",
    "```yaml\n",
    "tts:\n",
    "  batch_size: 4  # Internal model batch size\n",
    "  # This allows the model to handle batched tensor operations efficiently\n",
    "  # but concurrent HTTP/WebSocket requests are still processed one at a time\n",
    "```\n",
    "\n",
    "So the `batch_size` parameter optimizes the **internal computation efficiency** rather than enabling **concurrent request processing**. The actual request concurrency is controlled separately by the server's request handling architecture (the mutex in this case).\n",
    "\n",
    "# Mimi and lm\n",
    "\n",
    "Looking at `mimi.rs` and `lm.rs`, I can now see how they integrate into the multi-user architecture. The key insight is that these modules handle **real-time streaming** differently from the batched ASR system.\n",
    "\n",
    "## Multi-User Architecture with GPU Batching\n",
    "\n",
    "### 1. **Room-Based Broadcasting System**\n",
    "\n",
    "The `mimi.rs` module implements a **room-based broadcasting** architecture:\n",
    "\n",
    "```rust\n",
    "struct Room {\n",
    "    sender: Arc<tokio::sync::Mutex<Sender>>,\n",
    "    header_message: ws::Message,\n",
    "    rx: tokio::sync::broadcast::Receiver<ws::Message>,\n",
    "}\n",
    "\n",
    "struct Mimi {\n",
    "    rooms: std::collections::HashMap<String, Room>,\n",
    "    // ... other fields\n",
    "}\n",
    "```\n",
    "\n",
    "This allows:\n",
    "- **Multiple users per room** receiving the same audio stream\n",
    "- **Single sender per room** broadcasting to all receivers\n",
    "- **Broadcast channel** for efficient distribution\n",
    "\n",
    "### 2. **Real-Time Streaming vs Batched Processing**\n",
    "\n",
    "Unlike the batched ASR system, `mimi.rs` and `lm.rs` are designed for **real-time streaming**:\n",
    "\n",
    "```rust\n",
    "// In mimi.rs - processing individual audio chunks as they arrive\n",
    "Ok(MsgType::Codes) => {\n",
    "    let codes: Vec<u32> = msg[1..]\n",
    "        .chunks_exact(4)\n",
    "        .map(|chunk| u32::from_le_bytes([chunk[0], chunk[1], chunk[2], chunk[3]]))\n",
    "        .collect();\n",
    "    let ncodes = codes.len();\n",
    "    let codes = Tensor::from_vec(codes, (1, ncodes, 1), &self.device)?;  // Batch size = 1\n",
    "    let pcm = audio_tokenizer.decode_step(&codes.into(), &().into())?;\n",
    "    // ... process individual PCM chunks\n",
    "}\n",
    "```\n",
    "\n",
    "### 3. **GPU Batch Size Integration**\n",
    "\n",
    "The `batch_size` parameter affects these modules through:\n",
    "\n",
    "**Tensor Shape Configuration:**\n",
    "```rust\n",
    "// The tensor is shaped with batch dimension = 1 for real-time processing\n",
    "let codes = Tensor::from_vec(codes, (1, ncodes, 1), &self.device)?;\n",
    "// But the underlying model is configured for batch_size during initialization\n",
    "let audio_tokenizer = moshi::mimi::load(&mimi.audio_tokenizer_file, Some(8), dev)?;\n",
    "```\n",
    "\n",
    "**Model Initialization:**\n",
    "```rust\n",
    "// In both mimi.rs and lm.rs - models are loaded with batch size awareness\n",
    "let audio_tokenizer = moshi::mimi::load(&lm.audio_tokenizer_file, Some(8), dev)?;\n",
    "```\n",
    "\n",
    "### 4. **Multi-User Integration Flow**\n",
    "\n",
    "Here's how multi-user requests integrate:\n",
    "\n",
    "**Audio Reception (`mimi.rs`):**\n",
    "1. **Single sender** connects and sends audio codes\n",
    "2. **Real-time decoding** happens with batch_size=1 tensors\n",
    "3. **Broadcasting** to all room participants\n",
    "4. **Multiple receivers** get the same decoded audio stream\n",
    "\n",
    "**Language Modeling (`lm.rs`):**\n",
    "1. **Single WebSocket** connection processes incoming audio\n",
    "2. **Real-time token generation** with batch_size=1 operations\n",
    "3. **Text and audio responses** sent back over WebSocket\n",
    "4. **Broadcasting** text transcriptions/audio to connected clients\n",
    "\n",
    "### 5. **Why Not Full Batching?**\n",
    "\n",
    "These modules don't use full batching because:\n",
    "\n",
    "```rust\n",
    "// Real-time constraints - can't wait to fill a batch\n",
    "while let Some(opus) = opus_in_rx.recv().await {\n",
    "    // Process immediately as chunks arrive\n",
    "    let pcm = Tensor::new(pcm, &dev)?.reshape((1, 1, ()))?;  // Immediate processing\n",
    "    // ... immediate token generation\n",
    "}\n",
    "```\n",
    "\n",
    "**Reasons:**\n",
    "- **Latency requirements** - users expect real-time responses\n",
    "- **Variable arrival times** - users don't send data synchronously\n",
    "- **Individual user contexts** - each user may have different state\n",
    "\n",
    "### 6. **Batch Size Parameter Purpose**\n",
    "\n",
    "The `batch_size` in config serves to:\n",
    "- **Pre-allocate GPU memory** for tensor operations\n",
    "- **Configure model internal states** to handle batched operations efficiently\n",
    "- **Maintain consistency** with shared model infrastructure\n",
    "- **Allow future batching optimizations** without model reconfiguration\n",
    "\n",
    "## Architecture Summary\n",
    "\n",
    "```\n",
    "Multiple Users â†’ Individual WebSocket Connections\n",
    "     â†“\n",
    "Real-time Processing (batch_size=1 per user chunk)\n",
    "     â†“\n",
    "Shared Model Weights (configured for batch_size=N)\n",
    "     â†“\n",
    "Room-based Broadcasting (for distribution to multiple listeners)\n",
    "     â†“\n",
    "Individual User Responses\n",
    "```\n",
    "\n",
    "The system optimizes for **real-time interactive experience** rather than batch efficiency, using the batch_size parameter to configure underlying models for optimal performance while processing individual user requests.\n",
    "\n",
    "# Clients refused\n",
    "\n",
    "Based on the code analysis, the Rust server refuses clients based on **fixed capacity limits** derived from configuration parameters, not on dynamic load. Here's exactly when and why clients are refused:\n",
    "\n",
    "## 1. **ASR Service - Fixed Batch Size Limits**\n",
    "\n",
    "In `batched_asr.py`, clients are refused when:\n",
    "```python\n",
    "# When all batch slots are occupied\n",
    "reset_mask = torch.zeros(self.batch_size, dtype=torch.bool)\n",
    "# If no available slots, client request cannot be queued (TODO mentions this)\n",
    "```\n",
    "\n",
    "**Refusal Point**: When all `batch_size` slots are in use and no channel is available:\n",
    "- **Config Parameter**: `batch_size` in the ASR configuration\n",
    "- **Mechanism**: `channels()` method returns `None` when no slots available\n",
    "- **Client Experience**: Immediate refusal, no queuing (though TODO suggests adding batch queuing)\n",
    "\n",
    "## 2. **TTS Service - Mutex-Based Single Request**\n",
    "\n",
    "In `src/tts.rs`, clients are refused when:\n",
    "```rust\n",
    "pub async fn handle_socket(&self, socket: ws::WebSocket, query: crate::TtsStreamingQuery) -> Result<()> {\n",
    "    let _guard = self.mutex.lock().await;  // Only one request at a time\n",
    "    // ... processing\n",
    "}\n",
    "```\n",
    "\n",
    "**Refusal Point**: When attempting to acquire the mutex:\n",
    "- **Config Parameter**: Implicit in the mutex design (always 1)\n",
    "- **Mechanism**: Tokio's mutex timing out or connection queue limits\n",
    "- **Client Experience**: Connection may timeout or be rejected at network level\n",
    "\n",
    "## 3. **MIMI Service - Room Capacity Limits**\n",
    "\n",
    "In `src/mimi.rs`, clients are refused when:\n",
    "```rust\n",
    "// Only one producer per room allowed\n",
    "let mut sender = match room.sender.try_lock() {\n",
    "    Ok(s) => s,\n",
    "    Err(_) => anyhow::bail!(\"already a producer\"),  // Refused immediately\n",
    "};\n",
    "```\n",
    "\n",
    "**Refusal Points**:\n",
    "1. **Producer Limit**: Only one sender per room\n",
    "2. **Room Existence**: Non-existent room IDs\n",
    "3. **Broadcast Channel**: Channel capacity limits (10 messages buffered)\n",
    "\n",
    "## 4. **LM Service - Implicit Connection Limits**\n",
    "\n",
    "In `src/lm.rs`, clients face implicit limits:\n",
    "```rust\n",
    "// No explicit queuing or capacity management\n",
    "// Each WebSocket connection gets its own processing\n",
    "let (opus_in_tx, mut opus_in_rx) = tokio::sync::mpsc::unbounded_channel();\n",
    "```\n",
    "\n",
    "**Refusal Points**:\n",
    "- **System Resources**: OS-level connection limits\n",
    "- **Memory Exhaustion**: No explicit bounds checking\n",
    "- **Timeouts**: Fixed 360-second timeout\n",
    "\n",
    "## Key Refusal Patterns\n",
    "\n",
    "### **Hard Refusals** (Immediate):\n",
    "- **TTS**: When mutex is held by another client\n",
    "- **MIMI**: When trying to become producer in occupied room\n",
    "- **ASR**: When all batch slots are occupied (no queuing)\n",
    "\n",
    "### **Soft Refusals** (Timeout/Resource):\n",
    "- **All Services**: When system resources exhausted\n",
    "- **All Services**: When network timeouts occur\n",
    "- **MIMI/LM**: When broadcast channels overflow\n",
    "\n",
    "## Configuration Parameters That Control Capacity\n",
    "\n",
    "```yaml\n",
    "# These parameters directly control client capacity:\n",
    "asr:\n",
    "  batch_size: 8        # Max concurrent ASR requests\n",
    "\n",
    "tts:\n",
    "  batch_size: 1        # Internal batch size (concurrency still = 1 due to mutex)\n",
    "\n",
    "mimi:\n",
    "  rooms: [\"room1\", \"room2\"]  # Fixed number of rooms\n",
    "  # Each room allows exactly 1 producer + unlimited consumers\n",
    "```\n",
    "\n",
    "## No Dynamic Load Management\n",
    "\n",
    "The server notably **lacks** dynamic load management:\n",
    "- **No queueing systems** (except implicit OS-level connection queues)\n",
    "- **No resource monitoring** or adaptive refusal\n",
    "- **No load-based scaling** decisions\n",
    "- **Fixed resource allocation** based on configuration\n",
    "\n",
    "## Client Experience\n",
    "\n",
    "1. **ASR Clients**: Refused when all `batch_size` slots full\n",
    "2. **TTS Clients**: Blocked until previous client finishes (de facto refusal under load)\n",
    "3. **MIMI Clients**: Refused immediately if room producer slot taken\n",
    "4. **LM Clients**: May timeout or be refused at system level under high load\n",
    "\n",
    "The system is designed for **predictable, fixed-capacity operation** rather than elastic scaling, with refusal decisions based purely on **pre-configured limits** rather than dynamic system load assessment.\n",
    "\n",
    "# TTS token generation logic\n",
    "\n",
    "Now I can provide a detailed explanation of the `step` method in `tts_streaming.rs`. This is where the language model generates output tokens.\n",
    "\n",
    "## The `step` Method Breakdown\n",
    "\n",
    "The `step` method in `State` is the core of token generation:\n",
    "\n",
    "```rust\n",
    "pub fn step(\n",
    "    &mut self,\n",
    "    prev_text_token: u32,\n",
    "    allowed_tokens: AllowedTokens,\n",
    "    conditions: Option<&crate::conditioner::Condition>,\n",
    ") -> Result<u32> {\n",
    "```\n",
    "\n",
    "### 1. **Audio Code Preparation**\n",
    "First, it prepares the audio codes (tokens) for the model input:\n",
    "\n",
    "```rust\n",
    "let mut codes = Vec::with_capacity(self.model.generated_audio_codebooks());\n",
    "let dev = self.model.device();\n",
    "let batch_size = if self.cfg_alpha.is_some() { 2 } else { 1 };\n",
    "\n",
    "for codebook in 0..self.model.generated_audio_codebooks() {\n",
    "    // Complex logic to determine the correct audio token for each codebook\n",
    "    // Takes into account delays, padding, and previous tokens\n",
    "    let t = if codebook == 0 {\n",
    "        if self.step_idx == 0 {\n",
    "            Some(self.audio_pad_token())\n",
    "        } else if self.step_idx <= self.config.text_audio_delay_in_tokens {\n",
    "            None // Use literal zeros for first few seconds\n",
    "        } else {\n",
    "            Some(self.audio_tokens[self.step_idx - 1][codebook])\n",
    "        }\n",
    "    } else if self.step_idx <= self.config.acoustic_delay {\n",
    "        Some(self.audio_pad_token())\n",
    "    } else if self.step_idx <= self.config.text_audio_delay_in_tokens + self.config.acoustic_delay {\n",
    "        None // Use literal zeros\n",
    "    } else {\n",
    "        Some(self.audio_tokens[self.step_idx - self.config.acoustic_delay - 1][codebook])\n",
    "    };\n",
    "    \n",
    "    // Convert to tensor for model input\n",
    "    let t = match t {\n",
    "        Some(t) => Some(Tensor::from_vec(vec![t; batch_size], (batch_size, 1), dev)?),\n",
    "        None => None,\n",
    "    };\n",
    "    codes.push(t)\n",
    "}\n",
    "```\n",
    "\n",
    "### 2. **Previous Text Token Preparation**\n",
    "```rust\n",
    "let prev_text_token =\n",
    "    Some(Tensor::from_vec(vec![prev_text_token; batch_size], (batch_size, 1), dev)?);\n",
    "```\n",
    "\n",
    "### 3. **Language Model Forward Pass**\n",
    "This is where the actual language model inference happens:\n",
    "\n",
    "```rust\n",
    "let (text_logits, ys) = match self.ca_src.as_ref() {\n",
    "    None => self.model.forward_cond(prev_text_token, codes, conditions, &().into())?,\n",
    "    Some(ca_src) => {\n",
    "        self.model.forward_ca(prev_text_token, codes, ca_src, conditions, &().into())?\n",
    "    }\n",
    "};\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- `forward_cond` or `forward_ca` calls the transformer model\n",
    "- `ca_src` provides speaker conditioning information\n",
    "- `conditions` provides additional control conditions\n",
    "- Returns `text_logits` (for text token prediction) and `ys` (for audio token generation)\n",
    "\n",
    "### 4. **Classifier-Free Guidance (CFG) Adjustment**\n",
    "```rust\n",
    "let text_logits = match self.cfg_alpha {\n",
    "    None => text_logits.i((0, 0))?,\n",
    "    Some(a) => match text_logits.dim(0)? {\n",
    "        2 => ((text_logits.i((0, 0))? * a)? - (text_logits.i((1, 0))? * (a - 1.))?)?,\n",
    "        b_size => candle::bail!(\"unexpected batch size {b_size}\"),\n",
    "    },\n",
    "};\n",
    "```\n",
    "\n",
    "### 5. **Text Token Sampling**\n",
    "Based on allowed tokens and sampling strategy:\n",
    "\n",
    "```rust\n",
    "let text_token = match allowed_tokens {\n",
    "    AllowedTokens::Text(v) => v,           // Forced token\n",
    "    AllowedTokens::Pad => self.config.text_pad_token,  // Forced pad\n",
    "    AllowedTokens::PadOrEpad => {          // Sample between pad or end-of-phrase\n",
    "        if self.consecutive_pads > self.config.max_consecutive_pads {\n",
    "            self.config.text_eop_token\n",
    "        } else {\n",
    "            let text_token = self.text_lp.sample(&text_logits)?;\n",
    "            if text_token == self.config.text_pad_token {\n",
    "                self.config.text_pad_token\n",
    "            } else {\n",
    "                self.config.text_eop_token\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "};\n",
    "```\n",
    "\n",
    "### 6. **Consecutive Pad Tracking**\n",
    "```rust\n",
    "if text_token == self.config.text_pad_token {\n",
    "    self.consecutive_pads += 1\n",
    "} else {\n",
    "    self.consecutive_pads = 0\n",
    "}\n",
    "self.text_tokens[self.step_idx] = text_token;\n",
    "```\n",
    "\n",
    "### 7. **Audio Token Generation**\n",
    "Uses the model's depformer to generate audio tokens:\n",
    "\n",
    "```rust\n",
    "let last_audio_tokens = if self.step_idx < self.config.text_audio_delay_in_tokens {\n",
    "    None\n",
    "} else {\n",
    "    match self.cfg_alpha {\n",
    "        None => self.model.depformer_sample(\n",
    "            &ys,\n",
    "            Some(text_token),\n",
    "            self.forced_audio_tokens.forced_tokens(self.step_idx),\n",
    "            &mut self.audio_lp,\n",
    "        )?,\n",
    "        Some(cfg_alpha) => self.model.depformer_sample_cfg(\n",
    "            &ys,\n",
    "            cfg_alpha,\n",
    "            Some(text_token),\n",
    "            self.forced_audio_tokens.forced_tokens(self.step_idx),\n",
    "            &mut self.audio_lp,\n",
    "        )?,\n",
    "    }\n",
    "};\n",
    "```\n",
    "\n",
    "### 8. **Audio Token Storage**\n",
    "Stores generated audio tokens with proper delay handling:\n",
    "\n",
    "```rust\n",
    "let audio_pad_token = self.audio_pad_token();\n",
    "for c_idx in 0..self.model.generated_audio_codebooks() {\n",
    "    let delay = if c_idx == 0 { 0 } else { self.config.acoustic_delay };\n",
    "    let pos = &mut self.audio_tokens[self.step_idx.saturating_sub(delay)][c_idx];\n",
    "    match last_audio_tokens.as_ref() {\n",
    "        Some(lat) => {\n",
    "            if *pos == UNGENERATED {\n",
    "                *pos = lat[c_idx]\n",
    "            }\n",
    "        }\n",
    "        None => {\n",
    "            if *pos == UNGENERATED {\n",
    "                *pos = audio_pad_token\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### 9. **State Updates and Return**\n",
    "```rust\n",
    "self.step_idx += 1;\n",
    "if self.step_idx >= self.audio_tokens.len() {\n",
    "    candle::bail!(\"max step-idx reached\")\n",
    "}\n",
    "Ok(text_token)\n",
    "```\n",
    "\n",
    "## Key Architecture Insights\n",
    "\n",
    "1. **Interleaved Generation**: Text and audio tokens are generated together in a coordinated fashion\n",
    "2. **Delay Handling**: Complex delay management for proper audio-text synchronization\n",
    "3. **Conditioning**: Speaker embedding (`ca_src`) and control conditions influence generation\n",
    "4. **Classifier-Free Guidance**: Optional CFG for improved generation quality\n",
    "5. **Token Constraints**: `AllowedTokens` enforces generation constraints based on input state\n",
    "\n",
    "This method is called repeatedly in the generation loop in `tts.rs`, with each call advancing the generation by one step and returning the newly generated text token.\n",
    "\n",
    "# Batch transformer processing\n",
    "\n",
    "Looking at `lm_generate_multistream.rs`, I can explain how the transformer batch size is managed:\n",
    "\n",
    "## Batch Size Management in Transformer Operations\n",
    "\n",
    "### 1. **Dynamic Batch Size Based on CFG**\n",
    "The batch size is determined dynamically based on whether Classifier-Free Guidance (CFG) is used:\n",
    "\n",
    "```rust\n",
    "let batch_size = if self.cfg_alpha.is_some() { 2 } else { 1 };\n",
    "```\n",
    "\n",
    "- **Batch Size = 1**: Normal operation without CFG\n",
    "- **Batch Size = 2**: When CFG is enabled (one for conditional, one for unconditional)\n",
    "\n",
    "### 2. **Tensor Creation with Dynamic Batch Size**\n",
    "All tensors are created with the dynamic batch size:\n",
    "\n",
    "```rust\n",
    "// Audio tokens tensor\n",
    "let t = Tensor::from_vec(vec![t; batch_size], (batch_size, 1), dev)?;\n",
    "\n",
    "// Text token tensor  \n",
    "let text_token = match text_token {\n",
    "    Some(text_token) => {\n",
    "        Some(Tensor::from_vec(vec![text_token; batch_size], (batch_size, 1), dev)?)\n",
    "    }\n",
    "    None => None,\n",
    "};\n",
    "```\n",
    "\n",
    "### 3. **Model Forward Pass with Batch Support**\n",
    "The model handles the batched inputs:\n",
    "\n",
    "```rust\n",
    "let (text_logits, ys) = match ca_src.as_ref() {\n",
    "    None => {\n",
    "        let (logits, ys) = self.model.forward_cond(text_token, codes, conditions, &().into())?;\n",
    "        let logits = match self.cfg_alpha {\n",
    "            None => logits.i((0, 0))?,  // Extract first batch element\n",
    "            Some(a) => match logits.dim(0)? {\n",
    "                2 => ((logits.i((0, 0))? * a)? - (logits.i((1, 0))? * (a - 1.))?)?,  // CFG combination\n",
    "                b_size => candle::bail!(\"unexpected batch size {b_size}\"),\n",
    "            },\n",
    "        };\n",
    "        (logits, ys)\n",
    "    }\n",
    "    Some(ca_src) => {\n",
    "        if self.cfg_alpha.is_some() {\n",
    "            candle::bail!(\"cfg is not supported with cross attention\")\n",
    "        }\n",
    "        let (logits, ys) = self.model.forward_ca(text_token, codes, ca_src, None, &().into())?;\n",
    "        (logits.i((0, 0))?, ys)  // Extract first batch element\n",
    "    }\n",
    "};\n",
    "```\n",
    "\n",
    "### 4. **Batch Size Handling in Audio Generation**\n",
    "The audio token generation also respects the batch size:\n",
    "\n",
    "```rust\n",
    "let last_audio_tokens = match self.cfg_alpha {\n",
    "    None => self.model.depformer_sample(\n",
    "        &ys,\n",
    "        Some(text_token),\n",
    "        self.forced_audio_tokens.forced_tokens(self.step_idx),\n",
    "        &mut self.audio_lp,\n",
    "    )?,\n",
    "    Some(cfg_alpha) => self.model.depformer_sample_cfg(\n",
    "        &ys,\n",
    "        cfg_alpha,\n",
    "        Some(text_token),\n",
    "        self.forced_audio_tokens.forced_tokens(self.step_idx),\n",
    "        &mut self.audio_lp,\n",
    "    )?,\n",
    "};\n",
    "```\n",
    "\n",
    "### 5. **CFG Logic for Batch Processing**\n",
    "When CFG is enabled, the system:\n",
    "1. Processes both conditional and unconditional paths in a single batch of size 2\n",
    "2. Combines the results using the alpha parameter:\n",
    "\n",
    "```rust\n",
    "// CFG combination: conditional * alpha - unconditional * (alpha - 1)\n",
    "Some(a) => match logits.dim(0)? {\n",
    "    2 => ((logits.i((0, 0))? * a)? - (logits.i((1, 0))? * (a - 1.))?)?,\n",
    "    b_size => candle::bail!(\"unexpected batch size {b_size}\"),\n",
    "},\n",
    "```\n",
    "\n",
    "## Key Insights\n",
    "\n",
    "1. **Runtime Batch Size Determination**: Batch size is determined at inference time based on CFG usage\n",
    "2. **Model Compatibility**: The underlying transformer model must support batched operations\n",
    "3. **Memory Efficiency**: Only uses batch size 2 when necessary (CFG enabled)\n",
    "4. **Constraint Handling**: CFG is incompatible with cross-attention (`ca_src`) due to complexity\n",
    "\n",
    "The batch size management is minimal but effective - it scales between 1 and 2 based on whether advanced sampling techniques like CFG are enabled, allowing the transformer to efficiently handle both simple and complex generation scenarios.\n",
    "\n",
    "`batched_transformer.rs` implements a **streaming transformer architecture** designed for efficient batched processing. Here's how it works:\n",
    "\n",
    "## Core Architecture\n",
    "\n",
    "### 1. **Batched KV Cache Management**\n",
    "The key innovation is the **KV Cache Builder** that pre-allocates memory for batched operations:\n",
    "\n",
    "```rust\n",
    "pub struct StreamingTransformer {\n",
    "    builder: KvCacheBuilder,  // Pre-allocated batched KV cache\n",
    "    layers: Vec<StreamingTransformerLayer>,\n",
    "}\n",
    "\n",
    "impl StreamingTransformer {\n",
    "    pub fn new(batch_size: usize, cfg: &Config, vb: MaybeQuantizedVarBuilder) -> Result<Self> {\n",
    "        let builder = KvCacheBuilder::new(batch_size, cfg.context, vb.dtype(), vb.device())?;\n",
    "        // ... initialize layers with shared builder\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The `KvCacheBuilder` creates fixed-size caches for all batch elements, eliminating dynamic allocation during inference.\n",
    "\n",
    "### 2. **Streaming Multihead Attention**\n",
    "The attention mechanism handles streaming inputs with KV caching:\n",
    "\n",
    "```rust\n",
    "impl StreamingMultiheadAttention {\n",
    "    pub fn forward(&mut self, xs: &Tensor, rope: Option<&Rope>, iam: &IndicesAndMask) -> Result<Tensor> {\n",
    "        // Project input to QKV\n",
    "        let qkv = xs.apply(&self.in_proj)?.reshape((b, t, 3, self.num_heads, head_dim))?;\n",
    "        \n",
    "        // Apply rotary embeddings if needed\n",
    "        let q = qkv.i((.., .., 0))?;\n",
    "        let k = qkv.i((.., .., 1))?;\n",
    "        let v = qkv.i((.., .., 2))?;\n",
    "        \n",
    "        if let Some(rope) = rope.as_ref() {\n",
    "            q = rope.apply_rotary_emb(&q)?;\n",
    "            k = rope.apply_rotary_emb(&k)?;\n",
    "        }\n",
    "\n",
    "        // Append to KV cache and retrieve cached + new keys/values\n",
    "        let (k, v) = { self.kv_cache.append(&k.contiguous()?, &v.contiguous()?, iam)? };\n",
    "        \n",
    "        // Perform attention computation with context limiting\n",
    "        let k_len = k.dim(2)?;\n",
    "        let k_target_len = t + usize::min(self.context, k_len - t);\n",
    "        let (k, v) = if k_target_len < k_len {\n",
    "            let k = k.narrow(2, k_len - k_target_len, k_target_len)?;\n",
    "            let v = v.narrow(2, k_len - k_target_len, k_target_len)?;\n",
    "            (k, v)\n",
    "        } else {\n",
    "            (k.clone(), v.clone())\n",
    "        };\n",
    "\n",
    "        // Standard scaled dot-product attention\n",
    "        let xs = {\n",
    "            let pre_ws = q.matmul(&k.t()?)?;\n",
    "            let pre_ws = (pre_ws * (head_dim as f64).powf(-0.5))?;\n",
    "            let pre_ws = pre_ws.broadcast_add(iam.mask())?;\n",
    "            let ws = candle_nn::ops::softmax_last_dim(&pre_ws)?;\n",
    "            ws.matmul(&v)?\n",
    "        };\n",
    "        \n",
    "        xs.apply(&self.out_proj)\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### 3. **Batched Mask Handling**\n",
    "The system uses `IndicesAndMask` to efficiently handle batched attention masks:\n",
    "\n",
    "```rust\n",
    "let iam = match m.cpu() {\n",
    "    None => candle::bail!(\"batched-transformer expects a mask\"),\n",
    "    Some(m) => self.builder.indices_and_mask(t, m)?,\n",
    "};\n",
    "```\n",
    "\n",
    "This allows different batch elements to have different attention patterns while maintaining computational efficiency.\n",
    "\n",
    "### 4. **Streaming Processing Interface**\n",
    "The transformer implements a streaming interface for incremental processing:\n",
    "\n",
    "```rust\n",
    "impl StreamingModule for StreamingTransformer {\n",
    "    fn reset_state(&mut self) {\n",
    "        self.builder.reset();  // Reset all batch KV caches\n",
    "    }\n",
    "\n",
    "    fn step(&mut self, xs: &StreamTensor, m: &StreamMask) -> Result<StreamTensor> {\n",
    "        match xs.as_option() {\n",
    "            None => Ok(StreamTensor::empty()),\n",
    "            Some(xs) => Ok(StreamTensor::from_tensor(self.forward(xs, m)?)),\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "## Key Features\n",
    "\n",
    "### **Batched Memory Management**\n",
    "- Pre-allocated KV caches for all batch elements\n",
    "- Efficient memory reuse across time steps\n",
    "- Context window management to limit memory usage\n",
    "\n",
    "### **Flexible Positional Embeddings**\n",
    "Support for multiple positional embedding types:\n",
    "```rust\n",
    "let rope = match cfg.positional_embedding {\n",
    "    PositionalEmbedding::Rope => Some(RotaryEmbedding::new(...)?),\n",
    "    PositionalEmbedding::None | PositionalEmbedding::Sin => None,\n",
    "};\n",
    "```\n",
    "\n",
    "### **Cross-Attention Support**\n",
    "Optional cross-attention layers with shared parameters:\n",
    "```rust\n",
    "let cross_attn = match cfg.cross_attention.map(|v| v.1) {\n",
    "    Some(norm_type) => {\n",
    "        let norm_cross = Norm::new_shortcut(d_model, norm_type, vb.pp(\"norm_cross\"))?;\n",
    "        let cross_attn = match shared_ca_vb {\n",
    "            None => StreamingMultiheadCrossAttention::new(cfg, vb.pp(\"cross_attention\"), None)?,\n",
    "            Some(shared_vb) => StreamingMultiheadCrossAttention::new(\n",
    "                cfg, shared_vb.pp(\"cross_attention\"), Some(vb.pp(\"cross_attention.gate\")),\n",
    "            )?,\n",
    "        };\n",
    "        Some((norm_cross, cross_attn))\n",
    "    }\n",
    "    None => None,\n",
    "};\n",
    "```\n",
    "\n",
    "### **Projection Layers**\n",
    "Input/output projections for dimension matching:\n",
    "```rust\n",
    "pub struct ProjectedTransformer {\n",
    "    transformer: StreamingTransformer,\n",
    "    input_proj: Option<MaybeQuantizedLinear>,    // Project input to model dim\n",
    "    output_projs: Vec<Option<MaybeQuantizedLinear>>, // Project outputs to target dims\n",
    "}\n",
    "```\n",
    "\n",
    "## Batch Processing Workflow\n",
    "\n",
    "1. **Initialization**: Create transformer with fixed `batch_size`\n",
    "2. **Batched Forward Pass**: Process batch of sequences simultaneously\n",
    "3. **KV Cache Updates**: Append new keys/values to batched caches\n",
    "4. **Context Management**: Trim old cache entries beyond context window\n",
    "5. **Streaming Steps**: Process one time step at a time for real-time applications\n",
    "6. **Batch Reset**: Reset specific batch indices when sequences complete\n",
    "\n",
    "## Performance Benefits\n",
    "\n",
    "- **Memory Efficiency**: Pre-allocated batched caches eliminate fragmentation\n",
    "- **Computational Efficiency**: Batched tensor operations maximize GPU utilization\n",
    "- **Streaming Capability**: Incremental processing for real-time applications\n",
    "- **Context Management**: Automatic handling of long sequences with sliding windows\n",
    "\n",
    "This architecture is designed for scenarios where you need to process multiple sequences efficiently, either in batch mode for throughput or in streaming mode for low-latency applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c62326-e618-431f-995b-eff243ed932c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
